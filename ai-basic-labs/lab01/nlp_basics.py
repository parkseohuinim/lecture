"""
NLP ê¸°ì´ˆ ì‹¤ìŠµ
- í† í°í™”, ë¶ˆìš©ì–´ ì œê±°, lemmatization
- OpenAI ì„ë² ë”© ìƒì„± ë° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
- ê°„ë‹¨í•œ ë¬¸ì¥ ê²€ìƒ‰ê¸° êµ¬í˜„

ì‹¤ìŠµ í•­ëª©:
[ê¸°ì´ˆ] 1~5ë²ˆ - API í‚¤ë§Œ ìˆìœ¼ë©´ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥
1. tiktokenìœ¼ë¡œ í† í° ì´í•´í•˜ê¸° - GPTê°€ í…ìŠ¤íŠ¸ë¥¼ ì–´ë–»ê²Œ ë³´ëŠ”ê°€
2. NLTK ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ - í† í°í™”, ë¶ˆìš©ì–´, í‘œì œì–´ ì¶”ì¶œ
3. OpenAI ì„ë² ë”© ìƒì„± - í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
4. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° - ë²¡í„° ê°„ ìœ ì‚¬ì„± ì¸¡ì •
5. ê°„ë‹¨í•œ ê²€ìƒ‰ ì—”ì§„ - ì˜ë¯¸ ê¸°ë°˜ ë¬¸ì¥ ê²€ìƒ‰

[ì‹¬í™”] 6~9ë²ˆ - ì‹œê°í™”/ëª¨ë¸ ë¹„êµ
6. ì„ë² ë”© ì‹œê°í™” - t-SNEë¡œ ë²¡í„° ê³µê°„ ì´í•´í•˜ê¸° (matplotlib í•„ìš”)
7. ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸ - Sentence Transformers ì†Œê°œ
8. ì„ë² ë”© ëª¨ë¸ ë¹„êµ - small vs large ì„±ëŠ¥/ë¹„ìš© ë¶„ì„
9. í•œê¸€-ì˜ì–´ ì„ë² ë”© ë¹„êµ - ë‹¤êµ­ì–´ ì˜ë¯¸ ì •ë ¬(Alignment) ì‹¤í—˜ ğŸ†•

ì‹¤í–‰ ëª¨ë“œ:
  python nlp_basics.py          # ì „ì²´ ì‹¤ìŠµ (ê¸°ë³¸)
  python nlp_basics.py --demo   # ì¶œë ¥ ìœ„ì£¼ ë°ëª¨ (API í˜¸ì¶œ ìµœì†Œí™”)
  python nlp_basics.py --run    # ì‹¤ì œ ê³„ì‚° + ì‹œê°í™” íŒŒì¼ ì €ì¥
  python nlp_basics.py --quick  # í•µì‹¬ ì‹¤ìŠµë§Œ (1~5ë²ˆ)
"""

import os
import sys
import argparse
from pathlib import Path
from typing import List, Tuple, Dict, Any
import numpy as np
import tiktoken
import nltk
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from openai import OpenAI
from dotenv import load_dotenv
import ssl

# SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™” (NLTK ë‹¤ìš´ë¡œë“œìš©)
try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼ ë¡œë“œ
project_root = Path(__file__).parent.parent
load_dotenv(dotenv_path=project_root / '.env')

# ê³µí†µ ìœ í‹¸ë¦¬í‹° importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(project_root))
from utils import (
    print_section_header, 
    print_subsection, 
    print_key_points, 
    visualize_similarity_bar,
    cosine_similarity,
    cosine_similarity_normalized,
    is_normalized,
    interpret_cosine_similarity,
    get_openai_client,
    COSINE_THRESHOLDS
)


# ============================================================================
# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
# ============================================================================

def download_nltk_data():
    """í•„ìš”í•œ NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
    print("\n[INFO] NLTK ë°ì´í„° í™•ì¸ ì¤‘...")
    
    resources = [
        ('tokenizers/punkt_tab', 'punkt_tab'),
        ('corpora/stopwords', 'stopwords'),
        ('corpora/wordnet', 'wordnet'),
        ('corpora/omw-1.4', 'omw-1.4'),
        ('taggers/averaged_perceptron_tagger_eng', 'averaged_perceptron_tagger_eng'),  # POS íƒœê¹…ìš©
    ]
    
    download_needed = False
    
    for path, name in resources:
        try:
            nltk.data.find(path)
            print(f"  [OK] '{name}' ì´ë¯¸ ì„¤ì¹˜ë¨")
        except LookupError:
            print(f"  [~] '{name}' ì„¤ì¹˜ í™•ì¸ ì¤‘...")
            download_needed = True
            try:
                result = nltk.download(name, quiet=True)
                if result:
                    print(f"  [OK] '{name}' ì„¤ì¹˜ ì™„ë£Œ")
                else:
                    print(f"  [OK] '{name}' ì´ë¯¸ ìµœì‹  ìƒíƒœ")
            except Exception as e:
                print(f"  [X] '{name}' ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
    
    if not download_needed:
        print("\n[OK] ëª¨ë“  NLTK ë°ì´í„°ê°€ ì´ë¯¸ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤!")
    else:
        print("\n[OK] NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")


# ============================================================================
# 1. tiktokenìœ¼ë¡œ í† í° ì´í•´í•˜ê¸°
# ============================================================================

def count_tokens_with_tiktoken(text: str, model: str = "gpt-3.5-turbo") -> int:
    """
    tiktokenì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°
    
    Args:
        text: í† í° ìˆ˜ë¥¼ ê³„ì‚°í•  í…ìŠ¤íŠ¸
        model: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
    
    Returns:
        í† í° ìˆ˜
    """
    encoding = tiktoken.encoding_for_model(model)
    tokens = encoding.encode(text)
    return len(tokens)


def demo_tiktoken():
    """ì‹¤ìŠµ 1: tiktokenìœ¼ë¡œ í† í° ì´í•´í•˜ê¸°"""
    print("\n" + "="*80)
    print("[1] ì‹¤ìŠµ 1: tiktokenìœ¼ë¡œ í† í° ì´í•´í•˜ê¸°")
    print("="*80)
    print("ëª©í‘œ: GPTê°€ í…ìŠ¤íŠ¸ë¥¼ ì–´ë–»ê²Œ í† í°ìœ¼ë¡œ ë¶„í•´í•˜ëŠ”ì§€ ì´í•´")
    print("í•µì‹¬: í† í° != ë‹¨ì–´, í•œê¸€ì€ ì˜ì–´ë³´ë‹¤ ë” ë§ì€ í† í° ì‚¬ìš©")
    
    # í† í°ì´ë€ ë¬´ì—‡ì¸ê°€?
    print_section_header("í† í°(Token)ì´ë€?", "[INFO]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [TIP] í† í°ì˜ ê°œë…                                       â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  â€¢ GPTëŠ” í…ìŠ¤íŠ¸ë¥¼ 'í† í°' ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤               â”‚
  â”‚  â€¢ í† í° != ë‹¨ì–´ (ë‹¨ì–´ë³´ë‹¤ ì‘ê±°ë‚˜ í´ ìˆ˜ ìˆìŒ)             â”‚
  â”‚  â€¢ ì˜ì–´: 1 ë‹¨ì–´ = 1~2 í† í°                               â”‚
  â”‚  â€¢ í•œê¸€: 1 ê¸€ì = 1.5~3 í† í° (ë°”ì´íŠ¸ ë‹¨ìœ„ ë¶„í•´)          â”‚
  â”‚                                                         â”‚
  â”‚  ì™œ ì¤‘ìš”í•œê°€?                                            â”‚
  â”‚  â€¢ API ë¹„ìš©ì´ í† í° ë‹¨ìœ„ë¡œ ê³„ì‚°ë¨                         â”‚
  â”‚  â€¢ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì œí•œì´ í† í° ê¸°ì¤€                      â”‚
  â”‚  â€¢ ì˜ˆ: GPT-4 Turbo = 128K í† í° ì œí•œ                     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    texts = [
        "Hello, how are you?",
        "ì•ˆë…•í•˜ì„¸ìš”, ë°˜ê°‘ìŠµë‹ˆë‹¤!",
        "This is a longer sentence with more words to demonstrate token counting.",
        "AIì™€ ë¨¸ì‹ ëŸ¬ë‹ì€ í˜„ëŒ€ ê¸°ìˆ ì˜ í•µì‹¬ì…ë‹ˆë‹¤."
    ]
    
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    
    print_section_header("ì˜ì–´ vs í•œê¸€ í† í° ë¹„êµ", "[CMP]")
    
    for text in texts:
        token_count = count_tokens_with_tiktoken(text)
        char_count = len(text)
        chars_per_token = char_count / token_count  # í† í° 1ê°œë‹¹ ë¬¸ì ìˆ˜
        
        # íš¨ìœ¨ì„± í•´ì„ (í† í°ë‹¹ ë¬¸ìê°€ ë§ì„ìˆ˜ë¡ íš¨ìœ¨ì )
        if chars_per_token >= 4.0:
            efficiency = "ë§¤ìš° íš¨ìœ¨ì "
        elif chars_per_token >= 2.5:
            efficiency = "íš¨ìœ¨ì "
        elif chars_per_token >= 1.5:
            efficiency = "ë³´í†µ"
        else:
            efficiency = "ë¹„íš¨ìœ¨ì  (í† í° ë§ì´ ì†Œëª¨)"
        
        print(f"\n{'â”€'*60}")
        print(f"í…ìŠ¤íŠ¸: {text}")
        print(f"ë¬¸ì ìˆ˜: {char_count}ì | í† í° ìˆ˜: {token_count}ê°œ")
        print(f"í† í°ë‹¹ ë¬¸ì ìˆ˜: {chars_per_token:.2f}ì/í† í° â†’ {efficiency}")
        
        # ì‹¤ì œ í† í° ID í™•ì¸
        tokens = encoding.encode(text)
        print(f"\ní† í° ID: {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
        
        # ê°œë³„ í† í°ì„ ë””ì½”ë”©í•˜ê³  ë°”ì´íŠ¸ ì •ë³´ë„ í‘œì‹œ
        print(f"\ní† í° ë¶„ì„:")
        for i, token_id in enumerate(tokens[:8]):  # ì²˜ìŒ 8ê°œë§Œ
            decoded = encoding.decode([token_id])
            byte_repr = encoding.decode_single_token_bytes(token_id)
            
            # ì¶œë ¥ ê°€ëŠ¥í•œ ë¬¸ìì¸ì§€ í™•ì¸í•˜ê³  ì ì ˆíˆ í‘œì‹œ
            if decoded.isprintable() and not any(ord(c) > 127 for c in decoded):
                # ASCII ì¶œë ¥ ê°€ëŠ¥ ë¬¸ì
                display = f"'{decoded}'"
            elif all(ord(c) > 127 for c in decoded) and decoded.isprintable():
                # ì™„ì „í•œ ìœ ë‹ˆì½”ë“œ ë¬¸ì (í•œê¸€ ë“±)
                display = f"'{decoded}'"
            else:
                # ë¶ˆì™„ì „í•œ UTF-8 ë°”ì´íŠ¸ ì‹œí€€ìŠ¤ (BPE ë¶„í•´ë¡œ ì¸í•œ ë¶€ë¶„ ë°”ì´íŠ¸)
                display = f"<bytes: {byte_repr.hex()}>"
            
            print(f"  [{i+1}] ID:{token_id:6d} | {display:20s} | raw: {byte_repr}")
        
        if len(tokens) > 8:
            print(f"  ... (ë‚˜ë¨¸ì§€ {len(tokens) - 8}ê°œ í† í° ìƒëµ)")
        
        # í•œê¸€ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª…
        if any(ord(c) > 127 for c in text):
            # ì›ë³¸ í…ìŠ¤íŠ¸ì˜ UTF-8 ë°”ì´íŠ¸ í‘œí˜„
            original_bytes = text.encode('utf-8')
            
            print(f"\n  [!] í•œê¸€ í† í°í™” ìƒì„¸ ì„¤ëª…:")
            print(f"     ì›ë³¸ UTF-8 ë°”ì´íŠ¸: {original_bytes[:30]}{'...' if len(original_bytes) > 30 else ''}")
            print(f"     ì´ {len(original_bytes)} ë°”ì´íŠ¸ (í•œê¸€ 1ê¸€ì = 3ë°”ì´íŠ¸)")
            print(f"")
            print(f"     BPE í† í°í™” ê³¼ì •:")
            print(f"     - BPEëŠ” ë°”ì´íŠ¸ ë ˆë²¨ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” íŒ¨í„´ì„ í•™ìŠµ")
            print(f"     - í•œê¸€ì€ ì˜ì–´ë³´ë‹¤ í•™ìŠµ ë¹ˆë„ê°€ ë‚®ì•„ ë” ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë¶„í•´")
            print(f"     - ì˜ˆ: 'ì•ˆ'(U+C548) = b'\\xec\\x95\\x88' â†’ 2ê°œ í† í°ìœ¼ë¡œ ë¶„í•´ë  ìˆ˜ ìˆìŒ")
            print(f"     - í† í° 1: b'\\xec\\x95' (ë¶ˆì™„ì „, 2ë°”ì´íŠ¸)")
            print(f"     - í† í° 2: b'\\x88' (ë‚˜ë¨¸ì§€ 1ë°”ì´íŠ¸)")
            print(f"")
            print(f"     [OK] ëª¨ë“  í† í°ì˜ ë°”ì´íŠ¸ë¥¼ ì—°ê²°í•˜ë©´ ì›ë³¸ ì™„ë²½ ë³µì›!")
            print(f"     [!] ê°œë³„ í† í°ì€ ìœ íš¨í•œ ë¬¸ìê°€ ì•„ë‹ ìˆ˜ ìˆìŒ (ì •ìƒ)")
    
    # í•µì‹¬ í¬ì¸íŠ¸
    print_key_points([
        "- tiktoken: OpenAI ê³µì‹ í† í° ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬",
        "- ëª¨ë¸ë§ˆë‹¤ ë‹¤ë¥¸ ì¸ì½”ë” ì‚¬ìš© (gpt-3.5-turbo, gpt-4 ë“±)",
        "- í•œê¸€ì€ ì˜ì–´ë³´ë‹¤ 2~3ë°° ë” ë§ì€ í† í° ì†Œëª¨",
        "- API ë¹„ìš© ì¶”ì •: 1K í† í° = $0.001~0.01 (ëª¨ë¸ë³„ ìƒì´)",
        "- ì‹¤ë¬´ íŒ: ê¸´ í•œê¸€ ë¬¸ì„œëŠ” í† í° ë¹„ìš© ë¯¸ë¦¬ ê³„ì‚°!"
    ], "tiktoken í•µì‹¬ í¬ì¸íŠ¸")


# ============================================================================
# 2. NLTK ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
# ============================================================================

class TextPreprocessor:
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))
    
    def tokenize(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë¶„ë¦¬"""
        return word_tokenize(text.lower())
    
    def remove_stopwords(self, tokens: List[str]) -> List[str]:
        """ë¶ˆìš©ì–´ ì œê±°"""
        return [token for token in tokens if token not in self.stop_words]
    
    def get_wordnet_pos(self, treebank_tag: str):
        """
        Penn Treebank í’ˆì‚¬ íƒœê·¸ë¥¼ WordNet í’ˆì‚¬ë¡œ ë³€í™˜
        
        Args:
            treebank_tag: Penn Treebank í˜•ì‹ì˜ í’ˆì‚¬ íƒœê·¸
        
        Returns:
            WordNet í’ˆì‚¬ ìƒìˆ˜
        """
        if treebank_tag.startswith('J'):
            return wordnet.ADJ
        elif treebank_tag.startswith('V'):
            return wordnet.VERB
        elif treebank_tag.startswith('N'):
            return wordnet.NOUN
        elif treebank_tag.startswith('R'):
            return wordnet.ADV
        else:
            return wordnet.NOUN  # ê¸°ë³¸ê°’ì€ ëª…ì‚¬
    
    def lemmatize(self, tokens: List[str]) -> List[str]:
        """
        í‘œì œì–´ ì¶”ì¶œ (lemmatization) - í’ˆì‚¬ íƒœê¹… í¬í•¨
        
        Args:
            tokens: í† í° ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ê¸°ë³¸í˜•ìœ¼ë¡œ ë³€í™˜ëœ í† í° ë¦¬ìŠ¤íŠ¸
        """
        # í’ˆì‚¬ íƒœê¹… ë¨¼ì € ìˆ˜í–‰
        pos_tags = pos_tag(tokens)
        
        # í’ˆì‚¬ ì •ë³´ë¥¼ í™œìš©í•œ lemmatization
        lemmatized = []
        for word, pos in pos_tags:
            wordnet_pos = self.get_wordnet_pos(pos)
            lemma = self.lemmatizer.lemmatize(word, pos=wordnet_pos)
            lemmatized.append(lemma)
        
        return lemmatized
    
    def preprocess(self, text: str, remove_stopwords: bool = True, 
                   lemmatize: bool = True) -> List[str]:
        """
        ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            text: ì „ì²˜ë¦¬í•  í…ìŠ¤íŠ¸
            remove_stopwords: ë¶ˆìš©ì–´ ì œê±° ì—¬ë¶€
            lemmatize: í‘œì œì–´ ì¶”ì¶œ ì—¬ë¶€
        
        Returns:
            ì „ì²˜ë¦¬ëœ í† í° ë¦¬ìŠ¤íŠ¸
        """
        # 1. í† í°í™”
        tokens = self.tokenize(text)
        
        # 2. ì•ŒíŒŒë²³ë§Œ ë‚¨ê¸°ê¸°
        tokens = [token for token in tokens if token.isalpha()]
        
        # 3. ë¶ˆìš©ì–´ ì œê±°
        if remove_stopwords:
            tokens = self.remove_stopwords(tokens)
        
        # 4. í‘œì œì–´ ì¶”ì¶œ
        if lemmatize:
            tokens = self.lemmatize(tokens)
        
        return tokens


def demo_preprocessing():
    """ì‹¤ìŠµ 2: NLTK ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    print("\n" + "="*80)
    print("[2] ì‹¤ìŠµ 2: NLTK ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸")
    print("="*80)
    print("ëª©í‘œ: í…ìŠ¤íŠ¸ ì •ê·œí™”ì˜ í•„ìš”ì„±ê³¼ ë°©ë²• ì´í•´")
    print("í•µì‹¬: í† í°í™” -> ì •ê·œí™” -> ë¶ˆìš©ì–´ ì œê±° -> í‘œì œì–´ ì¶”ì¶œ")
    
    # ì „ì²˜ë¦¬ë€?
    print_section_header("í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë€?", "[DOC]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [TIP] ì™œ ì „ì²˜ë¦¬ê°€ í•„ìš”í•œê°€?                             â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  â€¢ "Running", "runs", "ran" -> ëª¨ë‘ "run"ì˜ ë³€í˜•         â”‚
  â”‚  â€¢ "the", "is", "a" -> ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ (ë¶ˆìš©ì–´)           â”‚
  â”‚  â€¢ ëŒ€ì†Œë¬¸ì í†µì¼ -> "AI" = "ai" = "Ai"                   â”‚
  â”‚                                                         â”‚
  â”‚  ì „ì²˜ë¦¬ ì—†ì´ ê²€ìƒ‰í•˜ë©´?                                   â”‚
  â”‚  â€¢ "cats" ê²€ìƒ‰ ì‹œ "cat" ë¬¸ì„œ ë†“ì¹¨                        â”‚
  â”‚  â€¢ "THE CAT" vs "the cat" ë‹¤ë¥´ê²Œ ì¸ì‹                    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    preprocessor = TextPreprocessor()
    
    text = "The cats are running quickly through the beautiful gardens and jumping over fences."
    
    print_section_header("ë‹¨ê³„ë³„ ì „ì²˜ë¦¬ ê³¼ì •", "[STEP]")
    print(f"\nì›ë³¸ í…ìŠ¤íŠ¸: {text}")
    
    # 1ë‹¨ê³„: í† í°í™”
    print_subsection("1ë‹¨ê³„: í† í°í™” (Tokenization)")
    tokens = preprocessor.tokenize(text)
    print(f"  ê²°ê³¼: {tokens}")
    print(f"  ì„¤ëª…: ë¬¸ì¥ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬, ì†Œë¬¸ì ë³€í™˜")
    
    # 2ë‹¨ê³„: ì•ŒíŒŒë²³ë§Œ ë‚¨ê¸°ê¸°
    print_subsection("2ë‹¨ê³„: ì•ŒíŒŒë²³ í•„í„°ë§")
    alpha_tokens = [token for token in tokens if token.isalpha()]
    print(f"  ê²°ê³¼: {alpha_tokens}")
    print(f"  ì„¤ëª…: êµ¬ë‘ì (., !) ì œê±°")
    
    # 3ë‹¨ê³„: ë¶ˆìš©ì–´ ì œê±°
    print_subsection("3ë‹¨ê³„: ë¶ˆìš©ì–´ ì œê±° (Stopword Removal)")
    no_stop = preprocessor.remove_stopwords(alpha_tokens)
    removed = [t for t in alpha_tokens if t not in no_stop]
    print(f"  ê²°ê³¼: {no_stop}")
    print(f"  ì œê±°ë¨: {removed}")
    print(f"  ì„¤ëª…: 'the', 'are', 'and' ë“± ì˜ë¯¸ ì—†ëŠ” ë‹¨ì–´ ì œê±°")
    
    # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì•ˆë‚´
    stop_words_sample = sorted(list(preprocessor.stop_words))[:15]
    print(f"\n  [INFO] ì˜ì–´ ë¶ˆìš©ì–´ (ì´ {len(preprocessor.stop_words)}ê°œ):")
    print(f"  ì˜ˆì‹œ: {stop_words_sample}...")
    print(f"  [TIP] ì „ì²´ ëª©ë¡: nltk.corpus.stopwords.words('english')")
    
    # 4ë‹¨ê³„: í‘œì œì–´ ì¶”ì¶œ (í’ˆì‚¬ íƒœê¹… í¬í•¨)
    print_subsection("4ë‹¨ê³„: í‘œì œì–´ ì¶”ì¶œ (Lemmatization + POS íƒœê¹…)")
    
    # í’ˆì‚¬ íƒœê¹… ê²°ê³¼ ë¨¼ì € ë³´ì—¬ì£¼ê¸°
    pos_tags = pos_tag(no_stop)
    print(f"  í’ˆì‚¬ íƒœê¹…: {pos_tags}")
    print(f"""
  Penn Treebank í’ˆì‚¬ íƒœê·¸ ì„¤ëª…:
    â€¢ NN/NNS  = ëª…ì‚¬ ë‹¨ìˆ˜/ë³µìˆ˜ (Noun singular/plural)
    â€¢ VB/VBG  = ë™ì‚¬ ê¸°ë³¸í˜•/í˜„ì¬ë¶„ì‚¬ (Verb base/gerund)
    â€¢ VBD/VBN = ë™ì‚¬ ê³¼ê±°í˜•/ê³¼ê±°ë¶„ì‚¬ (Verb past/past participle)
    â€¢ JJ      = í˜•ìš©ì‚¬ (Adjective)
    â€¢ RB      = ë¶€ì‚¬ (Adverb)
    â€¢ IN      = ì „ì¹˜ì‚¬ (Preposition)""")
    
    lemmatized = preprocessor.lemmatize(no_stop)
    
    # ë³€í™”ëœ ë‹¨ì–´ ê°•ì¡°
    changes = []
    for orig, lem in zip(no_stop, lemmatized):
        if orig != lem:
            changes.append(f"'{orig}' -> '{lem}'")
    
    print(f"\n  ê²°ê³¼: {lemmatized}")
    if changes:
        print(f"  ë³€í™˜ë¨: {', '.join(changes)}")
    print(f"  ì„¤ëª…: í’ˆì‚¬ì— ë”°ë¼ ê¸°ë³¸í˜•ìœ¼ë¡œ ë³€í™˜ (ë™ì‚¬ running->run, ëª…ì‚¬ cats->cat)")
    
    # Lemmatization vs Stemming ë¹„êµ
    print_section_header("Lemmatization vs Stemming", "[vs]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [CMP] ë¹„êµ                                              â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  ë‹¨ì–´        â”‚ Stemming     â”‚ Lemmatization            â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  running     â”‚ runn         â”‚ run (ë™ì‚¬ ê¸°ë³¸í˜•)         â”‚
  â”‚  flies       â”‚ fli          â”‚ fly (ë™ì‚¬ ê¸°ë³¸í˜•)         â”‚
  â”‚  studies     â”‚ studi        â”‚ study (ëª…ì‚¬ ê¸°ë³¸í˜•)       â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  íŠ¹ì§•        â”‚ ë¹ ë¦„, ê·œì¹™ ê¸°ë°˜â”‚ ì •í™•, ì‚¬ì „ ê¸°ë°˜          â”‚
  â”‚  ë‹¨ì         â”‚ ë¹„ë¬¸ë²•ì  ê²°ê³¼  â”‚ ëŠë¦¼, í’ˆì‚¬ í•„ìš”          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  [!] ì¤‘ìš”: Lemmatizationì€ í’ˆì‚¬(POS) íƒœê¹…ì´ í•„ìˆ˜!
      - í’ˆì‚¬ ì—†ì´ ì‹¤í–‰í•˜ë©´ ëª¨ë“  ë‹¨ì–´ë¥¼ ëª…ì‚¬ë¡œ ê°„ì£¼
      - running(ë™ì‚¬) â†’ running (ë³€í™˜ ì•ˆë¨) â† ì˜ëª»ëœ ê²°ê³¼
      - running(ë™ì‚¬) + POS íƒœê¹… â†’ run â† ì˜¬ë°”ë¥¸ ê²°ê³¼
      
  [!] Lemmatizationì˜ í•œê³„:
      - ë¹„êµê¸‰/ìµœìƒê¸‰ ì²˜ë¦¬ ì•ˆë¨ (better â†’ good ë³€í™˜ ë¶ˆê°€)
      - ë¶ˆê·œì¹™ ë³€í™” ì¼ë¶€ ë¯¸ì§€ì›
      - ì™„ë²½í•œ ë³€í™˜ì„ ìœ„í•´ì„  ê·œì¹™ ê¸°ë°˜ ì¶”ê°€ ì²˜ë¦¬ í•„ìš”
    """)
    
    # Lemmatization í•œê³„ ì‹¤ì œ í…ŒìŠ¤íŠ¸
    print_subsection("Lemmatization í•œê³„ í…ŒìŠ¤íŠ¸")
    print("  [ì‹¤í—˜] ë¹„êµê¸‰/ìµœìƒê¸‰/ë¶ˆê·œì¹™ ë³€í™” ë‹¨ì–´ í…ŒìŠ¤íŠ¸:\n")
    
    # í…ŒìŠ¤íŠ¸í•  ë‹¨ì–´ë“¤ (ê¸°ëŒ€ê°’ê³¼ í•¨ê»˜)
    test_words = [
        ("better", "good", "ë¹„êµê¸‰ â†’ ì›ê¸‰"),
        ("best", "good", "ìµœìƒê¸‰ â†’ ì›ê¸‰"),
        ("worse", "bad", "ë¹„êµê¸‰ â†’ ì›ê¸‰"),
        ("worst", "bad", "ìµœìƒê¸‰ â†’ ì›ê¸‰"),
        ("running", "run", "í˜„ì¬ë¶„ì‚¬ â†’ ê¸°ë³¸í˜•"),
        ("ran", "run", "ê³¼ê±°í˜• â†’ ê¸°ë³¸í˜•"),
        ("went", "go", "ë¶ˆê·œì¹™ ê³¼ê±° â†’ ê¸°ë³¸í˜•"),
        ("children", "child", "ë¶ˆê·œì¹™ ë³µìˆ˜ â†’ ë‹¨ìˆ˜"),
    ]
    
    print(f"  {'ì›ë³¸':<12} {'ê¸°ëŒ€ê°’':<10} {'ì‹¤ì œê²°ê³¼':<12} {'ì„±ê³µì—¬ë¶€':<8} ì„¤ëª…")
    print(f"  {'â”€'*60}")
    
    lemmatizer = preprocessor.lemmatizer
    success_count = 0
    
    for word, expected, description in test_words:
        # í’ˆì‚¬ íƒœê¹… í›„ lemmatization
        pos_tags = pos_tag([word])
        wordnet_pos = preprocessor.get_wordnet_pos(pos_tags[0][1])
        result = lemmatizer.lemmatize(word, pos=wordnet_pos)
        
        is_success = result == expected
        if is_success:
            success_count += 1
        
        status = "[v]" if is_success else "[x]"
        print(f"  {word:<12} {expected:<10} {result:<12} {status:<8} {description}")
    
    print(f"\n  [ê²°ê³¼] {success_count}/{len(test_words)}ê°œ ì„±ê³µ ({success_count/len(test_words)*100:.0f}%)")
    print(f"""
  [!] ê²°ë¡ : NLTK LemmatizerëŠ” ë¶ˆê·œì¹™ ë³€í™”ì— ì•½í•©ë‹ˆë‹¤!
      - ë¹„êµê¸‰/ìµœìƒê¸‰: better â†’ good, best â†’ good ë³€í™˜ ë¶ˆê°€
      - ì¼ë¶€ ë¶ˆê·œì¹™ ê³¼ê±°: ran â†’ run ë³€í™˜ ì‹¤íŒ¨
      - went â†’ goëŠ” ì„±ê³µí•˜ëŠ” ê²½ìš° ìˆìŒ (WordNet ì‚¬ì „ì— ë“±ë¡ëœ ê²½ìš°)
      - ì´ëŸ° ë¶ˆì¼ì¹˜ëŠ” WordNet ì‚¬ì „ì˜ ì»¤ë²„ë¦¬ì§€ ì°¨ì´ ë•Œë¬¸
      
  [TIP] ì‹¤ë¬´ ëŒ€ì•ˆ:
      - SpaCyì˜ lemmatizer (ë” ì •í™•í•˜ê³  ì¼ê´€ì„± ìˆìŒ)
      - ì»¤ìŠ¤í…€ ë§¤í•‘ í…Œì´ë¸” ì‚¬ìš© (ë¹„êµê¸‰/ìµœìƒê¸‰ ì „ìš©)
      - ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ì— ë§¡ê¸°ê¸° (ì „ì²˜ë¦¬ ìƒëµ)
    """)
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²°ê³¼
    print_subsection("ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²°ê³¼")
    result = preprocessor.preprocess(text)
    print(f"  ì›ë³¸: {text}")
    print(f"  ê²°ê³¼: {result}")
    print(f"  í† í° ìˆ˜: {len(text.split())} -> {len(result)} (ì•½ {(1-len(result)/len(text.split()))*100:.0f}% ê°ì†Œ)")
    
    # í•µì‹¬ í¬ì¸íŠ¸
    print_key_points([
        "- í† í°í™”: í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„ë¦¬",
        "- ë¶ˆìš©ì–´ ì œê±°: ì˜ë¯¸ ì—†ëŠ” ê³ ë¹ˆë„ ë‹¨ì–´ ì œê±° (the, is, a...)",
        "- í‘œì œì–´ ì¶”ì¶œ: ë‹¨ì–´ë¥¼ ì‚¬ì „ ê¸°ë³¸í˜•ìœ¼ë¡œ ë³€í™˜ (í’ˆì‚¬ íƒœê¹… í•„ìˆ˜!)",
        "- POS íƒœê¹…: ë™ì‚¬/í˜•ìš©ì‚¬/ë¶€ì‚¬ êµ¬ë¶„ì´ ìˆì–´ì•¼ ì •í™•í•œ lemmatization",
        "- ìš©ë„: í‚¤ì›Œë“œ ì¶”ì¶œ, BM25 ê²€ìƒ‰, í…ìŠ¤íŠ¸ ë¶„ì„"
    ], "ì „ì²˜ë¦¬ í•µì‹¬ í¬ì¸íŠ¸")
    
    # ì¤‘ìš” ì£¼ì˜ì‚¬í•­: ê²€ìƒ‰ ë°©ì‹ë³„ ì „ì²˜ë¦¬ í•„ìš”ì„±
    print_section_header("âš ï¸ ì¤‘ìš”: ê²€ìƒ‰ ë°©ì‹ë³„ ì „ì²˜ë¦¬ í•„ìš”ì„±", "[WARN]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [!] ì´ˆë³´ìê°€ ìì£¼ í˜¼ë™í•˜ëŠ” í•µì‹¬ í¬ì¸íŠ¸!                                 â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
  â”‚  â”‚  BM25 / í‚¤ì›Œë“œ ê²€ìƒ‰         â”‚  ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ (Semantic)      â”‚   â”‚
  â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚
  â”‚  â”‚  ì „ì²˜ë¦¬ í•„ìˆ˜! âœ“             â”‚  ì „ì²˜ë¦¬ ë¶ˆí•„ìš” (ì˜¤íˆë ¤ í•´ë¡œì›€!) âœ— â”‚   â”‚
  â”‚  â”‚                             â”‚                                   â”‚   â”‚
  â”‚  â”‚  ì´ìœ :                      â”‚  ì´ìœ :                            â”‚   â”‚
  â”‚  â”‚  â€¢ ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­ í•„ìš”    â”‚  â€¢ ì„ë² ë”© ëª¨ë¸ì´ ë¬¸ë§¥ íŒŒì•…        â”‚   â”‚
  â”‚  â”‚  â€¢ "cats" â‰  "cat" (ë‹¤ë¥¸ ë‹¨ì–´)â”‚  â€¢ ì›ë¬¸ ê·¸ëŒ€ë¡œê°€ ì˜ë¯¸ ë³´ì¡´       â”‚   â”‚
  â”‚  â”‚  â€¢ ë¶ˆìš©ì–´ê°€ ë…¸ì´ì¦ˆë¡œ ì‘ìš©   â”‚  â€¢ ì „ì²˜ë¦¬ ì‹œ ì˜ë¯¸ ì†ì‹¤ ê°€ëŠ¥!      â”‚   â”‚
  â”‚  â”‚                             â”‚                                   â”‚   â”‚
  â”‚  â”‚  ì˜ˆì‹œ:                      â”‚  ì˜ˆì‹œ:                            â”‚   â”‚
  â”‚  â”‚  "The cats are running"    â”‚  "The cats are running"           â”‚   â”‚
  â”‚  â”‚  â†’ ["cat", "run"]          â”‚  â†’ ê·¸ëŒ€ë¡œ ì„ë² ë”© ìƒì„±             â”‚   â”‚
  â”‚  â”‚  (ì „ì²˜ë¦¬ í›„ ê²€ìƒ‰)           â”‚  (ì›ë¬¸ ê·¸ëŒ€ë¡œ ê²€ìƒ‰)               â”‚   â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
  â”‚                                                                         â”‚
  â”‚  [!] ì‹¤ìˆ˜ ì‚¬ë¡€:                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  "ì„ë² ë”© ê²€ìƒ‰í•˜ë ¤ê³  ì „ì²˜ë¦¬í–ˆë”ë‹ˆ ê²€ìƒ‰ í’ˆì§ˆì´ ë–¨ì–´ì¡Œì–´ìš”"                â”‚
  â”‚  â†’ ì›ì¸: ë¶ˆìš©ì–´ ì œê±°ë¡œ "not", "no" ê°™ì€ ë¶€ì •ì–´ë„ ì œê±°ë¨                 â”‚
  â”‚  â†’ "I love this" vs "I do not love this" êµ¬ë¶„ ë¶ˆê°€!                    â”‚
  â”‚                                                                         â”‚
  â”‚  [ê²°ë¡ ]                                                                 â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  â€¢ BM25/TF-IDF ê²€ìƒ‰ â†’ ì „ì²˜ë¦¬ í•„ìˆ˜ (lab03 Hybrid ê²€ìƒ‰ì—ì„œ ì‚¬ìš©)         â”‚
  â”‚  â€¢ OpenAI/Sentence Transformers ì„ë² ë”© â†’ ì „ì²˜ë¦¬ í•˜ì§€ ë§ˆì„¸ìš”!           â”‚
  â”‚  â€¢ ì´ ì‹¤ìŠµì€ ì „ì²˜ë¦¬ì˜ "ê°œë…"ì„ ì´í•´í•˜ê¸° ìœ„í•œ ê²ƒ (ì„ë² ë”©ì— ì ìš© X)       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


# ============================================================================
# 3. OpenAI ì„ë² ë”© ìƒì„±
# ============================================================================

class EmbeddingGenerator:
    """OpenAI ì„ë² ë”© ìƒì„±ê¸°"""
    
    def __init__(self, api_key: str = None):
        # ê³µí†µ í—¬í¼ ì‚¬ìš© (SSL ì¸ì¦ì„œ ê²€ì¦ ìš°íšŒ í¬í•¨)
        self.client = get_openai_client(api_key)
        self.model = "text-embedding-3-small"
    
    def get_embedding(self, text: str) -> List[float]:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±
        
        Args:
            text: ì„ë² ë”©ì„ ìƒì„±í•  í…ìŠ¤íŠ¸
        
        Returns:
            ì„ë² ë”© ë²¡í„°
        
        Raises:
            Exception: API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
        """
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"\n[!] ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            print(f"[TIP] í™•ì¸ ì‚¬í•­:")
            print(f"     1. OPENAI_API_KEYê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
            print(f"     2. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸")
            print(f"     3. API ì‚¬ìš©ëŸ‰ í•œë„ í™•ì¸")
            raise
    
    def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë°°ì¹˜ë¡œ ìƒì„±
        
        Args:
            texts: ì„ë² ë”©ì„ ìƒì„±í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        
        Raises:
            Exception: API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
        """
        try:
            response = self.client.embeddings.create(
                model=self.model,
                input=texts
            )
            return [data.embedding for data in response.data]
        except Exception as e:
            print(f"\n[!] ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            print(f"[TIP] í™•ì¸ ì‚¬í•­:")
            print(f"     1. OPENAI_API_KEYê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
            print(f"     2. ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœ í™•ì¸")
            print(f"     3. ë°°ì¹˜ í¬ê¸°ê°€ ë„ˆë¬´ í¬ì§€ ì•Šì€ì§€ í™•ì¸ (ìµœëŒ€ 2048ê°œ)")
            raise


def demo_embeddings():
    """ì‹¤ìŠµ 3: OpenAI ì„ë² ë”© ìƒì„±"""
    print("\n" + "="*80)
    print("[3] ì‹¤ìŠµ 3: OpenAI ì„ë² ë”© ìƒì„±")
    print("="*80)
    print("ëª©í‘œ: í…ìŠ¤íŠ¸ê°€ ì–´ë–»ê²Œ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜ë˜ëŠ”ì§€ ì´í•´")
    print("í•µì‹¬: ì˜ë¯¸ê°€ ë¹„ìŠ·í•œ í…ìŠ¤íŠ¸ -> ë¹„ìŠ·í•œ ë²¡í„° -> ê°€ê¹Œìš´ ê±°ë¦¬")
    
    # ì„ë² ë”©ì´ë€?
    print_section_header("ì„ë² ë”©(Embedding)ì´ë€?", "[INFO]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [TIP] ì„ë² ë”©ì˜ ê°œë…                                     â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  â€¢ í…ìŠ¤íŠ¸ -> ê³ ì • ê¸¸ì´ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜                  â”‚
  â”‚  â€¢ ì˜ˆ: "ê³ ì–‘ì´" -> [0.1, -0.3, 0.5, ..., 0.2] (1536ì°¨ì›) â”‚
  â”‚                                                         â”‚
  â”‚  ì™œ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ”ê°€?                                   â”‚
  â”‚  â€¢ ì»´í“¨í„°ëŠ” ìˆ«ìë§Œ ì—°ì‚° ê°€ëŠ¥                             â”‚
  â”‚  â€¢ ë²¡í„° ê³µê°„ì—ì„œ ì˜ë¯¸ì  ìœ ì‚¬ì„± ì¸¡ì • ê°€ëŠ¥                 â”‚
  â”‚  â€¢ "ì™• - ë‚¨ì + ì—¬ì = ì—¬ì™•" ê°™ì€ ì—°ì‚° ê°€ëŠ¥             â”‚
  â”‚                                                         â”‚
  â”‚  OpenAI ì„ë² ë”© ëª¨ë¸:                                     â”‚
  â”‚  â€¢ text-embedding-3-small: 1536ì°¨ì›, ì €ë ´, ë¹ ë¦„          â”‚
  â”‚  â€¢ text-embedding-3-large: 3072ì°¨ì›, ê³ ì„±ëŠ¥              â”‚
  â”‚                                                         â”‚
  â”‚  [TIP] ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ:                                  â”‚
  â”‚  â€¢ small: ê²€ìƒ‰/RAG ëŒ€ë¶€ë¶„ì˜ ìš©ë„ì— ì¶©ë¶„                  â”‚
  â”‚  â€¢ large: ë²•ë¥ /ì˜í•™/ë…¼ë¬¸ê¸‰ ì˜ë¯¸ ì •ë°€ë„ê°€ í•„ìš”í•  ë•Œë§Œ     â”‚
  â”‚           (ë¹„ìš© 2ë°°, ì„±ëŠ¥ í–¥ìƒì€ ë„ë©”ì¸ì— ë”°ë¼ 5~15%)    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        print("\n[!] OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        return
    
    generator = EmbeddingGenerator()
    
    # ë‹¨ì¼ ì„ë² ë”©
    print_section_header("ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©", "[DOC]")
    
    text = "Artificial intelligence is transforming the world."
    embedding = generator.get_embedding(text)
    
    print(f"\ní…ìŠ¤íŠ¸: '{text}'")
    print(f"\nì„ë² ë”© ê²°ê³¼:")
    print(f"  â€¢ ë²¡í„° ì°¨ì›: {len(embedding)}")
    print(f"  â€¢ ì²˜ìŒ 5ê°œ ê°’: {[round(v, 4) for v in embedding[:5]]}")
    print(f"  â€¢ ë§ˆì§€ë§‰ 5ê°œ ê°’: {[round(v, 4) for v in embedding[-5:]]}")
    print(f"  â€¢ ê°’ì˜ ë²”ìœ„: [{min(embedding):.4f}, {max(embedding):.4f}]")
    
    # ë²¡í„° ì‹œê°í™” (ê°„ë‹¨í•œ íˆìŠ¤í† ê·¸ë¨)
    print(f"\n  ê°’ ë¶„í¬ ì‹œê°í™”:")
    bins = [0, 0, 0, 0, 0]  # -0.1~-0.05, -0.05~0, 0~0.05, 0.05~0.1, ê¸°íƒ€
    for v in embedding:
        if v < -0.05:
            bins[0] += 1
        elif v < 0:
            bins[1] += 1
        elif v < 0.05:
            bins[2] += 1
        elif v < 0.1:
            bins[3] += 1
        else:
            bins[4] += 1
    
    labels = ["< -0.05", "-0.05~0", "0~0.05", "0.05~0.1", "> 0.1"]
    max_bin = max(bins)
    for label, count in zip(labels, bins):
        bar_len = int(count / max_bin * 30)
        print(f"    {label:>10}: {'#' * bar_len} ({count})")
    
    # L2 ë…¸ë¦„ ê³„ì‚°
    l2_norm = np.sqrt(sum(v**2 for v in embedding))
    
    # OpenAI ì„ë² ë”© íŠ¹ì„± ì„¤ëª…
    print(f"""
  [!] OpenAI ì„ë² ë”© íŠ¹ì„±:
     â€¢ L2 ì •ê·œí™”ë¨: ë²¡í„° í¬ê¸°(L2 ë…¸ë¦„) = {l2_norm:.4f} (â‰ˆ 1.0)
     â€¢ ëŒ€ë¶€ë¶„ ê°’ì´ -0.1 ~ 0.1 ì‚¬ì´ì— ë¶„í¬
     â€¢ {len(embedding)}ì°¨ì›ì´ë¯€ë¡œ ê°œë³„ ê°’ì€ 0ì— ê°€ê¹Œì›€
     â€¢ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì— ìµœì í™”ëœ í˜•íƒœ
     â€¢ ì •ê·œí™” ë•ë¶„ì— ë‚´ì (dot product)ë§Œìœ¼ë¡œ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥""")
    
    # ë°°ì¹˜ ì„ë² ë”©
    print_section_header("ë°°ì¹˜ ì„ë² ë”© (íš¨ìœ¨ì ì¸ ë°©ë²•)", "[BATCH]")
    
    texts = [
        "I love machine learning.",
        "Deep learning is a subset of AI.",
        "Python is a great programming language."
    ]
    
    print("\n[DOC] ì„ë² ë”© ìƒì„± ì½”ë“œ:")
    print("  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print("  â”‚ # ë¹„íš¨ìœ¨ì : ê°œë³„ í˜¸ì¶œ (3ë²ˆ API í˜¸ì¶œ, ì§€ì—° ì‹œê°„ 3ë°°)")
    print("  â”‚ embeddings = []")
    print("  â”‚ for text in texts:")
    print("  â”‚     response = client.embeddings.create(input=text)")
    print("  â”‚     embeddings.append(response.data[0].embedding)")
    print("  â”‚")
    print("  â”‚ # íš¨ìœ¨ì : ë°°ì¹˜ í˜¸ì¶œ (1ë²ˆ API í˜¸ì¶œ, ë¹„ìš© ë™ì¼)")
    print("  â”‚ response = client.embeddings.create(input=texts)")
    print("  â”‚ embeddings = [data.embedding for data in response.data]")
    print("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    
    embeddings = generator.get_embeddings_batch(texts)
    
    print(f"\në°°ì¹˜ ì„ë² ë”© ê²°ê³¼:")
    for i, (text, emb) in enumerate(zip(texts, embeddings)):
        print(f"  {i+1}. '{text}'")
        print(f"     ì°¨ì›: {len(emb)}, ì²˜ìŒ 5ê°œ: {[round(v, 4) for v in emb[:5]]}")
    
    # í•µì‹¬ í¬ì¸íŠ¸
    print_key_points([
        "- ì„ë² ë”©: í…ìŠ¤íŠ¸ -> ê³ ì°¨ì› ë²¡í„° (ì˜ë¯¸ë¥¼ ìˆ«ìë¡œ ì¸ì½”ë”©)",
        "- text-embedding-3-small: 1536ì°¨ì›, ê²€ìƒ‰/RAG ëŒ€ë¶€ë¶„ì— ì¶©ë¶„",
        "- text-embedding-3-large: ë²•ë¥ /ì˜í•™/ë…¼ë¬¸ê¸‰ ì •ë°€ë„ í•„ìš”ì‹œë§Œ ì‚¬ìš©",
        "- ë°°ì¹˜ ì²˜ë¦¬: ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— -> API í˜¸ì¶œ ìµœì†Œí™”, ë¹„ìš© ì ˆì•½",
        "- ë¹„ìš©: small ~$0.00002/1Kí† í°, large ~$0.00013/1Kí† í°",
        "- ìš©ë„: ìœ ì‚¬ë„ ê²€ìƒ‰, í´ëŸ¬ìŠ¤í„°ë§, ë¶„ë¥˜, RAG"
    ], "ì„ë² ë”© í•µì‹¬ í¬ì¸íŠ¸")


# ============================================================================
# 4. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (utils.pyì—ì„œ importí•œ í•¨ìˆ˜ ì‚¬ìš©)
# ============================================================================
# Note: cosine_similarity, cosine_similarity_normalized í•¨ìˆ˜ëŠ” utils.pyì— ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
# from utils import cosine_similarity, cosine_similarity_normalized, is_normalized


def one_to_many_similarity(query_embedding: List[float], 
                          document_embeddings: List[List[float]]) -> List[float]:
    """
    1:N ìœ ì‚¬ë„ ê³„ì‚° (í•˜ë‚˜ì˜ ì¿¼ë¦¬ì™€ ì—¬ëŸ¬ ë¬¸ì„œ)
    
    Args:
        query_embedding: ì¿¼ë¦¬ ì„ë² ë”©
        document_embeddings: ë¬¸ì„œ ì„ë² ë”© ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ê° ë¬¸ì„œì™€ì˜ ìœ ì‚¬ë„ ë¦¬ìŠ¤íŠ¸
    """
    similarities = []
    for doc_emb in document_embeddings:
        sim = cosine_similarity(query_embedding, doc_emb)
        similarities.append(sim)
    return similarities


def many_to_many_similarity(embeddings1: List[List[float]], 
                           embeddings2: List[List[float]]) -> np.ndarray:
    """
    N:M ìœ ì‚¬ë„ ê³„ì‚° (ì—¬ëŸ¬ ì¿¼ë¦¬ì™€ ì—¬ëŸ¬ ë¬¸ì„œ)
    
    Args:
        embeddings1: ì²« ë²ˆì§¸ ì„ë² ë”© ë¦¬ìŠ¤íŠ¸
        embeddings2: ë‘ ë²ˆì§¸ ì„ë² ë”© ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ìœ ì‚¬ë„ í–‰ë ¬ (N x M)
    """
    matrix = np.zeros((len(embeddings1), len(embeddings2)))
    
    for i, emb1 in enumerate(embeddings1):
        for j, emb2 in enumerate(embeddings2):
            matrix[i][j] = cosine_similarity(emb1, emb2)
    
    return matrix


def demo_similarity():
    """ì‹¤ìŠµ 4: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    print("\n" + "="*80)
    print("[4] ì‹¤ìŠµ 4: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°")
    print("="*80)
    print("ëª©í‘œ: ë²¡í„° ê°„ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ëŠ” ë°©ë²• ì´í•´")
    print("í•µì‹¬: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = ë²¡í„° ë°©í–¥ì˜ ìœ ì‚¬ì„± (í¬ê¸° ë¬´ê´€)")
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë€?
    print_section_header("ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë€?", "[INFO]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [TIP] ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³µì‹                                â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚              A Â· B           Î£(Aáµ¢ Ã— Báµ¢)                 â”‚
  â”‚   cos Î¸ = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  =  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
  â”‚            |A| Ã— |B|      âˆš(Î£Aáµ¢Â²) Ã— âˆš(Î£Báµ¢Â²)            â”‚
  â”‚                                                         â”‚
  â”‚  êµ¬ì„± ìš”ì†Œ:                                              â”‚
  â”‚  â€¢ A Â· B   : ë‘ ë²¡í„°ì˜ ë‚´ì  (dot product)               â”‚
  â”‚  â€¢ |A|, |B|: ë²¡í„°ì˜ L2 ë…¸ë¦„ (í¬ê¸°, magnitude)           â”‚
  â”‚  â€¢ Î¸      : ë‘ ë²¡í„° ì‚¬ì´ì˜ ê°ë„                         â”‚
  â”‚                                                         â”‚
  â”‚  ê°’ì˜ ë²”ìœ„ (-1 ~ +1):                                    â”‚
  â”‚  â€¢ +1 : ì™„ì „íˆ ê°™ì€ ë°©í–¥ (ë§¤ìš° ìœ ì‚¬)                     â”‚
  â”‚  â€¢  0 : ì§ê° (ê´€ë ¨ ì—†ìŒ)                                â”‚
  â”‚  â€¢ -1 : ë°˜ëŒ€ ë°©í–¥ (ì‹¤ì œ ì„ë² ë”©ì—ì„œëŠ” ê±°ì˜ ì—†ìŒ)          â”‚
  â”‚                                                         â”‚
  â”‚  [TIP] ì‹¤ë¬´ í•´ì„ ê¸°ì¤€:                                   â”‚
  â”‚  â€¢ 0.8+ : ë§¤ìš° ìœ ì‚¬ (ê±°ì˜ ê°™ì€ ì˜ë¯¸)                     â”‚
  â”‚  â€¢ 0.6~0.8 : ê´€ë ¨ ìˆìŒ                                  â”‚
  â”‚  â€¢ 0.4~0.6 : ì•½ê°„ ê´€ë ¨                                  â”‚
  â”‚  â€¢ 0.4 ë¯¸ë§Œ : ë‹¤ë¥¸ ì£¼ì œ                                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    if not os.getenv("OPENAI_API_KEY"):
        print("\n[!] OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        return
    
    generator = EmbeddingGenerator()
    
    # ë¬¸ì¥ ì¤€ë¹„
    sentences = [
        "I love programming in Python.",
        "Python is my favorite programming language.",
        "I enjoy cooking Italian food.",
        "Machine learning is fascinating.",
    ]
    
    # ì„ë² ë”© ìƒì„±
    embeddings = generator.get_embeddings_batch(sentences)
    
    # 1:N ìœ ì‚¬ë„ ê³„ì‚°
    print_section_header("1:N ìœ ì‚¬ë„ ê³„ì‚°", "[>>>]")
    
    query = "I like coding with Python."
    query_embedding = generator.get_embedding(query)
    
    print(f"\nì¿¼ë¦¬: '{query}'")
    print(f"\nê° ë¬¸ì¥ê³¼ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„:")
    print(f"{'â”€'*60}")
    
    similarities = one_to_many_similarity(query_embedding, embeddings)
    
    # ê²°ê³¼ë¥¼ ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ í‘œì‹œ
    sorted_results = sorted(zip(sentences, similarities), key=lambda x: x[1], reverse=True)
    
    for sentence, sim in sorted_results:
        bar = visualize_similarity_bar(sim, 30)
        
        # ìœ ì‚¬ë„ í•´ì„ (utils.pyì˜ ê³µí†µ í•¨ìˆ˜ ì‚¬ìš©)
        interpretation = interpret_cosine_similarity(sim)
        
        print(f"\n  {bar} {sim:.4f} {interpretation}")
        print(f"  '{sentence}'")
    
    # ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥
    most_similar_idx = np.argmax(similarities)
    print(f"\n[#1] ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥: '{sentences[most_similar_idx]}'")
    print(f"     ìœ ì‚¬ë„: {similarities[most_similar_idx]:.4f}")
    
    # ì •ê·œí™”ëœ ë²¡í„° ë‚´ì  = ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‹¤ì¦
    print_section_header("ì •ê·œí™”ëœ ë²¡í„°: ë‚´ì  = ì½”ì‚¬ì¸ ìœ ì‚¬ë„", "[MATH]")
    
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [TIP] OpenAI ì„ë² ë”©ì˜ ë¹„ë°€                              â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  OpenAI ì„ë² ë”©ì€ L2 ì •ê·œí™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.                  â”‚
  â”‚  ì¦‰, ||A|| = ||B|| = 1.0                                â”‚
  â”‚                                                         â”‚
  â”‚  ë”°ë¼ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³µì‹ì´ ë‹¨ìˆœí•´ì§‘ë‹ˆë‹¤:                â”‚
  â”‚                                                         â”‚
  â”‚       A Â· B           A Â· B                             â”‚
  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  =  â”€â”€â”€â”€â”€â”€â”€â”€â”€  =  A Â· B                 â”‚
  â”‚    |A| Ã— |B|         1 Ã— 1                              â”‚
  â”‚                                                         â”‚
  â”‚  ê²°ë¡ : ì •ê·œí™”ëœ ë²¡í„°ì—ì„œ ë‚´ì ë§Œìœ¼ë¡œ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥!     â”‚
  â”‚        (ë‚˜ëˆ—ì…ˆ ì—°ì‚° ìƒëµ â†’ ë” ë¹ ë¥¸ ê³„ì‚°)                  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # ì‹¤ì œë¡œ ê²€ì¦
    print("[ì‹¤í—˜] ì •ê·œí™” ì—¬ë¶€ ë° ë‚´ì  vs ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¹„êµ:")
    print(f"{'â”€'*60}")
    
    # ì¿¼ë¦¬ ì„ë² ë”© ì •ê·œí™” í™•ì¸
    query_norm = np.linalg.norm(query_embedding)
    print(f"\n  ì¿¼ë¦¬ ë²¡í„° L2 ë…¸ë¦„: {query_norm:.6f}")
    print(f"  ì •ê·œí™” ì—¬ë¶€: {'[v] ì •ê·œí™”ë¨ (ë…¸ë¦„ â‰ˆ 1.0)' if is_normalized(query_embedding) else '[x] ì •ê·œí™” ì•ˆë¨'}")
    
    # ì²« ë²ˆì§¸ ë¬¸ì„œ ì„ë² ë”© ì •ê·œí™” í™•ì¸
    doc_norm = np.linalg.norm(embeddings[0])
    print(f"\n  ë¬¸ì„œ ë²¡í„° L2 ë…¸ë¦„: {doc_norm:.6f}")
    print(f"  ì •ê·œí™” ì—¬ë¶€: {'[v] ì •ê·œí™”ë¨ (ë…¸ë¦„ â‰ˆ 1.0)' if is_normalized(embeddings[0]) else '[x] ì •ê·œí™” ì•ˆë¨'}")
    
    # ë‚´ì  vs ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¹„êµ
    print(f"\n  ê³„ì‚° ë°©ë²• ë¹„êµ:")
    
    # ë°©ë²• 1: ì „ì²´ ê³µì‹ (ë‚˜ëˆ—ì…ˆ í¬í•¨)
    sim_full = cosine_similarity(query_embedding, embeddings[0])
    
    # ë°©ë²• 2: ë‚´ì ë§Œ (ì •ê·œí™”ëœ ë²¡í„°ìš©)
    sim_dot = cosine_similarity_normalized(query_embedding, embeddings[0])
    
    print(f"    ë°©ë²• 1 (ì „ì²´ ê³µì‹): {sim_full:.10f}")
    print(f"    ë°©ë²• 2 (ë‚´ì ë§Œ):    {sim_dot:.10f}")
    print(f"    ì°¨ì´:               {abs(sim_full - sim_dot):.2e}")
    
    if abs(sim_full - sim_dot) < 1e-6:
        print(f"\n  [v] ê²°ê³¼ê°€ ë™ì¼í•©ë‹ˆë‹¤! ì •ê·œí™”ëœ ë²¡í„°ì—ì„œëŠ” ë‚´ì ë§Œìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤.")
        print(f"      â†’ ì‹¤ë¬´ì—ì„œ ëŒ€ìš©ëŸ‰ ê²€ìƒ‰ ì‹œ ê³„ì‚° íš¨ìœ¨ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.")
    
    # N:M ìœ ì‚¬ë„ ê³„ì‚°
    print_section_header("N:M ìœ ì‚¬ë„ í–‰ë ¬", "[INFO]")
    
    queries = [
        "Programming languages",
        "Food and cooking"
    ]
    query_embeddings = generator.get_embeddings_batch(queries)
    
    similarity_matrix = many_to_many_similarity(query_embeddings, embeddings)
    
    print("\nìœ ì‚¬ë„ í–‰ë ¬:")
    print(f"{'â”€'*80}")
    
    # í—¤ë” ì¶œë ¥
    print(f"{'ì¿¼ë¦¬ \\ ë¬¸ì„œ':<20}", end="")
    for i in range(len(sentences)):
        print(f"Doc{i+1:2d}  ", end="")
    print()
    print(f"{'â”€'*80}")
    
    # ê° ì¿¼ë¦¬ë³„ ìœ ì‚¬ë„ ì¶œë ¥
    for i, query in enumerate(queries):
        print(f"{query:<20}", end="")
        for j in range(len(sentences)):
            score = similarity_matrix[i][j]
            # ë†’ì€ ì ìˆ˜ ê°•ì¡°
            if score >= 0.5:
                print(f"[{score:.4f}]", end="")
            else:
                print(f" {score:.4f} ", end="")
        print()
    
    print(f"{'â”€'*80}")
    
    # ë¬¸ì„œ ëª©ë¡ ì¶œë ¥
    print("\në¬¸ì„œ ëª©ë¡:")
    for i, sentence in enumerate(sentences):
        print(f"  Doc{i+1:2d}: {sentence}")
    
    # ê° ì¿¼ë¦¬ë³„ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ
    print("\n[*] ê° ì¿¼ë¦¬ë³„ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ:")
    for i, query in enumerate(queries):
        most_similar_idx = np.argmax(similarity_matrix[i])
        score = similarity_matrix[i][most_similar_idx]
        print(f"  '{query}'")
        print(f"    -> Doc{most_similar_idx+1}: '{sentences[most_similar_idx]}' ({score:.4f})")
    
    # í•µì‹¬ í¬ì¸íŠ¸
    print_key_points([
        "- ì½”ì‚¬ì¸ ìœ ì‚¬ë„: ë²¡í„° ë°©í–¥ì˜ ìœ ì‚¬ì„± ì¸¡ì • (-1 ~ 1)",
        "- ì„ë² ë”©ì—ì„œ: 0.8+ (ë§¤ìš° ìœ ì‚¬), 0.6~0.8 (ê´€ë ¨), 0.4~0.6 (ì•½ê°„ ê´€ë ¨), 0.4- (ë‹¤ë¥¸ ì£¼ì œ)",
        "- 1:N ê²€ìƒ‰: ì¿¼ë¦¬ vs ëª¨ë“  ë¬¸ì„œ -> ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°",
        "- N:M ê²€ìƒ‰: ì—¬ëŸ¬ ì¿¼ë¦¬ vs ì—¬ëŸ¬ ë¬¸ì„œ -> í–‰ë ¬ í˜•íƒœ ê²°ê³¼",
        "- ì‹¤ë¬´ íŒ: Vector DBëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ì´ ê³„ì‚°ì„ ìµœì í™”"
    ], "ìœ ì‚¬ë„ ê³„ì‚° í•µì‹¬ í¬ì¸íŠ¸")


# ============================================================================
# 5. ê°„ë‹¨í•œ ê²€ìƒ‰ ì—”ì§„
# ============================================================================

class SimpleSearchEngine:
    """ê°„ë‹¨í•œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„ (numpy ìµœì í™”)"""
    
    def __init__(self, api_key: str = None):
        self.generator = EmbeddingGenerator(api_key)
        self.documents: List[str] = []
        self.embeddings: np.ndarray = None  # numpy arrayë¡œ ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨)
    
    def add_documents(self, documents: List[str]):
        """
        ë¬¸ì„œë“¤ì„ ê²€ìƒ‰ ì—”ì§„ì— ì¶”ê°€
        
        Args:
            documents: ì¶”ê°€í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\n{len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ì¸ë±ì‹± ì¤‘...")
        self.documents.extend(documents)
        new_embeddings = self.generator.get_embeddings_batch(documents)
        
        # numpy arrayë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨ + ì—°ì‚° ì†ë„ í–¥ìƒ)
        new_embeddings_array = np.array(new_embeddings)
        if self.embeddings is None:
            self.embeddings = new_embeddings_array
        else:
            self.embeddings = np.vstack([self.embeddings, new_embeddings_array])
        
        print(f"ì¸ë±ì‹± ì™„ë£Œ! ({self.embeddings.shape[0]}ê°œ ë¬¸ì„œ Ã— {self.embeddings.shape[1]}ì°¨ì› ë²¡í„°)")
    
    def search(self, query: str, top_k: int = 3) -> List[Tuple[str, float]]:
        """
        ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
        
        Returns:
            (ë¬¸ì„œ, ìœ ì‚¬ë„) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸
        """
        if not self.documents or self.embeddings is None:
            return []
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = np.array(self.generator.get_embedding(query))
        
        # numpy ë²¡í„°í™” ì—°ì‚°ìœ¼ë¡œ ìœ ì‚¬ë„ ê³„ì‚° (ë” ë¹ ë¦„)
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = dot product / (norm1 * norm2)
        # OpenAI ì„ë² ë”©ì€ ì´ë¯¸ ì •ê·œí™”ë˜ì–´ ìˆìœ¼ë¯€ë¡œ dot productë§Œìœ¼ë¡œ ê³„ì‚° ê°€ëŠ¥
        similarities = np.dot(self.embeddings, query_embedding)
        
        # ìƒìœ„ kê°œ ê²°ê³¼ ì¶”ì¶œ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append((self.documents[idx], float(similarities[idx])))
        
        return results
    
    def print_search_results(self, query: str, top_k: int = 3):
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        print(f"\n[>>>] ê²€ìƒ‰ ì¿¼ë¦¬: '{query}'")
        print("â”€" * 60)
        
        results = self.search(query, top_k)
        
        if not results:
            print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nìƒìœ„ {len(results)}ê°œ ê²°ê³¼:\n")
        for i, (doc, score) in enumerate(results, 1):
            bar = visualize_similarity_bar(score, 25)
            
            # ì ìˆ˜ í•´ì„ (ì‹¤ë¬´ ê¸°ì¤€ê³¼ ì¼ê´€ë˜ê²Œ)
            if score >= 0.8:
                interpretation = "[v] ë§¤ìš° ìœ ì‚¬"
            elif score >= 0.6:
                interpretation = "[~] ê´€ë ¨ ìˆìŒ"
            elif score >= 0.4:
                interpretation = "[o] ì•½ê°„ ê´€ë ¨"
            else:
                interpretation = "[x] ë‹¤ë¥¸ ì£¼ì œ"
            
            print(f"[{i}] {bar} {score:.4f} {interpretation}")
            print(f"    {doc}\n")


def demo_search_engine():
    """ì‹¤ìŠµ 5: ê°„ë‹¨í•œ ê²€ìƒ‰ ì—”ì§„"""
    print("\n" + "="*80)
    print("[5] ì‹¤ìŠµ 5: ê°„ë‹¨í•œ ê²€ìƒ‰ ì—”ì§„ (ì˜ë¯¸ ê¸°ë°˜)")
    print("="*80)
    print("ëª©í‘œ: ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì‘ë™ ì›ë¦¬ ì´í•´")
    print("í•µì‹¬: ë¬¸ì„œ ì¸ë±ì‹± -> ì¿¼ë¦¬ ì„ë² ë”© -> ìœ ì‚¬ë„ ê²€ìƒ‰ -> ìˆœìœ„ ì •ë ¬")
    
    # ê²€ìƒ‰ ì—”ì§„ êµ¬ì¡°
    print_section_header("ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„ êµ¬ì¡°", "[ARCH]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [STEP 1] ì¸ë±ì‹± ë‹¨ê³„ (ì˜¤í”„ë¼ì¸)                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  ë¬¸ì„œë“¤ -> ì„ë² ë”© ìƒì„± -> ë²¡í„° ì €ì¥                       â”‚
  â”‚                                                         â”‚
  â”‚  [STEP 2] ê²€ìƒ‰ ë‹¨ê³„ (ì˜¨ë¼ì¸)                             â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  1. ì¿¼ë¦¬ ì…ë ¥                                           â”‚
  â”‚  2. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±                                    â”‚
  â”‚  3. ì €ì¥ëœ ë²¡í„°ë“¤ê³¼ ìœ ì‚¬ë„ ê³„ì‚°                          â”‚
  â”‚  4. ìƒìœ„ kê°œ ê²°ê³¼ ë°˜í™˜                                  â”‚
  â”‚                                                         â”‚
  â”‚  [!] ì´ê²ƒì´ RAGì˜ "Retrieval" ë¶€ë¶„!                      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    if not os.getenv("OPENAI_API_KEY"):
        print("\n[!] OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        return
    
    # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
    search_engine = SimpleSearchEngine()
    
    # ìƒ˜í”Œ ë¬¸ì„œ ì¶”ê°€
    print_section_header("ë¬¸ì„œ ì¸ë±ì‹±", "[LIST]")
    
    documents = [
        "Python is a high-level programming language known for its simplicity.",
        "Machine learning is a subset of artificial intelligence.",
        "Deep learning uses neural networks with multiple layers.",
        "Natural language processing helps computers understand human language.",
        "Computer vision enables machines to interpret visual information.",
        "Data science involves extracting insights from data.",
        "JavaScript is commonly used for web development.",
        "SQL is used for managing relational databases.",
        "Cloud computing provides on-demand computing resources.",
        "Cybersecurity protects systems from digital attacks.",
        "The weather is beautiful today with clear skies.",
        "I love eating pizza and pasta for dinner.",
        "Exercise and healthy eating are important for wellness.",
        "Traveling to new places broadens your perspective.",
        "Reading books is a great way to learn new things.",
    ]
    
    print("\nì¸ë±ì‹±í•  ë¬¸ì„œ:")
    for i, doc in enumerate(documents[:5], 1):
        print(f"  {i}. {doc}")
    print(f"  ... ({len(documents) - 5}ê°œ ë”)")
    
    search_engine.add_documents(documents)
    print(f"\n[OK] ì´ {len(documents)}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ")
    
    # ë‹¤ì–‘í•œ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
    print_section_header("ê²€ìƒ‰ í…ŒìŠ¤íŠ¸", "[>>>]")
    
    queries = [
        "What is AI and machine learning?",
        "Tell me about programming languages",
        "How can I stay healthy?",
        "I want to learn about databases",
    ]
    
    for query in queries:
        search_engine.print_search_results(query, top_k=3)
    
    # ê²€ìƒ‰ ì¿¼ë¦¬ ì‘ì„± íŒ
    print_section_header("ê²€ìƒ‰ ì¿¼ë¦¬ ì‘ì„± íŒ", "[TIP]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [!] ì¿¼ë¦¬ í‘œí˜„ì´ ìœ ì‚¬ë„ì— ì˜í–¥ì„ ì¤ë‹ˆë‹¤!                  â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  ë‚˜ìœ ì˜ˆ: "Tell me about programming languages"         â”‚
  â”‚    â†’ ë¶ˆí•„ìš”í•œ ë‹¨ì–´(Tell me about)ê°€ ìœ ì‚¬ë„ë¥¼ ë‚®ì¶¤       â”‚
  â”‚    â†’ ê²°ê³¼: 0.38 (ë‚®ì€ ê´€ë ¨ì„±)                           â”‚
  â”‚                                                         â”‚
  â”‚  ì¢‹ì€ ì˜ˆ: "programming languages"                       â”‚
  â”‚    â†’ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì‚¬ìš©í•˜ë©´ ìœ ì‚¬ë„ í–¥ìƒ                  â”‚
  â”‚    â†’ ê²°ê³¼: 0.55+ (ì¤‘ê°„ ê´€ë ¨ì„±)                          â”‚
  â”‚                                                         â”‚
  â”‚  [TIP] ì˜ë¯¸ ê²€ìƒ‰ì´ë¼ë„ ì¿¼ë¦¬ëŠ” ê°„ê²°í•˜ê²Œ!                  â”‚
  â”‚  [TIP] "What is", "Tell me about" ë“±ì€ ë…¸ì´ì¦ˆ           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # í‚¤ì›Œë“œ ê²€ìƒ‰ vs ì˜ë¯¸ ê²€ìƒ‰ ë¹„êµ
    print_section_header("í‚¤ì›Œë“œ ê²€ìƒ‰ vs ì˜ë¯¸ ê²€ìƒ‰", "[vs]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [CMP] ë¹„êµ                                              â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  í‚¤ì›Œë“œ ê²€ìƒ‰ (BM25)         â”‚ ì˜ë¯¸ ê²€ìƒ‰ (ì„ë² ë”©)          â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
  â”‚  "Python" ê²€ìƒ‰ ì‹œ           â”‚ "Python" ê²€ìƒ‰ ì‹œ           â”‚
  â”‚  -> "Python" í¬í•¨ ë¬¸ì„œë§Œ    â”‚ -> í”„ë¡œê·¸ë˜ë° ê´€ë ¨ ë¬¸ì„œë„  â”‚
  â”‚                             â”‚    (JavaScript, SQL ë“±)    â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
  â”‚  ì¥ì : ë¹ ë¦„, ì •í™•í•œ í‚¤ì›Œë“œ  â”‚ ì¥ì : ë™ì˜ì–´, ìœ ì‚¬ ê°œë…    â”‚
  â”‚  ë‹¨ì : ë™ì˜ì–´ ëª» ì°¾ìŒ       â”‚ ë‹¨ì : ì„ë² ë”© ë¹„ìš© í•„ìš”     â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
  â”‚  [TIP] ì‹¤ë¬´: ë‘˜ì„ ê²°í•©í•œ Hybrid ê²€ìƒ‰ ì‚¬ìš© (lab03 í•™ìŠµ)  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # í˜„ì¬ ë°©ì‹ì˜ í•œê³„ì™€ Vector DB í•„ìš”ì„±
    print_section_header("í˜„ì¬ ë°©ì‹ì˜ í•œê³„", "[!]")
    print(f"""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [!] ì„ í˜• ê²€ìƒ‰ì˜ ì‹œê°„ ë³µì¡ë„: O(n)                       â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  í˜„ì¬ ì¸ë±ì‹±ëœ ë¬¸ì„œ: {len(search_engine.documents)}ê°œ                              â”‚
  â”‚                                                         â”‚
  â”‚  ë¬¸ì„œ ìˆ˜ì— ë”°ë¥¸ ê²€ìƒ‰ ì‹œê°„ (O(n) ì„ í˜• ì¦ê°€):              â”‚
  â”‚  â€¢ 1,000ê°œ    â†’ ~10ms   (ì‹¤ì‹œê°„ ê°€ëŠ¥)                   â”‚
  â”‚  â€¢ 10,000ê°œ   â†’ ~100ms  (ì•½ê°„ ì§€ì—°)                     â”‚
  â”‚  â€¢ 100,000ê°œ  â†’ ~1ì´ˆ    (ëŠë¦¼)                          â”‚
  â”‚  â€¢ 1,000,000ê°œ â†’ ~10ì´ˆ   (ì‹¤ì‹œê°„ ë¶ˆê°€!)                 â”‚
  â”‚  (â€» ì‹¤ì œ ì‹œê°„ì€ í•˜ë“œì›¨ì–´/í™˜ê²½ì— ë”°ë¼ ë‹¤ë¦„)               â”‚
  â”‚                                                         â”‚
  â”‚  [>>>] Vector DB (ChromaDB, Pinecone ë“±)ì˜ í•´ê²°ì±…:      â”‚
  â”‚  â€¢ ANN (Approximate Nearest Neighbor) ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©     â”‚
  â”‚  â€¢ ì‹œê°„ ë³µì¡ë„: O(log n)                                â”‚
  â”‚  â€¢ 1,000,000ê°œ â†’ ~10ms (1000ë°° ë¹ ë¦„!)                   â”‚
  â”‚  â€¢ ì•½ê°„ì˜ ì •í™•ë„ trade-off (ë³´í†µ 95%+ ì •í™•ë„)           â”‚
  â”‚                                                         â”‚
  â”‚  â†’ lab02ì—ì„œ ChromaDBë¡œ ëŒ€ìš©ëŸ‰ ê²€ìƒ‰ í•™ìŠµ                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # í•µì‹¬ í¬ì¸íŠ¸
    print_key_points([
        "- ì˜ë¯¸ ê²€ìƒ‰: í‚¤ì›Œë“œê°€ ë‹¬ë¼ë„ ì˜ë¯¸ê°€ ë¹„ìŠ·í•˜ë©´ ê²€ìƒ‰ë¨",
        "- ì¿¼ë¦¬ íŒ: ë¶ˆí•„ìš”í•œ í‘œí˜„ ì œê±°, í•µì‹¬ í‚¤ì›Œë“œë§Œ ì‚¬ìš©",
        "- ì¸ë±ì‹±: ë¬¸ì„œë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥ (1íšŒ)",
        "- ê²€ìƒ‰: ì¿¼ë¦¬ ì„ë² ë”© -> ìœ ì‚¬ë„ ê³„ì‚° -> ìˆœìœ„ ì •ë ¬",
        "- í•œê³„: O(n) ë³µì¡ë„ -> Vector DB(O(log n))ë¡œ í•´ê²° (lab02)",
        "- ë°œì „: RAG = ê²€ìƒ‰ + LLM ë‹µë³€ ìƒì„± (lab03)"
    ], "ê²€ìƒ‰ ì—”ì§„ í•µì‹¬ í¬ì¸íŠ¸")


# ============================================================================
# 6. ì„ë² ë”© ì‹œê°í™”
# ============================================================================

def demo_embedding_visualization():
    """ì‹¤ìŠµ 6: ì„ë² ë”© ì‹œê°í™” - t-SNEë¡œ ë²¡í„° ê³µê°„ ì´í•´í•˜ê¸°"""
    print("\n" + "="*80)
    print("[6] ì‹¤ìŠµ 6: ì„ë² ë”© ì‹œê°í™” - t-SNEë¡œ ë²¡í„° ê³µê°„ ì´í•´í•˜ê¸°")
    print("="*80)
    print("ëª©í‘œ: ê³ ì°¨ì› ì„ë² ë”©ì„ 2Dë¡œ ì‹œê°í™”í•˜ì—¬ ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„° í™•ì¸")
    print("í•µì‹¬: ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ í…ìŠ¤íŠ¸ëŠ” ì‹œê°í™”ì—ì„œë„ ê°€ê¹Œì´ ëª¨ì„")
    
    # t-SNE ê°œë… ì„¤ëª…
    print_section_header("ì°¨ì› ì¶•ì†Œë€?", "[INFO]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [!] ë¬¸ì œ: 1536ì°¨ì›ì„ ì–´ë–»ê²Œ ì´í•´í• ê¹Œ?                   â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  ì„ë² ë”© ë²¡í„°: [0.1, -0.3, 0.5, ..., 0.2]  â† 1536ê°œ ìˆ«ì â”‚
  â”‚  â†’ ì‚¬ëŒì´ ì§ì ‘ í•´ì„í•˜ê¸° ë¶ˆê°€ëŠ¥                          â”‚
  â”‚  â†’ 2D/3Dë¡œ ë³€í™˜í•˜ì—¬ ì‹œê°í™”!                             â”‚
  â”‚                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [ALGO] ëŒ€í‘œì ì¸ ì°¨ì› ì¶•ì†Œ ì•Œê³ ë¦¬ì¦˜                      â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  1. t-SNE (t-distributed Stochastic Neighbor Embedding) â”‚
  â”‚     * ê°€ê¹Œìš´ ì ë“¤ì˜ ê´€ê³„ë¥¼ ë³´ì¡´                         â”‚
  â”‚     * í´ëŸ¬ìŠ¤í„° ì‹œê°í™”ì— ìµœì                             â”‚
  â”‚     * ëŠë¦¼ (O(nÂ²)), ëŒ€ìš©ëŸ‰ì— ë¶€ì í•©                     â”‚
  â”‚     * í•˜ì´í¼íŒŒë¼ë¯¸í„°: perplexity (5~50)                 â”‚
  â”‚                                                         â”‚
  â”‚  2. UMAP (Uniform Manifold Approximation & Projection)  â”‚
  â”‚     * t-SNEë³´ë‹¤ ë¹ ë¦„                                    â”‚
  â”‚     * ì „ì—­ êµ¬ì¡°ë„ ì–´ëŠ ì •ë„ ë³´ì¡´                        â”‚
  â”‚     * ìµœê·¼ ë” ë§ì´ ì‚¬ìš©ë¨                               â”‚
  â”‚     * í•˜ì´í¼íŒŒë¼ë¯¸í„°: n_neighbors, min_dist             â”‚
  â”‚                                                         â”‚
  â”‚  3. PCA (Principal Component Analysis)                  â”‚
  â”‚     * ê°€ì¥ ë¹ ë¦„, ì„ í˜• ë³€í™˜                              â”‚
  â”‚     * ë¶„ì‚°ì„ ìµœëŒ€í™”í•˜ëŠ” ì¶• ì„ íƒ                         â”‚
  â”‚     * ë¹„ì„ í˜• ê´€ê³„ í¬ì°© ì–´ë ¤ì›€                           â”‚
  â”‚                                                         â”‚
  â”‚  [TIP] ì„ íƒ ê°€ì´ë“œ:                                     â”‚
  â”‚  * í´ëŸ¬ìŠ¤í„° í™•ì¸: t-SNE ë˜ëŠ” UMAP                       â”‚
  â”‚  * ë¹ ë¥¸ íƒìƒ‰: PCA (ì„ ì²˜ë¦¬ í›„ t-SNE ì ìš©ë„ ê°€ëŠ¥)         â”‚
  â”‚  * ëŒ€ìš©ëŸ‰ (10ë§Œ+): UMAP ê¶Œì¥                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    if not os.getenv("OPENAI_API_KEY"):
        print("\n[!] OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        return
    
    # t-SNE ì‹œê°í™” ì‹¤ìŠµ (ì‹¤ì œ êµ¬í˜„)
    print_section_header("t-SNE ì‹œê°í™” ì‹¤ìŠµ", "[CODE]")
    
    # ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    try:
        from sklearn.manifold import TSNE
        tsne_available = True
    except ImportError:
        tsne_available = False
        print("\n[!] scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì„¤ì¹˜: pip install scikit-learn")
    
    try:
        import matplotlib.pyplot as plt
        matplotlib_available = True
    except ImportError:
        matplotlib_available = False
        print("\n[!] matplotlibì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì„¤ì¹˜: pip install matplotlib")
    
    # ìƒ˜í”Œ í…ìŠ¤íŠ¸ (ì¹´í…Œê³ ë¦¬ë³„ë¡œ êµ¬ë¶„)
    texts_by_category = {
        "í”„ë¡œê·¸ë˜ë°": [
            "Python is a great programming language",
            "JavaScript is used for web development",
            "Java is popular for enterprise applications",
            "C++ is used for system programming",
        ],
        "ìŒì‹": [
            "Pizza is my favorite Italian food",
            "Sushi is a traditional Japanese dish",
            "Tacos are delicious Mexican food",
            "Pasta with tomato sauce is amazing",
        ],
        "ìŠ¤í¬ì¸ ": [
            "Soccer is the most popular sport worldwide",
            "Basketball requires great athleticism",
            "Tennis is an individual sport",
            "Swimming is excellent exercise",
        ],
    }
    
    # í…ìŠ¤íŠ¸ì™€ ë¼ë²¨ ì¤€ë¹„
    all_texts = []
    labels = []
    for category, texts in texts_by_category.items():
        all_texts.extend(texts)
        labels.extend([category] * len(texts))
    
    print(f"\nì´ {len(all_texts)}ê°œ í…ìŠ¤íŠ¸ (3ê°œ ì¹´í…Œê³ ë¦¬)")
    for category, texts in texts_by_category.items():
        print(f"  * {category}: {len(texts)}ê°œ")
    
    # ì„ë² ë”© ìƒì„±
    generator = EmbeddingGenerator()
    print("\n[...] ì„ë² ë”© ìƒì„± ì¤‘...")
    embeddings = generator.get_embeddings_batch(all_texts)
    embeddings_array = np.array(embeddings)
    print(f"[OK] ì„ë² ë”© ì™„ë£Œ: {embeddings_array.shape}")
    
    if tsne_available and matplotlib_available:
        # t-SNE ì‹¤í–‰
        print("\n[...] t-SNE ì°¨ì› ì¶•ì†Œ ì¤‘...")
        tsne = TSNE(
            n_components=2,      # 2ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ
            perplexity=5,        # ì‘ì€ ë°ì´í„°ì…‹ì´ë¯€ë¡œ ë‚®ì€ ê°’
            random_state=42,     # ì¬í˜„ì„±
            max_iter=1000        # ë°˜ë³µ íšŸìˆ˜ (scikit-learn 1.5+ì—ì„œ n_iter â†’ max_iter)
        )
        embeddings_2d = tsne.fit_transform(embeddings_array)
        print(f"[OK] t-SNE ì™„ë£Œ: {embeddings_2d.shape}")
        
        # ì‹œê°í™” (í…ìŠ¤íŠ¸ ì¶œë ¥)
        print_section_header("ì‹œê°í™” ê²°ê³¼ (í…ìŠ¤íŠ¸)", "[CHART]")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì¢Œí‘œ ì¶œë ¥
        colors = {"í”„ë¡œê·¸ë˜ë°": "ğŸ”µ", "ìŒì‹": "ğŸŸ¢", "ìŠ¤í¬ì¸ ": "ğŸ”´"}
        
        print("\nì¢Œí‘œ (x, y):")
        print(f"{'â”€'*60}")
        for i, (text, label) in enumerate(zip(all_texts, labels)):
            x, y = embeddings_2d[i]
            icon = colors.get(label, "âšª")
            print(f"  {icon} ({x:6.2f}, {y:6.2f}) {text[:40]}...")
        
        # ASCII ì‹œê°í™”
        print(f"\n{'â”€'*60}")
        print("ASCII ì‚°ì ë„ (ëŒ€ëµì ì¸ ìœ„ì¹˜):")
        print(f"{'â”€'*60}")
        
        # ì¢Œí‘œ ì •ê·œí™” (0~40 ë²”ìœ„)
        x_min, x_max = embeddings_2d[:, 0].min(), embeddings_2d[:, 0].max()
        y_min, y_max = embeddings_2d[:, 1].min(), embeddings_2d[:, 1].max()
        
        # ê·¸ë¦¬ë“œ ìƒì„±
        grid_width, grid_height = 60, 20
        grid = [[' ' for _ in range(grid_width)] for _ in range(grid_height)]
        
        # ì  ë°°ì¹˜
        category_symbols = {"í”„ë¡œê·¸ë˜ë°": 'P', "ìŒì‹": 'F', "ìŠ¤í¬ì¸ ": 'S'}
        for i, (label) in enumerate(labels):
            x_norm = int((embeddings_2d[i, 0] - x_min) / (x_max - x_min + 1e-10) * (grid_width - 1))
            y_norm = int((embeddings_2d[i, 1] - y_min) / (y_max - y_min + 1e-10) * (grid_height - 1))
            y_norm = grid_height - 1 - y_norm  # yì¶• ë°˜ì „
            grid[y_norm][x_norm] = category_symbols[label]
        
        # ê·¸ë¦¬ë“œ ì¶œë ¥
        print("  +" + "-" * grid_width + "+")
        for row in grid:
            print("  |" + "".join(row) + "|")
        print("  +" + "-" * grid_width + "+")
        print(f"  ë²”ë¡€: P=í”„ë¡œê·¸ë˜ë°, F=ìŒì‹, S=ìŠ¤í¬ì¸ ")
        
        # matplotlib ì°¨íŠ¸ ì €ì¥ (ì„ íƒì )
        print_section_header("ì°¨íŠ¸ íŒŒì¼ ì €ì¥", "[FILE]")
        
        try:
            # í•œê¸€ í°íŠ¸ ì„¤ì • (Windows: Malgun Gothic, Mac: AppleGothic, Linux: NanumGothic)
            import platform
            system = platform.system()
            
            if system == "Windows":
                plt.rcParams['font.family'] = 'Malgun Gothic'
            elif system == "Darwin":  # macOS
                plt.rcParams['font.family'] = 'AppleGothic'
            else:  # Linux
                plt.rcParams['font.family'] = 'NanumGothic'
            
            # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
            plt.rcParams['axes.unicode_minus'] = False
            
            plt.figure(figsize=(10, 8))
            
            color_map = {"í”„ë¡œê·¸ë˜ë°": "blue", "ìŒì‹": "green", "ìŠ¤í¬ì¸ ": "red"}
            
            for category in texts_by_category.keys():
                mask = [l == category for l in labels]
                indices = [i for i, m in enumerate(mask) if m]
                plt.scatter(
                    embeddings_2d[indices, 0],
                    embeddings_2d[indices, 1],
                    c=color_map[category],
                    label=category,
                    s=100,
                    alpha=0.7
                )
            
            # í…ìŠ¤íŠ¸ ë¼ë²¨ ì¶”ê°€
            for i, text in enumerate(all_texts):
                plt.annotate(
                    text[:20] + "...",
                    (embeddings_2d[i, 0], embeddings_2d[i, 1]),
                    fontsize=8,
                    alpha=0.7
                )
            
            plt.title("t-SNE ì„ë² ë”© ì‹œê°í™” (ì¹´í…Œê³ ë¦¬ë³„ í´ëŸ¬ìŠ¤í„°)")
            plt.xlabel("ì°¨ì› 1")
            plt.ylabel("ì°¨ì› 2")
            plt.legend()
            plt.tight_layout()
            
            # íŒŒì¼ ì €ì¥
            output_path = Path(__file__).parent / "embedding_tsne_demo.png"
            plt.savefig(output_path, dpi=150)
            plt.close()
            
            print(f"[OK] ì°¨íŠ¸ ì €ì¥ ì™„ë£Œ: {output_path}")
            print("   â†’ ì´ íŒŒì¼ì„ ì—´ì–´ ì‹œê°í™”ë¥¼ í™•ì¸í•˜ì„¸ìš”!")
            
        except Exception as e:
            print(f"[!] ì°¨íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            print("   (í…ìŠ¤íŠ¸ ì‹œê°í™”ë¡œ ê²°ê³¼ëŠ” í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    else:
        print("\n[!] ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ì½”ë“œ ì˜ˆì‹œë§Œ ì œê³µí•©ë‹ˆë‹¤.")
        print("""
  [CODE] t-SNE ì‹œê°í™” ì½”ë“œ:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚ from sklearn.manifold import TSNE
  â”‚ import matplotlib.pyplot as plt
  â”‚ 
  â”‚ # 1. ì„ë² ë”© ì¤€ë¹„ (N x 1536 ë°°ì—´)
  â”‚ embeddings = np.array([...])
  â”‚ 
  â”‚ # 2. t-SNE ì‹¤í–‰
  â”‚ tsne = TSNE(n_components=2, perplexity=30, random_state=42)
  â”‚ embeddings_2d = tsne.fit_transform(embeddings)
  â”‚ 
  â”‚ # 3. ì‹œê°í™”
  â”‚ plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])
  â”‚ plt.title("Embedding Visualization")
  â”‚ plt.savefig("embeddings.png")
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        """)
    
    # ì‹œê°í™” í•´ì„ ê°€ì´ë“œ
    print_section_header("ì‹œê°í™” í•´ì„ ê°€ì´ë“œ", "[TIP]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [!] ì‹œê°í™” ê²°ê³¼ í•´ì„ ì‹œ ì£¼ì˜ì‚¬í•­                        â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  1. í´ëŸ¬ìŠ¤í„° í˜•ì„± í™•ì¸                                   â”‚
  â”‚     * ê°™ì€ ì¹´í…Œê³ ë¦¬ê°€ ë­‰ì³ìˆìœ¼ë©´ â†’ ì„ë² ë”© í’ˆì§ˆ ì¢‹ìŒ     â”‚
  â”‚     * ì„ì—¬ìˆìœ¼ë©´ â†’ í•´ë‹¹ ê°œë…ì´ ëª¨ë¸ì—ì„œ êµ¬ë¶„ ì•ˆ ë¨      â”‚
  â”‚                                                         â”‚
  â”‚  2. ê±°ë¦¬ í•´ì„ ì£¼ì˜                                       â”‚
  â”‚     * t-SNE/UMAPì˜ ê±°ë¦¬ëŠ” ì ˆëŒ€ì  ì˜ë¯¸ ì—†ìŒ              â”‚
  â”‚     * "ê°€ê¹Œì›€/ë©‚"ë§Œ ì˜ë¯¸ ìˆìŒ, ì •í™•í•œ ê±°ë¦¬ ì•„ë‹˜         â”‚
  â”‚     * ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ë©´ ëª¨ì–‘ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ (ëœë¤ì„±)   â”‚
  â”‚                                                         â”‚
  â”‚  3. íŒŒë¼ë¯¸í„° ì˜í–¥                                        â”‚
  â”‚     * perplexity ë†’ìŒ â†’ ì „ì—­ êµ¬ì¡° ê°•ì¡°                  â”‚
  â”‚     * perplexity ë‚®ìŒ â†’ ì§€ì—­ êµ¬ì¡° ê°•ì¡°                  â”‚
  â”‚     * ë°ì´í„° í¬ê¸°ì— ë”°ë¼ ì¡°ì • í•„ìš”                      â”‚
  â”‚                                                         â”‚
  â”‚  4. ì‹¤ë¬´ í™œìš©                                            â”‚
  â”‚     * ë°ì´í„° í’ˆì§ˆ í™•ì¸ (ì´ìƒì¹˜ íƒì§€)                    â”‚
  â”‚     * í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ê²€ì¦                              â”‚
  â”‚     * ìƒˆ ì¹´í…Œê³ ë¦¬ ë°œê²¬                                  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # í•µì‹¬ í¬ì¸íŠ¸
    print_key_points([
        "- ì°¨ì› ì¶•ì†Œ: 1536ì°¨ì› â†’ 2Dë¡œ ì‹œê°í™”í•˜ì—¬ ì´í•´",
        "- t-SNE: ê°€ê¹Œìš´ ê´€ê³„ ë³´ì¡´, í´ëŸ¬ìŠ¤í„° ì‹œê°í™”ì— ìµœì ",
        "- UMAP: t-SNEë³´ë‹¤ ë¹ ë¦„, ëŒ€ìš©ëŸ‰ì— ì í•©",
        "- í•´ì„ ì£¼ì˜: ì‹œê°í™” ê±°ë¦¬ â‰  ì‹¤ì œ ìœ ì‚¬ë„",
        "- í™œìš©: ë°ì´í„° í’ˆì§ˆ í™•ì¸, í´ëŸ¬ìŠ¤í„° ê²€ì¦, ì´ìƒì¹˜ íƒì§€"
    ], "ì„ë² ë”© ì‹œê°í™” í•µì‹¬ í¬ì¸íŠ¸")


# ============================================================================
# 7. ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸
# ============================================================================

def demo_sentence_transformers():
    """ì‹¤ìŠµ 7: ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸ - Sentence Transformers ì†Œê°œ"""
    print("\n" + "="*80)
    print("[7] ì‹¤ìŠµ 7: ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸ - Sentence Transformers ì†Œê°œ")
    print("="*80)
    print("ëª©í‘œ: OpenAI ì™¸ ë¬´ë£Œ ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸ ì´í•´")
    print("í•µì‹¬: ë¹„ìš© ì ˆê°, ì˜¤í”„ë¼ì¸ ì‚¬ìš©, ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥")
    
    # Sentence Transformers ì†Œê°œ
    print_section_header("Sentence Transformersë€?", "[INFO]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [LIB] Sentence Transformers (sentence-transformers)    â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  * Hugging Face ê¸°ë°˜ ë¬¸ì¥ ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬             â”‚
  â”‚  * ìˆ˜ë°± ê°œì˜ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ì œê³µ                      â”‚
  â”‚  * MIT ë¼ì´ì„ ìŠ¤ (ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥)                      â”‚
  â”‚  * ë¡œì»¬ ì‹¤í–‰ â†’ API ë¹„ìš© ì—†ìŒ!                          â”‚
  â”‚                                                         â”‚
  â”‚  [ì„¤ì¹˜]                                                 â”‚
  â”‚  pip install sentence-transformers                      â”‚
  â”‚                                                         â”‚
  â”‚  [ê¸°ë³¸ ì‚¬ìš©ë²•]                                          â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
  â”‚  â”‚ from sentence_transformers import SentenceTransformerâ”‚
  â”‚  â”‚                                                      â”‚
  â”‚  â”‚ # ëª¨ë¸ ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œ)                  â”‚
  â”‚  â”‚ model = SentenceTransformer('all-MiniLM-L6-v2')     â”‚
  â”‚  â”‚                                                      â”‚
  â”‚  â”‚ # ì„ë² ë”© ìƒì„±                                        â”‚
  â”‚  â”‚ sentences = ["Hello world", "How are you?"]         â”‚
  â”‚  â”‚ embeddings = model.encode(sentences)                 â”‚
  â”‚  â”‚ # embeddings.shape: (2, 384)                        â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # ì¸ê¸° ëª¨ë¸ ë¹„êµ
    print_section_header("ì¸ê¸° ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸", "[LIST]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [CMP] ì£¼ìš” ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸ ë¹„êµ                                        â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                             â”‚
  â”‚  ëª¨ë¸ëª…                      â”‚ ì°¨ì›  â”‚ í¬ê¸°   â”‚ ì†ë„   â”‚ í’ˆì§ˆ  â”‚ íŠ¹ì§•      â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚  all-MiniLM-L6-v2          â”‚ 384  â”‚ 80MB  â”‚ ë¹ ë¦„  â”‚ ì¢‹ìŒ  â”‚ ê°€ì¥ ì¸ê¸°  â”‚
  â”‚  all-mpnet-base-v2         â”‚ 768  â”‚ 420MB â”‚ ì¤‘ê°„  â”‚ ë†’ìŒ  â”‚ ê· í˜•ì¡í˜  â”‚
  â”‚  paraphrase-multilingual-  â”‚ 768  â”‚ 1GB   â”‚ ëŠë¦¼  â”‚ ë†’ìŒ  â”‚ ë‹¤êµ­ì–´    â”‚
  â”‚    MiniLM-L12-v2           â”‚      â”‚       â”‚       â”‚      â”‚ (í•œê¸€ OK) â”‚
  â”‚  e5-large-v2               â”‚ 1024 â”‚ 1.3GB â”‚ ëŠë¦¼  â”‚ ìµœê³   â”‚ SOTAê¸‰    â”‚
  â”‚  bge-large-en-v1.5         â”‚ 1024 â”‚ 1.3GB â”‚ ëŠë¦¼  â”‚ ìµœê³   â”‚ ì¤‘êµ­ BAAI â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚                                                                             â”‚
  â”‚  [TIP] ì„ íƒ ê°€ì´ë“œ:                                                         â”‚
  â”‚  * ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…: all-MiniLM-L6-v2 (ì‘ê³  ë¹ ë¦„)                            â”‚
  â”‚  * í”„ë¡œë•ì…˜ í’ˆì§ˆ: all-mpnet-base-v2 ë˜ëŠ” e5-large                           â”‚
  â”‚  * í•œê¸€ ì§€ì›: paraphrase-multilingual-MiniLM-L12-v2                         â”‚
  â”‚  * ìµœê³  ì„±ëŠ¥: bge-large-en-v1.5 ë˜ëŠ” e5-large-v2                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # OpenAI vs Sentence Transformers ë¹„êµ
    print_section_header("OpenAI vs Sentence Transformers", "[vs]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [CMP] ë¹„êµí‘œ                                                            â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  í•­ëª©            â”‚ OpenAI                 â”‚ Sentence Transformers      â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  ë¹„ìš©           â”‚ $0.02 / 1M í† í°        â”‚ ë¬´ë£Œ (ë¡œì»¬ ì‹¤í–‰)            â”‚
  â”‚  ì†ë„           â”‚ ë„¤íŠ¸ì›Œí¬ ì§€ì—° ìˆìŒ      â”‚ GPU ìˆìœ¼ë©´ ë§¤ìš° ë¹ ë¦„       â”‚
  â”‚  í’ˆì§ˆ           â”‚ ìµœìƒê¸‰                 â”‚ ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¦„           â”‚
  â”‚  ì˜¤í”„ë¼ì¸       â”‚ âœ— ë¶ˆê°€                 â”‚ âœ“ ê°€ëŠ¥                     â”‚
  â”‚  ì»¤ìŠ¤í„°ë§ˆì´ì§•   â”‚ âœ— ë¶ˆê°€ (APIë§Œ ì œê³µ)    â”‚ âœ“ íŒŒì¸íŠœë‹ ê°€ëŠ¥            â”‚
  â”‚  ë‹¤êµ­ì–´         â”‚ âœ“ ìš°ìˆ˜                 â”‚ ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¦„           â”‚
  â”‚  ì„¤ì¹˜           â”‚ pip install openai     â”‚ pip install sentence-      â”‚
  â”‚                 â”‚                        â”‚   transformers             â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  [TIP] ì–¸ì œ ë¬´ì—‡ì„ ì„ íƒí• ê¹Œ?                                            â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  OpenAI ì„ íƒ:                                                           â”‚
  â”‚  * ìµœê³  í’ˆì§ˆì´ í•„ìš”í•  ë•Œ                                                â”‚
  â”‚  * ë‹¤êµ­ì–´ ì§€ì›ì´ ì¤‘ìš”í•  ë•Œ                                              â”‚
  â”‚  * ì¸í”„ë¼ ê´€ë¦¬ ì—†ì´ ë¹ ë¥´ê²Œ ì‹œì‘í•  ë•Œ                                    â”‚
  â”‚  * ì‚¬ìš©ëŸ‰ì´ ì ì„ ë•Œ (ë¹„ìš© ë¶€ë‹´ ì ìŒ)                                    â”‚
  â”‚                                                                         â”‚
  â”‚  Sentence Transformers ì„ íƒ:                                            â”‚
  â”‚  * ë¹„ìš© ì ˆê°ì´ ì¤‘ìš”í•  ë•Œ (ëŒ€ìš©ëŸ‰ ì²˜ë¦¬)                                  â”‚
  â”‚  * ì˜¤í”„ë¼ì¸/ì—ì–´ê°­ í™˜ê²½                                                 â”‚
  â”‚  * ë°ì´í„° ë³´ì•ˆ (ì™¸ë¶€ ì „ì†¡ ë¶ˆê°€)                                         â”‚
  â”‚  * ì»¤ìŠ¤í…€ ë„ë©”ì¸ íŒŒì¸íŠœë‹ í•„ìš”                                          â”‚
  â”‚  * GPU ì„œë²„ê°€ ìˆì„ ë•Œ                                                   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ
    print_section_header("Sentence Transformers ì‚¬ìš© ì˜ˆì‹œ", "[CODE]")
    
    # ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    try:
        from sentence_transformers import SentenceTransformer
        st_available = True
    except ImportError:
        st_available = False
    
    if st_available:
        print("\n[OK] sentence-transformersê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        print("\n[...] ëª¨ë¸ ë¡œë”© ì¤‘ (ìµœì´ˆ ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œ)...")
        
        try:
            # ê°€ë²¼ìš´ ëª¨ë¸ ì‚¬ìš©
            model = SentenceTransformer('all-MiniLM-L6-v2')
            
            # í…ŒìŠ¤íŠ¸ ë¬¸ì¥
            sentences = [
                "Python is a programming language",
                "Java is also a programming language",
                "I love eating pizza",
            ]
            
            print(f"[OK] ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: all-MiniLM-L6-v2")
            print(f"\ní…ŒìŠ¤íŠ¸ ë¬¸ì¥:")
            for i, s in enumerate(sentences, 1):
                print(f"  {i}. {s}")
            
            # ì„ë² ë”© ìƒì„±
            embeddings = model.encode(sentences)
            
            print(f"\nì„ë² ë”© ê²°ê³¼:")
            print(f"  * Shape: {embeddings.shape}")
            print(f"  * ì°¨ì›: {embeddings.shape[1]}")
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            print(f"\nì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¹„êµ:")
            sim_1_2 = cosine_similarity(embeddings[0].tolist(), embeddings[1].tolist())
            sim_1_3 = cosine_similarity(embeddings[0].tolist(), embeddings[2].tolist())
            
            print(f"  Python vs Java: {sim_1_2:.4f} (í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¼ë¦¬)")
            print(f"  Python vs Pizza: {sim_1_3:.4f} (ë‹¤ë¥¸ ì£¼ì œ)")
            print(f"\n  â†’ ê°™ì€ ì£¼ì œëŠ” ìœ ì‚¬ë„ê°€ ë†’ìŒ!")
            
        except Exception as e:
            print(f"\n[!] ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            print("   ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— ì¸í„°ë„· ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    else:
        print("\n[!] sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì„¤ì¹˜: pip install sentence-transformers")
        print("""
  [CODE] ì„¤ì¹˜ í›„ ì‚¬ìš© ì˜ˆì‹œ:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â”‚ from sentence_transformers import SentenceTransformer
  â”‚ 
  â”‚ # ëª¨ë¸ ë¡œë“œ
  â”‚ model = SentenceTransformer('all-MiniLM-L6-v2')
  â”‚ 
  â”‚ # ì„ë² ë”© ìƒì„±
  â”‚ sentences = ["Hello world", "ì•ˆë…•í•˜ì„¸ìš”"]
  â”‚ embeddings = model.encode(sentences)
  â”‚ 
  â”‚ print(embeddings.shape)  # (2, 384)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        """)
    
    # íŒŒì¸íŠœë‹ ê°€ì´ë“œ
    print_section_header("íŒŒì¸íŠœë‹ ê°€ì´ë“œ (ì‹¬í™”)", "[ADV]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [!] ë„ë©”ì¸ íŠ¹í™” íŒŒì¸íŠœë‹                                â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  ê¸°ë³¸ ëª¨ë¸ì´ íŠ¹ì • ë„ë©”ì¸ì—ì„œ ì„±ëŠ¥ì´ ë‚®ì„ ë•Œ:            â”‚
  â”‚  * ë²•ë¥ /ì˜ë£Œ/ê¸ˆìœµ ì „ë¬¸ ìš©ì–´                             â”‚
  â”‚  * íšŒì‚¬ ë‚´ë¶€ ë¬¸ì„œ ìŠ¤íƒ€ì¼                                â”‚
  â”‚  * íŠ¹ì • ì–¸ì–´/ë°©ì–¸                                       â”‚
  â”‚                                                         â”‚
  â”‚  [ë°©ë²•]                                                 â”‚
  â”‚  1. ëŒ€ì¡° í•™ìŠµ (Contrastive Learning)                   â”‚
  â”‚     - ìœ ì‚¬í•œ ë¬¸ì¥ ìŒ / ë‹¤ë¥¸ ë¬¸ì¥ ìŒ ë°ì´í„° ì¤€ë¹„         â”‚
  â”‚     - ìœ ì‚¬í•œ ê²ƒì€ ê°€ê¹ê²Œ, ë‹¤ë¥¸ ê²ƒì€ ë©€ê²Œ í•™ìŠµ           â”‚
  â”‚                                                         â”‚
  â”‚  2. í•„ìš” ë°ì´í„°                                         â”‚
  â”‚     - ìµœì†Œ 1,000~10,000ê°œ ë¬¸ì¥ ìŒ                       â”‚
  â”‚     - (query, positive, negative) í˜•íƒœ                  â”‚
  â”‚                                                         â”‚
  â”‚  [CODE] íŒŒì¸íŠœë‹ ì˜ˆì‹œ:                                  â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
  â”‚  â”‚ from sentence_transformers import (                  â”‚
  â”‚  â”‚     SentenceTransformer, InputExample, losses       â”‚
  â”‚  â”‚ )                                                    â”‚
  â”‚  â”‚ from torch.utils.data import DataLoader              â”‚
  â”‚  â”‚                                                      â”‚
  â”‚  â”‚ model = SentenceTransformer('all-MiniLM-L6-v2')     â”‚
  â”‚  â”‚                                                      â”‚
  â”‚  â”‚ # í•™ìŠµ ë°ì´í„° ì¤€ë¹„                                   â”‚
  â”‚  â”‚ train_examples = [                                   â”‚
  â”‚  â”‚     InputExample(texts=["ì§ˆë¬¸", "ì •ë‹µ ë¬¸ì„œ"], label=1.0),â”‚
  â”‚  â”‚     InputExample(texts=["ì§ˆë¬¸", "ë¬´ê´€ ë¬¸ì„œ"], label=0.0),â”‚
  â”‚  â”‚ ]                                                    â”‚
  â”‚  â”‚                                                      â”‚
  â”‚  â”‚ train_dataloader = DataLoader(train_examples, batch_size=16)â”‚
  â”‚  â”‚ train_loss = losses.CosineSimilarityLoss(model)     â”‚
  â”‚  â”‚                                                      â”‚
  â”‚  â”‚ model.fit(                                           â”‚
  â”‚  â”‚     train_objectives=[(train_dataloader, train_loss)],â”‚
  â”‚  â”‚     epochs=3                                         â”‚
  â”‚  â”‚ )                                                    â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
  â”‚                                                         â”‚
  â”‚  [TIP] íŒŒì¸íŠœë‹ ì‹œê¸°:                                   â”‚
  â”‚  * ê¸°ë³¸ ëª¨ë¸ë¡œ Recall@5 < 80% ì¼ ë•Œ                     â”‚
  â”‚  * ë„ë©”ì¸ ìš©ì–´ê°€ ë§ì•„ì„œ ê²€ìƒ‰ í’ˆì§ˆ ë‚®ì„ ë•Œ               â”‚
  â”‚  * ë°ì´í„°ê°€ ì¶©ë¶„í•  ë•Œ (ìµœì†Œ 1,000ìŒ)                    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # í•µì‹¬ í¬ì¸íŠ¸
    print_key_points([
        "- Sentence Transformers: ë¬´ë£Œ ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ë¼ì´ë¸ŒëŸ¬ë¦¬",
        "- all-MiniLM-L6-v2: ê°€ë³ê³  ë¹ ë¦„, í”„ë¡œí† íƒ€ì…ì— ì í•©",
        "- OpenAI vs ì˜¤í”ˆì†ŒìŠ¤: í’ˆì§ˆ vs ë¹„ìš©/ì»¤ìŠ¤í„°ë§ˆì´ì§•",
        "- íŒŒì¸íŠœë‹: ë„ë©”ì¸ íŠ¹í™” ì‹œ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥",
        "- ë‹¤êµ­ì–´: paraphrase-multilingual ëª¨ë¸ ì‚¬ìš©"
    ], "Sentence Transformers í•µì‹¬ í¬ì¸íŠ¸")


# ============================================================================
# 8. ì„ë² ë”© ëª¨ë¸ ë¹„êµ
# ============================================================================

def demo_embedding_model_comparison():
    """ì‹¤ìŠµ 8: ì„ë² ë”© ëª¨ë¸ ë¹„êµ - small vs large ì„±ëŠ¥/ë¹„ìš© ë¶„ì„"""
    print("\n" + "="*80)
    print("[8] ì‹¤ìŠµ 8: ì„ë² ë”© ëª¨ë¸ ë¹„êµ - small vs large ì„±ëŠ¥/ë¹„ìš© ë¶„ì„")
    print("="*80)
    print("ëª©í‘œ: ëª¨ë¸ ì„ íƒ ì‹œ ê³ ë ¤í•´ì•¼ í•  ìš”ì†Œ ì´í•´")
    print("í•µì‹¬: í’ˆì§ˆ, ë¹„ìš©, ì†ë„ì˜ Trade-off")
    
    # OpenAI ì„ë² ë”© ëª¨ë¸ ë¹„êµ
    print_section_header("OpenAI ì„ë² ë”© ëª¨ë¸ ë¹„êµ", "[CMP]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [OpenAI] ì„ë² ë”© ëª¨ë¸ ìƒì„¸ ë¹„êµ                                              â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                             â”‚
  â”‚  ëª¨ë¸                     â”‚ ì°¨ì›  â”‚ ê°€ê²©(/1Mí† í°) â”‚ ìµœëŒ€ í† í° â”‚ íŠ¹ì§•       â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚  text-embedding-3-small  â”‚ 1536 â”‚ $0.02        â”‚ 8,191   â”‚ ê°€ì„±ë¹„ ìµœê³ â”‚
  â”‚  text-embedding-3-large  â”‚ 3072 â”‚ $0.13        â”‚ 8,191   â”‚ ìµœê³  í’ˆì§ˆ  â”‚
  â”‚  text-embedding-ada-002  â”‚ 1536 â”‚ $0.10        â”‚ 8,191   â”‚ ë ˆê±°ì‹œ    â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚                                                                             â”‚
  â”‚  [!] text-embedding-3 ì‹œë¦¬ì¦ˆ íŠ¹ì§•:                                          â”‚
  â”‚  * ì°¨ì› ì¶•ì†Œ ì§€ì›: dimensions íŒŒë¼ë¯¸í„°ë¡œ 256~3072 ì§€ì • ê°€ëŠ¥                 â”‚
  â”‚  * ì˜ˆ: small ëª¨ë¸ì„ 256ì°¨ì›ìœ¼ë¡œ ì‚¬ìš© â†’ ì €ì¥ ê³µê°„ ì ˆì•½                       â”‚
  â”‚                                                                             â”‚
  â”‚  [CODE] ì°¨ì› ì¶•ì†Œ ì˜ˆì‹œ:                                                     â”‚
  â”‚  response = client.embeddings.create(                                       â”‚
  â”‚      model="text-embedding-3-small",                                        â”‚
  â”‚      input="Hello world",                                                   â”‚
  â”‚      dimensions=256  # 1536 ëŒ€ì‹  256ì°¨ì›                                    â”‚
  â”‚  )                                                                          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # ë¹„ìš© ê³„ì‚° ì˜ˆì‹œ
    print_section_header("ë¹„ìš© ê³„ì‚° ì˜ˆì‹œ", "[CALC]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [CASE] ì›”ê°„ ë¹„ìš© ì‹œë®¬ë ˆì´ì…˜                            â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  ì‹œë‚˜ë¦¬ì˜¤: RAG ì‹œìŠ¤í…œ ìš´ì˜                               â”‚
  â”‚  * ë¬¸ì„œ 10,000ê°œ ì¸ë±ì‹± (ê° 500 í† í°)                   â”‚
  â”‚  * ì¼ì¼ ì¿¼ë¦¬ 1,000ê°œ (ê° 50 í† í°)                       â”‚
  â”‚                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  1. ì´ˆê¸° ì¸ë±ì‹± ë¹„ìš© (1íšŒì„±)                            â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  * ì´ í† í°: 10,000 Ã— 500 = 5M í† í°                      â”‚
  â”‚                                                         â”‚
  â”‚  â”‚ ëª¨ë¸                     â”‚ ë¹„ìš©        â”‚             â”‚
  â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚             â”‚
  â”‚  â”‚ text-embedding-3-small  â”‚ $0.10      â”‚             â”‚
  â”‚  â”‚ text-embedding-3-large  â”‚ $0.65      â”‚             â”‚
  â”‚  â”‚ text-embedding-ada-002  â”‚ $0.50      â”‚             â”‚
  â”‚                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  2. ì›”ê°„ ì¿¼ë¦¬ ë¹„ìš©                                      â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  * ì›”ê°„ í† í°: 1,000 Ã— 50 Ã— 30 = 1.5M í† í°              â”‚
  â”‚                                                         â”‚
  â”‚  â”‚ ëª¨ë¸                     â”‚ ì›” ë¹„ìš©     â”‚ ì—° ë¹„ìš©    â”‚
  â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚  â”‚ text-embedding-3-small  â”‚ $0.03      â”‚ $0.36     â”‚
  â”‚  â”‚ text-embedding-3-large  â”‚ $0.20      â”‚ $2.34     â”‚
  â”‚                                                         â”‚
  â”‚  [ê²°ë¡ ]                                                 â”‚
  â”‚  * Smallë¡œë„ ëŒ€ë¶€ë¶„ì˜ ìš©ë„ì— ì¶©ë¶„!                      â”‚
  â”‚  * LargeëŠ” ë²•ë¥ /ì˜ë£Œ ë“± ì •ë°€ë„ê°€ ì¤‘ìš”í•  ë•Œë§Œ            â”‚
  â”‚  * ì°¨ì´: ì—°ê°„ $2 ì •ë„ (ì†Œê·œëª¨ ê¸°ì¤€)                    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # í’ˆì§ˆ ë¹„êµ (ë²¤ì¹˜ë§ˆí¬)
    print_section_header("í’ˆì§ˆ ë¹„êµ (MTEB ë²¤ì¹˜ë§ˆí¬)", "[BENCH]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [MTEB] Massive Text Embedding Benchmark ê²°ê³¼                           â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  MTEBëŠ” 56ê°œ ë°ì´í„°ì…‹ì—ì„œ ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” í‘œì¤€ ë²¤ì¹˜ë§ˆí¬      â”‚
  â”‚                                                                         â”‚
  â”‚  ëª¨ë¸                      â”‚ í‰ê·  ì ìˆ˜ â”‚ ê²€ìƒ‰ ì ìˆ˜ â”‚ í´ëŸ¬ìŠ¤í„°ë§ â”‚ ë¶„ë¥˜  â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”‚
  â”‚  text-embedding-3-large   â”‚ 64.6     â”‚ 55.4     â”‚ 49.0      â”‚ 75.5 â”‚
  â”‚  text-embedding-3-small   â”‚ 62.3     â”‚ 51.8     â”‚ 44.0      â”‚ 73.5 â”‚
  â”‚  text-embedding-ada-002   â”‚ 61.0     â”‚ 49.2     â”‚ 45.9      â”‚ 70.9 â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”‚
  â”‚  bge-large-en-v1.5        â”‚ 64.2     â”‚ 54.3     â”‚ 46.1      â”‚ 75.0 â”‚
  â”‚  e5-large-v2              â”‚ 62.2     â”‚ 50.6     â”‚ 44.5      â”‚ 73.8 â”‚
  â”‚  all-mpnet-base-v2        â”‚ 57.8     â”‚ 43.8     â”‚ 43.7      â”‚ 65.0 â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”‚
  â”‚                                                                         â”‚
  â”‚  [í•´ì„]                                                                 â”‚
  â”‚  * OpenAI large: ìµœê³  ìˆ˜ì¤€, íŠ¹íˆ ê²€ìƒ‰ íƒœìŠ¤í¬ì—ì„œ ê°•í•¨                   â”‚
  â”‚  * OpenAI small: large ëŒ€ë¹„ 2~3ì  ë‚®ìŒ, ë¹„ìš© ëŒ€ë¹„ íš¨ìœ¨ì                 â”‚
  â”‚  * bge/e5: OpenAIì™€ ë¹„ìŠ·í•œ ìˆ˜ì¤€, ë¬´ë£Œë¡œ ì‚¬ìš© ê°€ëŠ¥                       â”‚
  â”‚  * mpnet: ê°€ë³ì§€ë§Œ í’ˆì§ˆ ì°¨ì´ ìˆìŒ                                       â”‚
  â”‚                                                                         â”‚
  â”‚  [!] ì‹¤ë¬´ ì˜ë¯¸:                                                         â”‚
  â”‚  * ì ìˆ˜ 2~3ì  ì°¨ì´ = Recall@5ì—ì„œ ì•½ 1~2% ì°¨ì´                          â”‚
  â”‚  * ëŒ€ë¶€ë¶„ì˜ RAG ì‹œìŠ¤í…œì—ì„œëŠ” ì²´ê° ì–´ë ¤ì›€                                â”‚
  â”‚  * ë²•ë¥ /ì˜ë£Œì²˜ëŸ¼ 1%ë„ ì¤‘ìš”í•œ ë„ë©”ì¸ì—ì„œë§Œ large ê¶Œì¥                    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # ì‹¤ì œ ë¹„êµ ì‹¤í—˜ (ì„ íƒì )
    if os.getenv("OPENAI_API_KEY"):
        print_section_header("ì‹¤ì œ ë¹„êµ ì‹¤í—˜", "[EXP]")
        print("\n[...] small vs large ëª¨ë¸ ë¹„êµ ì¤‘...")
        
        try:
            from openai import OpenAI
            client = get_openai_client()
            
            test_texts = [
                "What is machine learning?",
                "Machine learning is a subset of artificial intelligence",
                "I love eating pizza for dinner",
            ]
            
            # Small ëª¨ë¸
            response_small = client.embeddings.create(
                model="text-embedding-3-small",
                input=test_texts
            )
            embeddings_small = [d.embedding for d in response_small.data]
            
            # Large ëª¨ë¸
            response_large = client.embeddings.create(
                model="text-embedding-3-large",
                input=test_texts
            )
            embeddings_large = [d.embedding for d in response_large.data]
            
            print(f"\ní…ìŠ¤íŠ¸:")
            for i, t in enumerate(test_texts, 1):
                print(f"  {i}. {t}")
            
            print(f"\nìœ ì‚¬ë„ ë¹„êµ (í…ìŠ¤íŠ¸ 1 vs ë‚˜ë¨¸ì§€):")
            print(f"{'â”€'*60}")
            print(f"{'ë¹„êµ ëŒ€ìƒ':<30} {'Small':<12} {'Large':<12}")
            print(f"{'â”€'*60}")
            
            # Small ìœ ì‚¬ë„
            sim_small_1_2 = cosine_similarity(embeddings_small[0], embeddings_small[1])
            sim_small_1_3 = cosine_similarity(embeddings_small[0], embeddings_small[2])
            
            # Large ìœ ì‚¬ë„
            sim_large_1_2 = cosine_similarity(embeddings_large[0], embeddings_large[1])
            sim_large_1_3 = cosine_similarity(embeddings_large[0], embeddings_large[2])
            
            print(f"{'vs ML ì„¤ëª… (ê´€ë ¨)':<30} {sim_small_1_2:<12.4f} {sim_large_1_2:<12.4f}")
            print(f"{'vs Pizza (ë¬´ê´€)':<30} {sim_small_1_3:<12.4f} {sim_large_1_3:<12.4f}")
            print(f"{'â”€'*60}")
            
            # ì°¨ì´ ë¶„ì„
            gap_small = sim_small_1_2 - sim_small_1_3
            gap_large = sim_large_1_2 - sim_large_1_3
            
            print(f"\nê´€ë ¨/ë¬´ê´€ ì ìˆ˜ ì°¨ì´:")
            print(f"  Small: {gap_small:.4f}")
            print(f"  Large: {gap_large:.4f}")
            
            if gap_large > gap_small:
                print(f"\n  â†’ Large ëª¨ë¸ì´ ê´€ë ¨/ë¬´ê´€ì„ ë” ì˜ êµ¬ë¶„í•¨!")
            else:
                print(f"\n  â†’ ì´ ì˜ˆì‹œì—ì„œëŠ” ì°¨ì´ê°€ ë¯¸ë¯¸í•¨")
            
            print(f"\nì°¨ì› ë¹„êµ:")
            print(f"  Small: {len(embeddings_small[0])} ì°¨ì›")
            print(f"  Large: {len(embeddings_large[0])} ì°¨ì›")
            
        except Exception as e:
            print(f"\n[!] ë¹„êµ ì‹¤í—˜ ì‹¤íŒ¨: {e}")
    
    # ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ
    print_section_header("ëª¨ë¸ ì„ íƒ ê°€ì´ë“œ", "[GUIDE]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [FLOW] ëª¨ë¸ ì„ íƒ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  Q1. ë¹„ìš©ì´ ê°€ì¥ ì¤‘ìš”í•œê°€?                              â”‚
  â”‚   â”‚                                                     â”‚
  â”‚   â”œâ”€ YES â†’ Q2. GPU ì„œë²„ê°€ ìˆëŠ”ê°€?                       â”‚
  â”‚   â”‚         â”‚                                           â”‚
  â”‚   â”‚         â”œâ”€ YES â†’ Sentence Transformers (ë¬´ë£Œ)       â”‚
  â”‚   â”‚         â”‚        (all-MiniLM-L6-v2 ë˜ëŠ” bge-large)  â”‚
  â”‚   â”‚         â”‚                                           â”‚
  â”‚   â”‚         â””â”€ NO â†’ text-embedding-3-small              â”‚
  â”‚   â”‚                 (ê°€ì¥ ì €ë ´í•œ API)                   â”‚
  â”‚   â”‚                                                     â”‚
  â”‚   â””â”€ NO â†’ Q3. ìµœê³  í’ˆì§ˆì´ í•„ìš”í•œê°€?                     â”‚
  â”‚            â”‚                                            â”‚
  â”‚            â”œâ”€ YES â†’ text-embedding-3-large              â”‚
  â”‚            â”‚        (ë²•ë¥ /ì˜ë£Œ/ê¸ˆìœµ)                    â”‚
  â”‚            â”‚                                            â”‚
  â”‚            â””â”€ NO â†’ text-embedding-3-small               â”‚
  â”‚                    (ëŒ€ë¶€ë¶„ì˜ RAG ì‹œìŠ¤í…œ)                â”‚
  â”‚                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [TIP] ì¶”ê°€ ê³ ë ¤ì‚¬í•­                                    â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  * ë‹¤êµ­ì–´ (í•œê¸€): OpenAI > ì˜¤í”ˆì†ŒìŠ¤ (multilingual ì œì™¸) â”‚
  â”‚  * ì˜¤í”„ë¼ì¸ í•„ìˆ˜: Sentence Transformers only            â”‚
  â”‚  * ë°ì´í„° ë³´ì•ˆ: ë¡œì»¬ ëª¨ë¸ (ì™¸ë¶€ ì „ì†¡ ë¶ˆê°€)              â”‚
  â”‚  * íŒŒì¸íŠœë‹ í•„ìš”: Sentence Transformers (OpenAI ë¶ˆê°€)   â”‚
  â”‚  * ë¹ ë¥¸ PoC: OpenAI (ì„¤ì¹˜ ì—†ì´ ë°”ë¡œ ì‚¬ìš©)               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # í•µì‹¬ í¬ì¸íŠ¸
    print_key_points([
        "- text-embedding-3-small: ê°€ì„±ë¹„ ìµœê³ , ëŒ€ë¶€ë¶„ì˜ ìš©ë„ì— ì¶©ë¶„",
        "- text-embedding-3-large: ìµœê³  í’ˆì§ˆ, ì •ë°€ë„ê°€ ì¤‘ìš”í•œ ë„ë©”ì¸ìš©",
        "- ë¹„ìš© ì°¨ì´: small $0.02 vs large $0.13 (6.5ë°° ì°¨ì´)",
        "- í’ˆì§ˆ ì°¨ì´: MTEB ê¸°ì¤€ ì•½ 2~3ì  (ì‹¤ë¬´ì—ì„œ ì²´ê° ì–´ë ¤ì›€)",
        "- ì˜¤í”ˆì†ŒìŠ¤: bge-large, e5-largeê°€ OpenAIê¸‰ ì„±ëŠ¥"
    ], "ì„ë² ë”© ëª¨ë¸ ë¹„êµ í•µì‹¬ í¬ì¸íŠ¸")


# ============================================================================
# 9. í•œê¸€-ì˜ì–´ ì„ë² ë”© ë¹„êµ
# ============================================================================

def demo_korean_english_comparison():
    """ì‹¤ìŠµ 9: í•œê¸€-ì˜ì–´ ì„ë² ë”© ë¹„êµ - ë‹¤êµ­ì–´ ì˜ë¯¸ ì •ë ¬(Alignment) ì‹¤í—˜"""
    print("\n" + "="*80)
    print("[9] ì‹¤ìŠµ 9: í•œê¸€-ì˜ì–´ ì„ë² ë”© ë¹„êµ - ë‹¤êµ­ì–´ ì˜ë¯¸ ì •ë ¬ ì‹¤í—˜")
    print("="*80)
    print("ëª©í‘œ: í•œê¸€ê³¼ ì˜ì–´ê°€ ê°™ì€ ì˜ë¯¸ì¼ ë•Œ ì„ë² ë”©ì´ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ í™•ì¸")
    print("í•µì‹¬: ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ì˜ Cross-lingual Alignment í’ˆì§ˆ ë¹„êµ")
    
    # ë‹¤êµ­ì–´ ì •ë ¬ì´ë€?
    print_section_header("ë‹¤êµ­ì–´ ì˜ë¯¸ ì •ë ¬(Cross-lingual Alignment)ì´ë€?", "[INFO]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [!] ë¬¸ì œ: í•œê¸€ ì§ˆë¬¸ìœ¼ë¡œ ì˜ì–´ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆì„ê¹Œ?   â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚  ì´ìƒì ì¸ ë‹¤êµ­ì–´ ì„ë² ë”©:                                 â”‚
  â”‚  â€¢ "íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë‹¤"                         â”‚
  â”‚  â€¢ "Python is a programming language"                  â”‚
  â”‚  â†’ ë‘ ë²¡í„°ê°€ ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œì›Œì•¼ í•¨!                â”‚
  â”‚                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [DEMO] ë²¡í„° ê³µê°„ ì‹œê°í™”                                â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                         â”‚
  â”‚         ì˜ì–´                í•œê¸€                        â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
  â”‚  â”‚         "Python"    "íŒŒì´ì¬"                 â”‚       â”‚
  â”‚  â”‚              *  â†â”€â”€â†’  *   â† ê°€ê¹Œì›€ (ì¢‹ìŒ)   â”‚       â”‚
  â”‚  â”‚                                              â”‚       â”‚
  â”‚  â”‚         "pizza"     "í”¼ì"                  â”‚       â”‚
  â”‚  â”‚              *  â†â”€â”€â†’  *   â† ê°€ê¹Œì›€ (ì¢‹ìŒ)   â”‚       â”‚
  â”‚  â”‚                                              â”‚       â”‚
  â”‚  â”‚  "Python" *                                  â”‚       â”‚
  â”‚  â”‚                         * "í”¼ì"            â”‚       â”‚
  â”‚  â”‚              â†‘                              â”‚       â”‚
  â”‚  â”‚           ë©‚ (ë‹¤ë¥¸ ì£¼ì œ)                    â”‚       â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
  â”‚                                                         â”‚
  â”‚  [TIP] ì´ê²ƒì´ ì™œ ì¤‘ìš”í•œê°€?                              â”‚
  â”‚  â€¢ í•œê¸€ RAGì—ì„œ ì˜ì–´ ê¸°ìˆ  ë¬¸ì„œ ê²€ìƒ‰ ê°€ëŠ¥               â”‚
  â”‚  â€¢ ë‹¤êµ­ì–´ ê³ ê°ì„¼í„° ì±—ë´‡ êµ¬í˜„ ê°€ëŠ¥                       â”‚
  â”‚  â€¢ ë²ˆì—­ ì—†ì´ êµì°¨ ì–¸ì–´ ê²€ìƒ‰ ê°€ëŠ¥                        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    if not os.getenv("OPENAI_API_KEY"):
        print("\n[!] OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        return
    
    # í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ì¤€ë¹„ (í•œê¸€-ì˜ì–´ ìŒ)
    test_pairs = [
        {
            "korean": "íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤",
            "english": "Python is a programming language",
            "category": "í”„ë¡œê·¸ë˜ë°"
        },
        {
            "korean": "ë‚˜ëŠ” í”¼ìë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤",
            "english": "I love eating pizza",
            "category": "ìŒì‹"
        },
        {
            "korean": "ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤",
            "english": "Machine learning is a subset of artificial intelligence",
            "category": "AI/ML"
        },
        {
            "korean": "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ìŠµë‹ˆë‹¤",
            "english": "The weather is nice today",
            "category": "ë‚ ì”¨"
        },
    ]
    
    print_section_header("í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ìŒ", "[DATA]")
    print(f"\n{'â”€'*70}")
    print(f"{'ì¹´í…Œê³ ë¦¬':<12} {'í•œê¸€':<30} {'ì˜ì–´':<30}")
    print(f"{'â”€'*70}")
    for pair in test_pairs:
        print(f"{pair['category']:<12} {pair['korean']:<30} {pair['english']:<30}")
    print(f"{'â”€'*70}")
    
    # ========== OpenAI ì„ë² ë”© í…ŒìŠ¤íŠ¸ ==========
    print_section_header("1. OpenAI ì„ë² ë”© ë‹¤êµ­ì–´ ì •ë ¬ í…ŒìŠ¤íŠ¸", "[OPENAI]")
    
    generator = EmbeddingGenerator()
    
    # ëª¨ë“  í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
    all_texts = []
    labels = []
    for pair in test_pairs:
        all_texts.extend([pair['korean'], pair['english']])
        labels.extend([f"{pair['category']}_KO", f"{pair['category']}_EN"])
    
    print("\n[...] OpenAI ì„ë² ë”© ìƒì„± ì¤‘...")
    openai_embeddings = generator.get_embeddings_batch(all_texts)
    print(f"[OK] {len(openai_embeddings)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    
    # ìœ ì‚¬ë„ ë¶„ì„
    print(f"\n[ë¶„ì„ ê²°ê³¼] í•œê¸€-ì˜ì–´ ìŒë³„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„:")
    print(f"{'â”€'*70}")
    print(f"{'ì¹´í…Œê³ ë¦¬':<12} {'í•œê¸€ vs ì˜ì–´ (ê°™ì€ ì˜ë¯¸)':<25} {'í•´ì„':<20}")
    print(f"{'â”€'*70}")
    
    openai_same_meaning_sims = []
    for i, pair in enumerate(test_pairs):
        ko_emb = openai_embeddings[i * 2]
        en_emb = openai_embeddings[i * 2 + 1]
        sim = cosine_similarity(ko_emb, en_emb)
        openai_same_meaning_sims.append(sim)
        
        interpretation = interpret_cosine_similarity(sim)
        bar = visualize_similarity_bar(sim, 20)
        print(f"{pair['category']:<12} {bar} {sim:.4f}  {interpretation}")
    
    avg_openai_same = np.mean(openai_same_meaning_sims)
    print(f"{'â”€'*70}")
    print(f"{'í‰ê· ':<12} {'':<21} {avg_openai_same:.4f}")
    
    # ë‹¤ë¥¸ ì˜ë¯¸ ë¹„êµ (í”„ë¡œê·¸ë˜ë° í•œê¸€ vs ìŒì‹ ì˜ì–´)
    print(f"\n[ëŒ€ì¡°êµ°] ë‹¤ë¥¸ ì˜ë¯¸ ìŒ ë¹„êµ:")
    print(f"{'â”€'*70}")
    
    # í”„ë¡œê·¸ë˜ë° í•œê¸€ vs ìŒì‹ ì˜ì–´
    ko_prog_emb = openai_embeddings[0]  # íŒŒì´ì¬ í•œê¸€
    en_food_emb = openai_embeddings[3]  # pizza ì˜ì–´
    sim_diff = cosine_similarity(ko_prog_emb, en_food_emb)
    bar_diff = visualize_similarity_bar(sim_diff, 20)
    print(f"'íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë°...' vs 'I love eating pizza'")
    print(f"  â†’ {bar_diff} {sim_diff:.4f} (ë‹¤ë¥¸ ì£¼ì œ)")
    
    print(f"\n[ê²°ë¡ ] OpenAI ì„ë² ë”©:")
    print(f"  â€¢ ê°™ì€ ì˜ë¯¸ (í•œê¸€-ì˜ì–´): í‰ê·  {avg_openai_same:.4f}")
    print(f"  â€¢ ë‹¤ë¥¸ ì˜ë¯¸ (í•œê¸€-ì˜ì–´): {sim_diff:.4f}")
    print(f"  â€¢ ì°¨ì´: {avg_openai_same - sim_diff:.4f}")
    
    if avg_openai_same > 0.8:
        print(f"  â†’ [v] ë›°ì–´ë‚œ ë‹¤êµ­ì–´ ì •ë ¬! í•œê¸€ RAGì— ì í•©")
    elif avg_openai_same > 0.6:
        print(f"  â†’ [~] ì–‘í˜¸í•œ ë‹¤êµ­ì–´ ì •ë ¬. ëŒ€ë¶€ë¶„ì˜ ìš©ë„ì— OK")
    else:
        print(f"  â†’ [x] ë‹¤êµ­ì–´ ì •ë ¬ì´ ì•½í•¨. multilingual ëª¨ë¸ ê¶Œì¥")
    
    # ========== Sentence Transformers ë¹„êµ ==========
    print_section_header("2. Sentence Transformers ë‹¤êµ­ì–´ ëª¨ë¸ ë¹„êµ", "[ST]")
    
    try:
        from sentence_transformers import SentenceTransformer
        st_available = True
    except ImportError:
        st_available = False
        print("\n[!] sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì„¤ì¹˜: pip install sentence-transformers")
    
    if st_available:
        # ë‹¤êµ­ì–´ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        multilingual_models = [
            ("paraphrase-multilingual-MiniLM-L12-v2", "ë‹¤êµ­ì–´ íŠ¹í™”"),
            ("all-MiniLM-L6-v2", "ì˜ì–´ ì¤‘ì‹¬ (ë¹„êµìš©)"),
        ]
        
        results = {}
        
        for model_name, description in multilingual_models:
            print(f"\n[...] '{model_name}' ëª¨ë¸ ë¡œë”© ì¤‘...")
            
            try:
                model = SentenceTransformer(model_name)
                
                # ì„ë² ë”© ìƒì„±
                st_embeddings = model.encode(all_texts)
                
                # ê°™ì€ ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°
                same_meaning_sims = []
                for i, pair in enumerate(test_pairs):
                    ko_emb = st_embeddings[i * 2]
                    en_emb = st_embeddings[i * 2 + 1]
                    sim = cosine_similarity(ko_emb.tolist(), en_emb.tolist())
                    same_meaning_sims.append(sim)
                
                avg_sim = np.mean(same_meaning_sims)
                
                # ë‹¤ë¥¸ ì˜ë¯¸ ìœ ì‚¬ë„
                sim_diff_st = cosine_similarity(
                    st_embeddings[0].tolist(),  # íŒŒì´ì¬ í•œê¸€
                    st_embeddings[3].tolist()   # pizza ì˜ì–´
                )
                
                results[model_name] = {
                    "avg_same": avg_sim,
                    "diff": sim_diff_st,
                    "gap": avg_sim - sim_diff_st,
                    "description": description
                }
                
                print(f"[OK] '{model_name}' ì™„ë£Œ")
                print(f"     ê°™ì€ ì˜ë¯¸ í‰ê· : {avg_sim:.4f}, ë‹¤ë¥¸ ì˜ë¯¸: {sim_diff_st:.4f}, ì°¨ì´: {avg_sim - sim_diff_st:.4f}")
                
            except Exception as e:
                print(f"[!] '{model_name}' ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ê²°ê³¼ ë¹„êµí‘œ
        print_section_header("3. ëª¨ë¸ë³„ ë‹¤êµ­ì–´ ì •ë ¬ ë¹„êµ", "[CMP]")
        print(f"\n{'â”€'*75}")
        print(f"{'ëª¨ë¸':<40} {'ê°™ì€ ì˜ë¯¸':<12} {'ë‹¤ë¥¸ ì˜ë¯¸':<12} {'ì°¨ì´(Gap)':<10}")
        print(f"{'â”€'*75}")
        
        # OpenAI ê²°ê³¼ ì¶”ê°€
        print(f"{'OpenAI text-embedding-3-small':<40} {avg_openai_same:<12.4f} {sim_diff:<12.4f} {avg_openai_same - sim_diff:<10.4f}")
        
        for model_name, data in results.items():
            print(f"{model_name:<40} {data['avg_same']:<12.4f} {data['diff']:<12.4f} {data['gap']:<10.4f}")
        
        print(f"{'â”€'*75}")
        
        # ìŠ¹ì ê²°ì •
        all_results = {"OpenAI": {"gap": avg_openai_same - sim_diff, "avg_same": avg_openai_same}}
        all_results.update({k: {"gap": v["gap"], "avg_same": v["avg_same"]} for k, v in results.items()})
        
        best_model = max(all_results.items(), key=lambda x: x[1]["gap"])
        
        print(f"\n[*] ë‹¤êµ­ì–´ ì •ë ¬ ìµœê³  ëª¨ë¸: {best_model[0]}")
        print(f"    (ê°™ì€ ì˜ë¯¸ ìœ ì‚¬ë„ì™€ ë‹¤ë¥¸ ì˜ë¯¸ ìœ ì‚¬ë„ ì°¨ì´ê°€ ê°€ì¥ í¼)")
    
    # ì‹¤ë¬´ ê°€ì´ë“œ
    print_section_header("í•œê¸€ RAG ì‹¤ë¬´ ê°€ì´ë“œ", "[GUIDE]")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [CASE] í•œê¸€ RAG ì‹œë‚˜ë¦¬ì˜¤ë³„ ê¶Œì¥ ëª¨ë¸                                    â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  ì‹œë‚˜ë¦¬ì˜¤ 1: í•œê¸€ ë¬¸ì„œ + í•œê¸€ ì§ˆë¬¸                                       â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  â€¢ OpenAI text-embedding-3-small (ê¶Œì¥)                                â”‚
  â”‚  â€¢ ì´ìœ : í•œê¸€ í’ˆì§ˆ ìµœìƒê¸‰, APIë¡œ ê°„í¸ ì‚¬ìš©                              â”‚
  â”‚                                                                         â”‚
  â”‚  ì‹œë‚˜ë¦¬ì˜¤ 2: ì˜ì–´ ë¬¸ì„œ + í•œê¸€ ì§ˆë¬¸ (ë˜ëŠ” ë°˜ëŒ€)                          â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  â€¢ OpenAI (ê°€ì¥ ì•ˆì „í•œ ì„ íƒ)                                            â”‚
  â”‚  â€¢ paraphrase-multilingual-MiniLM-L12-v2 (ë¹„ìš© ì ˆê° ì‹œ)                 â”‚
  â”‚  â€¢ ì´ìœ : ë‹¤êµ­ì–´ ì •ë ¬(Alignment)ì´ ì¤‘ìš”                                  â”‚
  â”‚                                                                         â”‚
  â”‚  ì‹œë‚˜ë¦¬ì˜¤ 3: í•œê¸€ ì „ìš©, ë¹„ìš© ìµœì†Œí™”                                     â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  â€¢ paraphrase-multilingual-MiniLM-L12-v2 (ë¬´ë£Œ)                         â”‚
  â”‚  â€¢ ë˜ëŠ” KoSimCSE (í•œê¸€ íŠ¹í™” ëª¨ë¸)                                       â”‚
  â”‚  â€¢ ì´ìœ : ë¡œì»¬ ì‹¤í–‰, API ë¹„ìš© ì—†ìŒ                                       â”‚
  â”‚                                                                         â”‚
  â”‚  [!] ì£¼ì˜ì‚¬í•­                                                           â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  â€¢ ì˜ì–´ ì¤‘ì‹¬ ëª¨ë¸ (all-MiniLM ë“±)ì€ í•œê¸€ì—ì„œ ì„±ëŠ¥ ì €í•˜                  â”‚
  â”‚  â€¢ ë°˜ë“œì‹œ í•œê¸€ í…ŒìŠ¤íŠ¸ í›„ ëª¨ë¸ ì„ íƒ!                                     â”‚
  â”‚  â€¢ ë„ë©”ì¸ ìš©ì–´ê°€ ë§ìœ¼ë©´ íŒŒì¸íŠœë‹ ê³ ë ¤                                   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # í•µì‹¬ í¬ì¸íŠ¸
    print_key_points([
        "- ë‹¤êµ­ì–´ ì •ë ¬: ê°™ì€ ì˜ë¯¸ì˜ í•œê¸€/ì˜ì–´ê°€ ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ì •ë„",
        "- OpenAI: ë‹¤êµ­ì–´ ì •ë ¬ ìš°ìˆ˜, í•œê¸€ RAGì— ì•ˆì „í•œ ì„ íƒ",
        "- multilingual ëª¨ë¸: ëª…ì‹œì ìœ¼ë¡œ ë‹¤êµ­ì–´ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©",
        "- ì˜ì–´ ì¤‘ì‹¬ ëª¨ë¸: all-MiniLM ë“±ì€ í•œê¸€ì—ì„œ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥",
        "- ì‹¤ë¬´ íŒ: í•œê¸€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë°˜ë“œì‹œ ê²€ì¦ í›„ ëª¨ë¸ ì„ íƒ!"
    ], "í•œê¸€-ì˜ì–´ ì„ë² ë”© ë¹„êµ í•µì‹¬ í¬ì¸íŠ¸")


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def parse_args():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="NLP ê¸°ì´ˆ ì‹¤ìŠµ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‹¤í–‰ ëª¨ë“œ:
  python nlp_basics.py          # ì „ì²´ ì‹¤ìŠµ (ê¸°ë³¸)
  python nlp_basics.py --demo   # ì¶œë ¥ ìœ„ì£¼ ë°ëª¨ (API í˜¸ì¶œ ìµœì†Œí™”)
  python nlp_basics.py --run    # ì‹¤ì œ ê³„ì‚° + ì‹œê°í™” íŒŒì¼ ì €ì¥
  python nlp_basics.py --quick  # í•µì‹¬ ì‹¤ìŠµë§Œ (1~5ë²ˆ)

ì˜ˆì‹œ:
  python nlp_basics.py --run    # ëª¨ë“  ì‹¤ìŠµ ì‹¤í–‰ + PNG íŒŒì¼ ì €ì¥
  python nlp_basics.py --quick  # ë¹ ë¥¸ ë°ëª¨ (ê¸°ì´ˆë§Œ)
        """
    )
    
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument(
        "--demo", 
        action="store_true",
        help="ì¶œë ¥ ìœ„ì£¼ ë°ëª¨ ëª¨ë“œ (API í˜¸ì¶œ ìµœì†Œí™”, ë¹ ë¥¸ ì‹¤í–‰)"
    )
    mode_group.add_argument(
        "--run", 
        action="store_true",
        help="ì‹¤ì œ ê³„ì‚° ëª¨ë“œ (ëª¨ë“  ì‹¤í—˜ ì‹¤í–‰ + ì‹œê°í™” íŒŒì¼ ì €ì¥)"
    )
    mode_group.add_argument(
        "--quick", 
        action="store_true",
        help="í•µì‹¬ ì‹¤ìŠµë§Œ (1~5ë²ˆ, API í‚¤ ì—†ì–´ë„ ì¼ë¶€ ê°€ëŠ¥)"
    )
    
    parser.add_argument(
        "--save-plots",
        action="store_true",
        default=True,
        help="ì‹œê°í™” ê²°ê³¼ë¥¼ PNG íŒŒì¼ë¡œ ì €ì¥ (ê¸°ë³¸: True)"
    )
    
    return parser.parse_args()


def main():
    """ëª¨ë“  ë°ëª¨ ì‹¤í–‰"""
    args = parse_args()
    
    # ì‹¤í–‰ ëª¨ë“œ ê²°ì •
    if args.demo:
        mode = "demo"
        mode_desc = "ë°ëª¨ ëª¨ë“œ (ì¶œë ¥ ìœ„ì£¼)"
    elif args.run:
        mode = "run"
        mode_desc = "ì‹¤í–‰ ëª¨ë“œ (ì „ì²´ ê³„ì‚° + íŒŒì¼ ì €ì¥)"
    elif args.quick:
        mode = "quick"
        mode_desc = "í€µ ëª¨ë“œ (í•µì‹¬ ì‹¤ìŠµ 1~5ë²ˆë§Œ)"
    else:
        mode = "full"
        mode_desc = "ì „ì²´ ì‹¤ìŠµ"
    
    print("\n" + "="*80)
    print("[LAB] NLP ê¸°ì´ˆ ì‹¤ìŠµ")
    print(f"[MODE] {mode_desc}")
    print("="*80)
    
    print("\n[LIST] ì‹¤ìŠµ í•­ëª©:")
    print("  1. tiktokenìœ¼ë¡œ í† í° ì´í•´í•˜ê¸° - GPTê°€ í…ìŠ¤íŠ¸ë¥¼ ì–´ë–»ê²Œ ë³´ëŠ”ê°€")
    print("  2. NLTK ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ - í† í°í™”, ë¶ˆìš©ì–´, í‘œì œì–´ ì¶”ì¶œ")
    print("  3. OpenAI ì„ë² ë”© ìƒì„± - í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜")
    print("  4. ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° - ë²¡í„° ê°„ ìœ ì‚¬ì„± ì¸¡ì •")
    print("  5. ê°„ë‹¨í•œ ê²€ìƒ‰ ì—”ì§„ - ì˜ë¯¸ ê¸°ë°˜ ë¬¸ì¥ ê²€ìƒ‰")
    
    if mode != "quick":
        print("  6. ì„ë² ë”© ì‹œê°í™” - t-SNEë¡œ ë²¡í„° ê³µê°„ ì´í•´í•˜ê¸°")
        print("  7. ì˜¤í”ˆì†ŒìŠ¤ ì„ë² ë”© ëª¨ë¸ - Sentence Transformers ì†Œê°œ")
        print("  8. ì„ë² ë”© ëª¨ë¸ ë¹„êµ - small vs large ì„±ëŠ¥/ë¹„ìš© ë¶„ì„")
        print("  9. í•œê¸€-ì˜ì–´ ì„ë² ë”© ë¹„êµ - ë‹¤êµ­ì–´ ì˜ë¯¸ ì •ë ¬ ì‹¤í—˜ ğŸ†•")
    
    if mode == "demo":
        print("\n[INFO] ë°ëª¨ ëª¨ë“œ: API í˜¸ì¶œì„ ìµœì†Œí™”í•˜ì—¬ ë¹ ë¥´ê²Œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    elif mode == "run":
        print("\n[INFO] ì‹¤í–‰ ëª¨ë“œ: ëª¨ë“  ì‹¤í—˜ì„ ì‹¤ì œë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
        print(f"       ì‹œê°í™” íŒŒì¼ ì €ì¥: {args.save_plots}")
    
    # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    download_nltk_data()
    
    try:
        # 1. tiktoken ë°ëª¨ (í•­ìƒ ì‹¤í–‰)
        demo_tiktoken()
        
        # 2. ì „ì²˜ë¦¬ ë°ëª¨ (í•­ìƒ ì‹¤í–‰)
        demo_preprocessing()
        
        # 3. ì„ë² ë”© ë°ëª¨
        if mode != "demo" or os.getenv("OPENAI_API_KEY"):
            demo_embeddings()
        else:
            print("\n[SKIP] ì‹¤ìŠµ 3: OPENAI_API_KEYê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        # 4. ìœ ì‚¬ë„ ê³„ì‚° ë°ëª¨
        if mode != "demo" or os.getenv("OPENAI_API_KEY"):
            demo_similarity()
        else:
            print("\n[SKIP] ì‹¤ìŠµ 4: OPENAI_API_KEYê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        # 5. ê²€ìƒ‰ ì—”ì§„ ë°ëª¨
        if mode != "demo" or os.getenv("OPENAI_API_KEY"):
            demo_search_engine()
        else:
            print("\n[SKIP] ì‹¤ìŠµ 5: OPENAI_API_KEYê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        # 6~9: ì‹¬í™” ì‹¤ìŠµ (quick ëª¨ë“œì—ì„œëŠ” ê±´ë„ˆëœ€)
        if mode != "quick":
            # 6. ì„ë² ë”© ì‹œê°í™”
            demo_embedding_visualization()
            
            # 7. Sentence Transformers
            demo_sentence_transformers()
            
            # 8. ì„ë² ë”© ëª¨ë¸ ë¹„êµ
            demo_embedding_model_comparison()
            
            # 9. í•œê¸€-ì˜ì–´ ì„ë² ë”© ë¹„êµ (ìƒˆë¡œ ì¶”ê°€!)
            if mode == "run" or os.getenv("OPENAI_API_KEY"):
                demo_korean_english_comparison()
            else:
                print("\n[SKIP] ì‹¤ìŠµ 9: --run ëª¨ë“œ ë˜ëŠ” OPENAI_API_KEY í•„ìš”")
        
        # ì™„ë£Œ ë©”ì‹œì§€
        print("\n" + "="*80)
        print("[OK] ëª¨ë“  ì‹¤ìŠµ ì™„ë£Œ!")
        print("="*80)
        
        print("\n[INFO] ì˜¤ëŠ˜ ë°°ìš´ ë‚´ìš© ìš”ì•½:")
        print("  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print("  â”‚ 1. í† í°: GPTê°€ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë‹¨ìœ„ (ë¹„ìš© ê³„ì‚° ê¸°ì¤€)")
        print("  â”‚ 2. ì „ì²˜ë¦¬: BM25ì—” í•„ìˆ˜, ì„ë² ë”©ì—” ë¶ˆí•„ìš” âš ï¸")
        print("  â”‚ 3. ì„ë² ë”©: í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜")
        print("  â”‚ 4. ì½”ì‚¬ì¸ ìœ ì‚¬ë„: ë²¡í„° ê°„ ìœ ì‚¬ì„± ì¸¡ì • (-1 ~ 1)")
        print("  â”‚ 5. ì˜ë¯¸ ê²€ìƒ‰: ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬ ë¬¸ì„œ ì°¾ê¸°")
        
        if mode != "quick":
            print("  â”‚ 6. ì‹œê°í™”: t-SNE/UMAPìœ¼ë¡œ ë²¡í„° ê³µê°„ ì´í•´")
            print("  â”‚ 7. ì˜¤í”ˆì†ŒìŠ¤: Sentence Transformersë¡œ ë¹„ìš© ì ˆê°")
            print("  â”‚ 8. ëª¨ë¸ ì„ íƒ: í’ˆì§ˆ/ë¹„ìš© Trade-off ì´í•´")
            print("  â”‚ 9. ë‹¤êµ­ì–´: í•œê¸€-ì˜ì–´ ì„ë² ë”© ì •ë ¬ í’ˆì§ˆ í™•ì¸ ğŸ†•")
        
        print("  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
        # ìƒì„±ëœ íŒŒì¼ ì•ˆë‚´
        if mode == "run":
            output_dir = Path(__file__).parent
            print(f"\n[FILE] ìƒì„±ëœ íŒŒì¼:")
            
            tsne_file = output_dir / "embedding_tsne_demo.png"
            if tsne_file.exists():
                print(f"   - {tsne_file.name} : t-SNE ì‹œê°í™”")
        
        print("\n[TIP] ë‹¤ìŒ ë‹¨ê³„:")
        print("   - lab02/vector_db.py : Vector DB (ChromaDB)ë¡œ ëŒ€ìš©ëŸ‰ ê²€ìƒ‰")
        print("   - lab03/rag_basic.py : RAG ì‹œìŠ¤í…œ êµ¬ì¶• (ê²€ìƒ‰ + LLM)")
        
        if mode == "quick":
            print("\n[TIP] ì‹¬í™” ì‹¤ìŠµì„ ì›í•˜ì‹œë©´:")
            print("   python nlp_basics.py --run")
        
    except Exception as e:
        print(f"\n[X] ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
