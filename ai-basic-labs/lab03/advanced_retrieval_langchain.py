"""
Advanced RAG (Retrieval-Augmented Generation) ì‹¤ìŠµ
ì‹¤ë¬´ RAG íŒ€ë“¤ì´ í•˜ëŠ” ìµœì í™” ê¸°ëŠ¥ ì§ì ‘ ë‹¤ë£¨ê¸°

ì‹¤ìŠµ í•­ëª©:
1. Sparse + Dense í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
2. Re-ranking ì ìš© (BGE reranker)
3. Multi-hop ì§ˆì˜: ë‘ ë‹¨ê³„ ê²€ìƒ‰ìœ¼ë¡œ ë‹µ ì°¾ê¸°
4. Chunk size ì‹¤í—˜: 512 vs 1024 vs 2048
5. ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬ ì‹¤í—˜
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import time
from dataclasses import dataclass

# LangChain
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate

# Sparse ê²€ìƒ‰ (BM25)
from rank_bm25 import BM25Okapi

# Re-ranking
from sentence_transformers import CrossEncoder

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import chromadb
from openai import OpenAI
from dotenv import load_dotenv
import tiktoken
import pdfplumber
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼ ë¡œë“œ
project_root = Path(__file__).parent.parent
load_dotenv(dotenv_path=project_root / '.env')

# ê³µí†µ ìœ í‹¸ë¦¬í‹° importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(project_root))

# ê³µí†µ ë°ì´í„° ì„í¬íŠ¸
from shared_data import SAMPLE_TEXT, MIN_TEXT_LENGTH, get_sample_or_document_text


@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    score: float
    metadata: Dict[str, Any]
    rank: int


# SAMPLE_TEXTëŠ” shared_data.pyì—ì„œ ì„í¬íŠ¸ë¨


class DocumentProcessor:
    """ë¬¸ì„œ ì²˜ë¦¬ ë° ì²­í‚¹ (rag_basic.pyì˜ TextChunker ë°©ì‹ ì‚¬ìš©)"""
    
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        """
        Args:
            chunk_size: ì²­í¬ í¬ê¸° (512, 1024, 2048 ë“±)
            chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹¨ í¬ê¸°
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def load_pdf(self, file_path: str) -> str:
        """PDF íŒŒì¼ ë¡œë“œ"""
        text = ""
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
        return text
    
    def load_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì§ì ‘ ë¡œë“œ (PDF ì—†ì´ ì‹¤ìŠµ ê°€ëŠ¥)"""
        return text
    
    def chunk_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ë¬¸ì¥ ê²½ê³„ ê³ ë ¤)"""
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + self.chunk_size
            
            # í…ìŠ¤íŠ¸ ëì´ë©´ ê·¸ëƒ¥ ì¶”ê°€
            if end >= text_length:
                chunk = text[start:].strip()
                if chunk:
                    chunks.append(chunk)
                break
            
            # ë¬¸ì¥ ê²½ê³„ ì°¾ê¸°
            best_end = -1
            
            # 1ìˆœìœ„: ë‹¨ë½ ë (ë¹ˆ ì¤„)
            double_newline = text.rfind('\n\n', start, end + 50)
            if double_newline != -1:
                best_end = double_newline + 2
            
            # 2ìˆœìœ„: ë¬¸ì¥ ë (ë§ˆì¹¨í‘œ + ì¤„ë°”ê¿ˆ)
            if best_end == -1:
                for i in range(end, max(start, end - 100), -1):
                    if i < text_length - 1 and text[i] == '.' and text[i+1] == '\n':
                        best_end = i + 2
                        break
            
            # 3ìˆœìœ„: ë§ˆì¹¨í‘œ + ê³µë°±
            if best_end == -1:
                period_space = text.rfind('. ', start, end + 30)
                if period_space != -1:
                    best_end = period_space + 2
            
            # 4ìˆœìœ„: ì¤„ë°”ê¿ˆ
            if best_end == -1:
                newline = text.rfind('\n', start, end + 20)
                if newline != -1:
                    best_end = newline + 1
            
            # 5ìˆœìœ„: ê³µë°±
            if best_end == -1:
                space = text.rfind(' ', start, end)
                if space != -1 and space > start + self.chunk_size // 2:
                    best_end = space + 1
            
            # ìµœì¢…: ê°•ì œë¡œ ìë¥´ê¸°
            if best_end == -1:
                best_end = end
            
            chunk = text[start:best_end].strip()
            if chunk:
                chunks.append(chunk)
            
            # ë‹¤ìŒ ì²­í¬ ì‹œì‘ ìœ„ì¹˜ (ì˜¤ë²„ë© ì ìš©)
            next_start = best_end - self.chunk_overlap
            
            # ì§„í–‰ì´ ì—†ìœ¼ë©´ ê°•ì œë¡œ ì•ìœ¼ë¡œ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
            if next_start <= start:
                next_start = best_end
            
            start = next_start
        
        return chunks
    
    def create_chunks(self, text: str, metadata: Optional[Dict] = None) -> List[Document]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ Document ê°ì²´ ìƒì„±"""
        chunks = self.chunk_text(text)
        
        documents = []
        for i, chunk in enumerate(chunks):
            doc_metadata = metadata.copy() if metadata else {}
            doc_metadata.update({
                "chunk_id": i,
                "chunk_size": len(chunk),
                "total_chunks": len(chunks)
            })
            documents.append(Document(page_content=chunk, metadata=doc_metadata))
        
        return documents


class HybridRetriever:
    """Sparse (BM25) + Dense (Vector) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
    
    # í•œê¸€ ì¡°ì‚¬ íŒ¨í„´ (ê°„ë‹¨ ë²„ì „)
    KOREAN_PARTICLES = [
        'ì´ë€', 'ì´ë€?', 'ë€', 'ë€?', 'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼',
        'ì˜', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ê¹Œì§€',
        'ë¶€í„°', 'ì´ë‹¤', 'ì…ë‹ˆë‹¤', 'ì¸ê°€', 'ì¸ê°€?', 'ì¸ì§€', 'í•˜ëŠ”', 'ë˜ëŠ”'
    ]
    
    @staticmethod
    def tokenize_korean(text: str) -> List[str]:
        """
        ê°„ë‹¨í•œ í•œê¸€ í† í°í™” (êµìœ¡ìš©)
        
        ì²˜ë¦¬ ê³¼ì •:
        - êµ¬ë‘ì  ì œê±°
        - ê³µë°± ë¶„ë¦¬
        - ì¼ë°˜ì ì¸ ì¡°ì‚¬ ì œê±°
        
        âš ï¸ í•œê³„:
        - ì‹¤ì œ í˜•íƒœì†Œ ë¶„ì„ì´ ì•„ë‹˜ (ë‹¨ìˆœ ê·œì¹™ ê¸°ë°˜ ì¡°ì‚¬ ì œê±°)
        - ë³µì¡í•œ ì–´ë¯¸ ì²˜ë¦¬ ë¶ˆê°€ ("í–ˆìŠµë‹ˆë‹¤", "ê² ìŠµë‹ˆë‹¤" ë“±)
        - ì˜ˆ: "í•™ìŠµí•˜ëŠ”" â†’ "í•™ìŠµí•˜" (ë¶ˆì™„ì „)
        
        ğŸ”§ ì‹¤ë¬´ ê¶Œì¥:
        - KoNLPy (Mecab, Okt, Komoran ë“±) í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©
        - pip install konlpy
        
        ğŸ“š êµìœ¡ ëª©ì :
        - BM25ì˜ í† í° ë§¤ì¹­ ì›ë¦¬ ì´í•´
        - í•œê¸€ NLPì˜ ì–´ë ¤ì›€ ì¸ì‹
        """
        import re
        
        # êµ¬ë‘ì ì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜
        text = re.sub(r'[.,!?;:()"\'\[\]{}]', ' ', text)
        
        # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        tokens = text.split()
        
        # ì¡°ì‚¬ ì œê±° ì‹œë„
        cleaned_tokens = []
        for token in tokens:
            cleaned = token
            # ê¸´ ì¡°ì‚¬ë¶€í„° ì œê±° ì‹œë„
            for particle in sorted(HybridRetriever.KOREAN_PARTICLES, key=len, reverse=True):
                if cleaned.endswith(particle) and len(cleaned) > len(particle):
                    cleaned = cleaned[:-len(particle)]
                    break
            if cleaned:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹ˆë©´ ì¶”ê°€
                cleaned_tokens.append(cleaned)
        
        return cleaned_tokens
    
    def __init__(
        self,
        documents: List[Document],
        embeddings: OpenAIEmbeddings,
        persist_directory: str = "./chroma_db",
        collection_name: str = "hybrid_search"
    ):
        """
        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            embeddings: ì„ë² ë”© ëª¨ë¸
            persist_directory: Chroma DB ì €ì¥ ê²½ë¡œ
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        """
        self.documents = documents
        self.embeddings = embeddings
        
        # Dense ê²€ìƒ‰: Vector DB (Chroma)
        print(f"Dense ê²€ìƒ‰ ì¤€ë¹„ ì¤‘... (ì»¬ë ‰ì…˜: {collection_name})")
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ (ê¹¨ë—í•œ ì‹œì‘)
        try:
            chroma_client = chromadb.PersistentClient(path=persist_directory)
            chroma_client.delete_collection(name=collection_name)
        except:
            pass  # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ë¬´ì‹œ
        
        self.vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory=persist_directory,
            collection_name=collection_name
        )
        
        # Sparse ê²€ìƒ‰: BM25 (í•œê¸€ í† í°í™” ì ìš©)
        print("Sparse ê²€ìƒ‰ ì¤€ë¹„ ì¤‘... (BM25 + í•œê¸€ í† í°í™”)")
        self.corpus = [doc.page_content for doc in documents]
        self.tokenized_corpus = [self.tokenize_korean(doc) for doc in self.corpus]
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        
        print(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì¤€ë¹„ ì™„ë£Œ (ë¬¸ì„œ ìˆ˜: {len(documents)})")
    
    def sparse_search(self, query: str, k: int = 10) -> List[SearchResult]:
        """BM25 Sparse ê²€ìƒ‰ (í•œê¸€ í† í°í™” ì ìš©)"""
        tokenized_query = self.tokenize_korean(query)
        scores = self.bm25.get_scores(tokenized_query)
        
        # ì‹¤ì œ ë¬¸ì„œ ìˆ˜ë§Œí¼ë§Œ ë°˜í™˜
        k = min(k, len(self.documents))
        
        # ìƒìœ„ kê°œ ê²°ê³¼ (ì ìˆ˜ > 0ì¸ ê²ƒë§Œ)
        top_indices = np.argsort(scores)[::-1]
        
        results = []
        rank = 1
        for idx in top_indices:
            score = float(scores[idx])
            
            # ì ìˆ˜ê°€ 0ì´ë©´ í‚¤ì›Œë“œ ë§¤ì¹­ ì—†ìŒ - ê²°ê³¼ì—ì„œ ì œì™¸
            if score == 0:
                continue
            
            results.append(SearchResult(
                content=self.documents[idx].page_content,
                score=score,
                metadata={**self.documents[idx].metadata, "matched_tokens": tokenized_query},
                rank=rank
            ))
            rank += 1
            
            if len(results) >= k:
                break
        
        # ë§¤ì¹­ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì•ˆë‚´ ë©”ì‹œì§€ í¬í•¨
        if not results:
            # ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ë¬¸ì„œë¼ë„ ë°˜í™˜ (ì°¸ê³ ìš©)
            results.append(SearchResult(
                content=f"[í‚¤ì›Œë“œ '{' '.join(tokenized_query)}' ë§¤ì¹­ ì—†ìŒ]",
                score=0.0,
                metadata={"no_match": True},
                rank=1
            ))
        
        return results
    
    def dense_search(self, query: str, k: int = 10) -> List[SearchResult]:
        """Vector DB Dense ê²€ìƒ‰"""
        # ì‹¤ì œ ë¬¸ì„œ ìˆ˜ë§Œí¼ë§Œ ë°˜í™˜
        k = min(k, len(self.documents))
        docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=k)
        
        results = []
        for rank, (doc, score) in enumerate(docs_with_scores):
            # scoreëŠ” L2 ê±°ë¦¬(distance)ì´ë¯€ë¡œ 0~1 ì ìˆ˜ë¡œ ë³€í™˜
            # 1/(1+distance) ë°©ì‹: í•­ìƒ 0~1 ë²”ìœ„
            # âš ï¸ ì´ê²ƒì€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ì•„ë‹˜! (ê±°ë¦¬ ê¸°ë°˜ ì ìˆ˜)
            distance_score = 1 / (1 + score)
            results.append(SearchResult(
                content=doc.page_content,
                score=float(distance_score),
                metadata={**doc.metadata, "raw_distance": float(score)},
                rank=rank + 1
            ))
        
        return results
    
    def hybrid_search(
        self,
        query: str,
        k: int = 10,
        alpha: float = 0.5
    ) -> List[SearchResult]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Sparse + Dense)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            alpha: Dense ê°€ì¤‘ì¹˜ (0~1, 1-alphaê°€ Sparse ê°€ì¤‘ì¹˜)
        
        ì ìˆ˜ ê³„ì‚° ë°©ì‹:
            - Sparse: BM25 ì ìˆ˜ë¥¼ ìµœëŒ€ê°’ìœ¼ë¡œ ì •ê·œí™” í›„ (1-alpha) ê°€ì¤‘ì¹˜
            - Dense: ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ìµœëŒ€ê°’ìœ¼ë¡œ ì •ê·œí™” í›„ alpha ê°€ì¤‘ì¹˜
            - ìµœì¢…: ë‘ ì ìˆ˜ í•©ì‚° (0~1 ë²”ìœ„)
        """
        # Sparse ë° Dense ê²€ìƒ‰ ìˆ˜í–‰
        sparse_results = self.sparse_search(query, k=k*2)
        dense_results = self.dense_search(query, k=k*2)
        
        # ì›ë³¸ ì ìˆ˜ ì €ì¥ (ë””ë²„ê¹…/ë¶„ì„ìš©)
        sparse_raw_scores = {r.content: r.score for r in sparse_results}
        dense_raw_scores = {r.content: r.score for r in dense_results}
        
        # ì ìˆ˜ ì •ê·œí™”
        sparse_scores = [r.score for r in sparse_results]
        dense_scores = [r.score for r in dense_results]
        
        sparse_max = max(sparse_scores) if sparse_scores and max(sparse_scores) > 0 else 1.0
        dense_max = max(dense_scores) if dense_scores and max(dense_scores) > 0 else 1.0
        
        # ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
        combined = {}
        
        for result in sparse_results:
            content = result.content
            normalized_score = (result.score / sparse_max) * (1 - alpha)
            combined[content] = SearchResult(
                content=content,
                score=normalized_score,
                metadata={
                    **result.metadata,
                    "sparse_raw": result.score,
                    "sparse_max": sparse_max,
                    "sparse_norm": normalized_score,
                },
                rank=0
            )
        
        for result in dense_results:
            content = result.content
            normalized_score = (result.score / dense_max) * alpha
            
            if content in combined:
                # ê¸°ì¡´ Sparse ê²°ê³¼ì— Dense ì ìˆ˜ ì¶”ê°€
                combined[content].score += normalized_score
                combined[content].metadata["dense_raw"] = result.score
                combined[content].metadata["dense_max"] = dense_max
                combined[content].metadata["dense_norm"] = normalized_score
            else:
                combined[content] = SearchResult(
                    content=content,
                    score=normalized_score,
                    metadata={
                        **result.metadata,
                        "dense_raw": result.score,
                        "dense_max": dense_max,
                        "dense_norm": normalized_score,
                    },
                    rank=0
                )
        
        # ì ìˆ˜ë¡œ ì •ë ¬
        sorted_results = sorted(combined.values(), key=lambda x: x.score, reverse=True)
        
        # ìˆœìœ„ ì¬í• ë‹¹
        for rank, result in enumerate(sorted_results[:k]):
            result.rank = rank + 1
        
        return sorted_results[:k]


class Reranker:
    """Re-ranking ëª¨ë¸ (ê²½ëŸ‰ ëª¨ë¸ ì‚¬ìš©)"""
    
    def __init__(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"):
        """
        Args:
            model_name: ë¦¬ë­í‚¹ ëª¨ë¸ ì´ë¦„
            ê¸°ë³¸ê°’: cross-encoder/ms-marco-MiniLM-L-6-v2 (ê²½ëŸ‰, ~80MB)
            ëŒ€ì•ˆ: BAAI/bge-reranker-base (ê³ ì„±ëŠ¥, ~500MB)
        """
        print(f"[...] Re-ranker ëª¨ë¸ ë¡œë”© ì¤‘... ({model_name})")
        
        # HuggingFace SSL ê²€ì¦ ìš°íšŒ ì„¤ì • (ì—¬ëŸ¬ ë°©ë²• ì ìš©)
        import ssl
        import os
        import warnings
        
        # 1. SSL ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²€ì¦í•˜ì§€ ì•ŠëŠ” ëª¨ë“œë¡œ ë³€ê²½
        try:
            _create_unverified_https_context = ssl._create_unverified_context
        except AttributeError:
            pass
        else:
            ssl._create_default_https_context = _create_unverified_https_context
        
        # 2. í™˜ê²½ ë³€ìˆ˜ë¡œ ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™”
        os.environ['CURL_CA_BUNDLE'] = ''
        os.environ['REQUESTS_CA_BUNDLE'] = ''
        os.environ['SSL_CERT_FILE'] = ''
        
        # 3. urllib3 ê²½ê³  ì–µì œ
        warnings.filterwarnings('ignore', message='Unverified HTTPS request')
        
        # 4. requests ì„¸ì…˜ì˜ SSL ê²€ì¦ ë¹„í™œì„±í™” (ê°€ì¥ íš¨ê³¼ì )
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        from urllib3.poolmanager import PoolManager
        
        class SSLAdapter(HTTPAdapter):
            def init_poolmanager(self, *args, **kwargs):
                kwargs['ssl_version'] = ssl.PROTOCOL_TLS
                kwargs['cert_reqs'] = ssl.CERT_NONE
                return super().init_poolmanager(*args, **kwargs)
        
        # requestsì˜ ê¸°ë³¸ ì„¸ì…˜ ìˆ˜ì •
        session = requests.Session()
        session.verify = False
        adapter = SSLAdapter()
        session.mount('https://', adapter)
        session.mount('http://', adapter)
        
        # huggingface_hubì´ ì‚¬ìš©í•  ì„¸ì…˜ íŒ¨ì¹˜
        try:
            from huggingface_hub import configure_http_backend
            configure_http_backend(backend_factory=lambda: session)
        except:
            pass
        
        # 5. urllib3ì˜ SSL ê²½ê³  ì™„ì „íˆ ì œê±°
        try:
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        except:
            pass
        
        self.model = CrossEncoder(model_name)
        print("[OK] Re-ranker ì¤€ë¹„ ì™„ë£Œ")
    
    def rerank(
        self,
        query: str,
        results: List[SearchResult],
        top_k: int = 5
    ) -> List[SearchResult]:
        """
        ê²€ìƒ‰ ê²°ê³¼ ì¬ìˆœìœ„í™”
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            results: ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
        """
        import math
        
        if not results:
            return []
        
        # ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ìƒì„±
        pairs = [[query, result.content] for result in results]
        
        # Re-ranking ì ìˆ˜ ê³„ì‚° (Cross-EncoderëŠ” ë¡œì§“ ì ìˆ˜ ë°˜í™˜: -infinity ~ +infinity)
        raw_scores = self.model.predict(pairs)
        
        # ë¡œì§“ ì ìˆ˜ë¥¼ 0~1 ë²”ìœ„ì˜ í™•ë¥ ë¡œ ë³€í™˜ (sigmoid í•¨ìˆ˜)
        # ì´ë ‡ê²Œ í•˜ë©´ Before/After ì ìˆ˜ ìŠ¤ì¼€ì¼ì´ ì¼ì¹˜í•˜ì—¬ ë¹„êµ ê°€ëŠ¥
        normalized_scores = [1 / (1 + math.exp(-s)) for s in raw_scores]
        
        # ì ìˆ˜ ì—…ë°ì´íŠ¸ ë° ì¬ì •ë ¬
        reranked = []
        for result, norm_score, raw_score in zip(results, normalized_scores, raw_scores):
            reranked.append(SearchResult(
                content=result.content,
                score=float(norm_score),  # ì •ê·œí™”ëœ ì ìˆ˜ (0~1 ë²”ìœ„)
                metadata={**result.metadata, "raw_rerank_score": float(raw_score)},
                rank=0
            ))
        
        reranked.sort(key=lambda x: x.score, reverse=True)
        
        # ìˆœìœ„ ì¬í• ë‹¹
        for rank, result in enumerate(reranked[:top_k]):
            result.rank = rank + 1
        
        return reranked[:top_k]


class MultiHopRetriever:
    """Multi-hop ì§ˆì˜: ë‘ ë‹¨ê³„ ê²€ìƒ‰ìœ¼ë¡œ ë‹µ ì°¾ê¸°"""
    
    def __init__(
        self,
        retriever: HybridRetriever,
        reranker: Optional[Reranker] = None,
        llm: Optional[ChatOpenAI] = None
    ):
        """
        Args:
            retriever: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸°
            reranker: ë¦¬ë­ì»¤ (ì„ íƒ)
            llm: LLM ëª¨ë¸ (ì¿¼ë¦¬ ë¶„í•´ìš©)
        """
        self.retriever = retriever
        self.reranker = reranker
        # SSL ê²€ì¦ ìš°íšŒ ì„¤ì •ì´ ì ìš©ëœ LLM ìƒì„±
        if llm is None:
            import httpx
            http_client = httpx.Client(verify=False)
            self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, http_client=http_client)
        else:
            self.llm = llm
        
        # ì¿¼ë¦¬ ë¶„í•´ í”„ë¡¬í”„íŠ¸
        self.decompose_prompt = ChatPromptTemplate.from_template(
            """ë‹¤ìŒ ì§ˆë¬¸ì„ ë‘ ê°œì˜ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ì„¸ìš”.
ì²« ë²ˆì§¸ ì§ˆë¬¸ì˜ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ë‘ ë²ˆì§¸ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

ì›ë³¸ ì§ˆë¬¸: {question}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
1. [ì²« ë²ˆì§¸ í•˜ìœ„ ì§ˆë¬¸]
2. [ë‘ ë²ˆì§¸ í•˜ìœ„ ì§ˆë¬¸]

í•˜ìœ„ ì§ˆë¬¸:"""
        )
    
    def decompose_query(self, query: str) -> List[str]:
        """ë³µì¡í•œ ì¿¼ë¦¬ë¥¼ í•˜ìœ„ ì¿¼ë¦¬ë¡œ ë¶„í•´"""
        print(f"\n[>>>] ì¿¼ë¦¬ ë¶„í•´ ì¤‘: {query}")
        
        chain = self.decompose_prompt | self.llm
        response = chain.invoke({"question": query})
        
        # ì‘ë‹µ íŒŒì‹±
        lines = response.content.strip().split('\n')
        sub_queries = []
        for line in lines:
            line = line.strip()
            if line and (line[0].isdigit() or line.startswith('-')):
                # "1. " ë˜ëŠ” "- " ì œê±°
                sub_query = line.split('.', 1)[-1].strip() if '.' in line else line[1:].strip()
                sub_queries.append(sub_query)
        
        print(f"  -> í•˜ìœ„ ì§ˆë¬¸ 1: {sub_queries[0] if len(sub_queries) > 0 else 'N/A'}")
        print(f"  -> í•˜ìœ„ ì§ˆë¬¸ 2: {sub_queries[1] if len(sub_queries) > 1 else 'N/A'}")
        
        return sub_queries
    
    def multi_hop_search(
        self,
        query: str,
        k_per_hop: int = 5,
        use_reranker: bool = True
    ) -> Tuple[List[SearchResult], Dict[str, Any]]:
        """
        Multi-hop ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ì›ë³¸ ì¿¼ë¦¬
            k_per_hop: ê° í™‰ë‹¹ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
            use_reranker: ë¦¬ë­ì»¤ ì‚¬ìš© ì—¬ë¶€
        
        Returns:
            (ìµœì¢… ê²€ìƒ‰ ê²°ê³¼, ë©”íƒ€ë°ì´í„°)
        """
        metadata = {"sub_queries": [], "hop_results": []}
        
        # 1ë‹¨ê³„: ì¿¼ë¦¬ ë¶„í•´
        sub_queries = self.decompose_query(query)
        metadata["sub_queries"] = sub_queries
        
        if len(sub_queries) < 2:
            print("[!] ì¿¼ë¦¬ ë¶„í•´ ì‹¤íŒ¨, ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì²´")
            results = self.retriever.hybrid_search(query, k=k_per_hop)
            if use_reranker and self.reranker:
                results = self.reranker.rerank(query, results, top_k=k_per_hop)
            return results, metadata
        
        # 2ë‹¨ê³„: ì²« ë²ˆì§¸ í™‰ ê²€ìƒ‰
        print(f"\n[>>>] Hop 1 ê²€ìƒ‰ ì¤‘...")
        hop1_results = self.retriever.hybrid_search(sub_queries[0], k=k_per_hop)
        if use_reranker and self.reranker:
            hop1_results = self.reranker.rerank(sub_queries[0], hop1_results, top_k=k_per_hop)
        
        metadata["hop_results"].append({
            "query": sub_queries[0],
            "num_results": len(hop1_results)
        })
        
        # 3ë‹¨ê³„: ì²« ë²ˆì§¸ í™‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë‘ ë²ˆì§¸ í™‰ ê²€ìƒ‰
        print(f"[>>>] Hop 2 ê²€ìƒ‰ ì¤‘...")
        hop1_context = "\n\n".join([r.content[:200] + "..." for r in hop1_results[:3]])
        enhanced_query = f"{sub_queries[1]}\n\nì°¸ê³  ì •ë³´:\n{hop1_context}"
        
        hop2_results = self.retriever.hybrid_search(enhanced_query, k=k_per_hop)
        if use_reranker and self.reranker:
            hop2_results = self.reranker.rerank(sub_queries[1], hop2_results, top_k=k_per_hop)
        
        metadata["hop_results"].append({
            "query": sub_queries[1],
            "num_results": len(hop2_results)
        })
        
        # 4ë‹¨ê³„: ë‘ í™‰ì˜ ê²°ê³¼ ë³‘í•© (ì¤‘ë³µ ì œê±°)
        combined = {}
        for result in hop1_results + hop2_results:
            if result.content not in combined:
                combined[result.content] = result
            else:
                # ë” ë†’ì€ ì ìˆ˜ ìœ ì§€
                if result.score > combined[result.content].score:
                    combined[result.content] = result
        
        final_results = sorted(combined.values(), key=lambda x: x.score, reverse=True)[:k_per_hop]
        
        # ìˆœìœ„ ì¬í• ë‹¹
        for rank, result in enumerate(final_results):
            result.rank = rank + 1
        
        print(f"[OK] Multi-hop ê²€ìƒ‰ ì™„ë£Œ (ìµœì¢… ê²°ê³¼: {len(final_results)}ê°œ)")
        
        return final_results, metadata


class ContextWindowManager:
    """ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬"""
    
    def __init__(self, model: str = "gpt-4o-mini", max_tokens: int = 4096):
        """
        Args:
            model: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
        """
        self.model = model
        self.max_tokens = max_tokens
        self.encoding = tiktoken.encoding_for_model(model)
    
    def count_tokens(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
        return len(self.encoding.encode(text))
    
    def fit_context(
        self,
        query: str,
        results: List[SearchResult],
        system_prompt: str = "",
        reserve_tokens: int = 1000
    ) -> Tuple[List[SearchResult], Dict[str, int]]:
        """
        ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ì— ë§ê²Œ ê²°ê³¼ ì¡°ì •
        
        Args:
            query: ì¿¼ë¦¬
            results: ê²€ìƒ‰ ê²°ê³¼
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            reserve_tokens: ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ ì˜ˆì•½ í† í°
        
        Returns:
            (ì¡°ì •ëœ ê²°ê³¼, í† í° í†µê³„)
        """
        # ê³ ì • í† í° ê³„ì‚°
        system_tokens = self.count_tokens(system_prompt)
        query_tokens = self.count_tokens(query)
        fixed_tokens = system_tokens + query_tokens + reserve_tokens
        
        # ì»¨í…ìŠ¤íŠ¸ì— ì‚¬ìš© ê°€ëŠ¥í•œ í† í°
        available_tokens = self.max_tokens - fixed_tokens
        
        print(f"\n[INFO] ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬:")
        print(f"  - ìµœëŒ€ í† í°: {self.max_tokens}")
        print(f"  - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {system_tokens} í† í°")
        print(f"  - ì¿¼ë¦¬: {query_tokens} í† í°")
        print(f"  - ì˜ˆì•½ (ì‘ë‹µìš©): {reserve_tokens} í† í°")
        print(f"  - ì‚¬ìš© ê°€ëŠ¥: {available_tokens} í† í°")
        
        # ê²°ê³¼ë¥¼ í† í° ì œí•œì— ë§ê²Œ ì¡°ì •
        fitted_results = []
        used_tokens = 0
        
        for result in results:
            result_tokens = self.count_tokens(result.content)
            
            if used_tokens + result_tokens <= available_tokens:
                fitted_results.append(result)
                used_tokens += result_tokens
            else:
                # ë¶€ë¶„ì ìœ¼ë¡œ í¬í•¨ ê°€ëŠ¥í•œì§€ í™•ì¸
                remaining_tokens = available_tokens - used_tokens
                if remaining_tokens > 100:  # ìµœì†Œ 100 í† í°ì€ ìˆì–´ì•¼ ì˜ë¯¸ ìˆìŒ
                    # í…ìŠ¤íŠ¸ ìë¥´ê¸°
                    tokens = self.encoding.encode(result.content)
                    truncated_tokens = tokens[:remaining_tokens]
                    truncated_content = self.encoding.decode(truncated_tokens)
                    
                    fitted_results.append(SearchResult(
                        content=truncated_content + "...",
                        score=result.score,
                        metadata={**result.metadata, "truncated": True},
                        rank=result.rank
                    ))
                    used_tokens += remaining_tokens
                break
        
        stats = {
            "total_tokens": self.max_tokens,
            "system_tokens": system_tokens,
            "query_tokens": query_tokens,
            "reserve_tokens": reserve_tokens,
            "available_tokens": available_tokens,
            "used_tokens": used_tokens,
            "num_results": len(fitted_results),
            "num_truncated": sum(1 for r in fitted_results if r.metadata.get("truncated", False))
        }
        
        print(f"  - ì‚¬ìš©ëœ í† í°: {used_tokens}")
        print(f"  - í¬í•¨ëœ ê²°ê³¼: {len(fitted_results)}ê°œ")
        print(f"  - ì˜ë¦° ê²°ê³¼: {stats['num_truncated']}ê°œ")
        
        return fitted_results, stats


class AdvancedRAGSystem:
    """ê³ ê¸‰ RAG ì‹œìŠ¤í…œ í†µí•©"""
    
    def __init__(
        self,
        chunk_size: int = 1024,
        model: str = "gpt-4o-mini",
        use_reranker: bool = True
    ):
        """
        Args:
            chunk_size: ì²­í¬ í¬ê¸°
            model: LLM ëª¨ë¸
            use_reranker: ë¦¬ë­ì»¤ ì‚¬ìš© ì—¬ë¶€
        """
        self.chunk_size = chunk_size
        self.model = model
        self.use_reranker = use_reranker
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        import httpx
        # SSL ì¸ì¦ì„œ ê²€ì¦ ìš°íšŒ ì„¤ì • (íšŒì‚¬ ë°©í™”ë²½ ë“±ìœ¼ë¡œ ì¸í•œ ì¸ì¦ì„œ ë¬¸ì œ í•´ê²°)
        http_client = httpx.Client(verify=False)
        
        self.doc_processor = DocumentProcessor(chunk_size=chunk_size)
        self.embeddings = OpenAIEmbeddings(http_client=http_client)
        self.llm = ChatOpenAI(model=model, temperature=0, http_client=http_client)
        self.client = OpenAI(http_client=http_client)
        
        self.retriever = None
        self.reranker = None
        self.multi_hop_retriever = None
        self.context_manager = ContextWindowManager(model=model)
        
        print(f"Advanced RAG System ì´ˆê¸°í™” ì™„ë£Œ (chunk_size={chunk_size})")
    
    def ingest_documents(
        self,
        file_path: str,
        collection_name: Optional[str] = None
    ):
        """ë¬¸ì„œ ìˆ˜ì§‘ ë° ì¸ë±ì‹±"""
        print(f"\n[FILE] ë¬¸ì„œ ë¡œë”© ì¤‘: {file_path}")
        
        # ë¬¸ì„œ ë¡œë“œ
        text = self.doc_processor.load_pdf(file_path)
        print(f"  - ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        
        self._ingest_text_internal(text, file_path, collection_name)
    
    def ingest_text(
        self,
        text: str,
        source_name: str = "sample_text",
        collection_name: Optional[str] = None
    ):
        """í…ìŠ¤íŠ¸ ì§ì ‘ ìˆ˜ì§‘ ë° ì¸ë±ì‹± (PDF ì—†ì´ ì‹¤ìŠµ ê°€ëŠ¥)"""
        print(f"\n[DOC] í…ìŠ¤íŠ¸ ë¡œë”© ì¤‘: {source_name}")
        print(f"  - ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        
        self._ingest_text_internal(text, source_name, collection_name)
    
    def _ingest_text_internal(
        self,
        text: str,
        source: str,
        collection_name: Optional[str] = None
    ):
        """ë‚´ë¶€ ì¸ë±ì‹± ë¡œì§"""
        # ì²­í‚¹
        documents = self.doc_processor.create_chunks(
            text,
            metadata={"source": source, "chunk_size": self.chunk_size}
        )
        print(f"  - ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(documents)}ê°œ")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        if collection_name is None:
            collection_name = f"advanced_rag_{self.chunk_size}"
        
        self.retriever = HybridRetriever(
            documents=documents,
            embeddings=self.embeddings,
            collection_name=collection_name
        )
        
        # ë¦¬ë­ì»¤ ì´ˆê¸°í™”
        if self.use_reranker:
            self.reranker = Reranker()
        
        # Multi-hop ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.multi_hop_retriever = MultiHopRetriever(
            retriever=self.retriever,
            reranker=self.reranker,
            llm=self.llm
        )
        
        print("[OK] ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ")
    
    def search(
        self,
        query: str,
        method: str = "hybrid",
        k: int = 5,
        alpha: float = 0.5,
        use_reranker: bool = True
    ) -> List[SearchResult]:
        """
        ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            method: ê²€ìƒ‰ ë°©ë²• ("sparse", "dense", "hybrid")
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            alpha: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œ Dense ê°€ì¤‘ì¹˜
            use_reranker: ë¦¬ë­ì»¤ ì‚¬ìš© ì—¬ë¶€
        """
        if not self.retriever:
            raise ValueError("ë¬¸ì„œë¥¼ ë¨¼ì € ì¸ë±ì‹±í•˜ì„¸ìš” (ingest_documents í˜¸ì¶œ)")
        
        # ì‹¤ì œ ë¬¸ì„œ ìˆ˜ í™•ì¸
        total_docs = len(self.retriever.documents)
        actual_k = min(k, total_docs)
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        if method == "sparse":
            results = self.retriever.sparse_search(query, k=actual_k*2 if use_reranker else actual_k)
        elif method == "dense":
            results = self.retriever.dense_search(query, k=actual_k*2 if use_reranker else actual_k)
        elif method == "hybrid":
            results = self.retriever.hybrid_search(query, k=actual_k*2 if use_reranker else actual_k, alpha=alpha)
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ê²€ìƒ‰ ë°©ë²•: {method}")
        
        # ë¦¬ë­í‚¹
        if use_reranker and self.reranker:
            results = self.reranker.rerank(query, results, top_k=actual_k)
        else:
            results = results[:actual_k]
        
        return results
    
    def multi_hop_search(
        self,
        query: str,
        k: int = 5,
        use_reranker: bool = True
    ) -> Tuple[List[SearchResult], Dict[str, Any]]:
        """Multi-hop ê²€ìƒ‰"""
        if not self.multi_hop_retriever:
            raise ValueError("ë¬¸ì„œë¥¼ ë¨¼ì € ì¸ë±ì‹±í•˜ì„¸ìš” (ingest_documents í˜¸ì¶œ)")
        
        return self.multi_hop_retriever.multi_hop_search(
            query,
            k_per_hop=k,
            use_reranker=use_reranker
        )
    
    def generate_answer(
        self,
        query: str,
        results: List[SearchResult],
        manage_context: bool = True
    ) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        
        Args:
            query: ì¿¼ë¦¬
            results: ê²€ìƒ‰ ê²°ê³¼
            manage_context: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬ ì—¬ë¶€
        """
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ìƒì„¸í•œ ë‹µë³€ ìœ ë„)
        system_prompt = """ë‹¹ì‹ ì€ AI/ML ì „ë¬¸ê°€ì´ë©°, ì œê³µëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•˜ê³  êµìœ¡ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ë‹µë³€ ì‘ì„± ê·œì¹™:
1. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ëª¨ë‘ í™œìš©í•˜ì—¬ **êµ¬ì²´ì ì´ê³  ìƒì„¸í•˜ê²Œ** ë‹µë³€
2. í•µì‹¬ ê°œë…, ì˜ˆì‹œ, ì‘ìš© ë¶„ì•¼ë¥¼ í¬í•¨í•˜ì—¬ **ìµœì†Œ 3-5ë¬¸ì¥ ì´ìƒ** ì‘ì„±
3. ë¦¬ìŠ¤íŠ¸ë‚˜ ë²ˆí˜¸ë¥¼ í™œìš©í•˜ì—¬ ê°€ë…ì„± ìˆê²Œ ì •ë¦¬
4. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ìˆëŠ” ë‚´ìš©ë§Œ í™œìš©
5. "ë¬¸ì„œì— ë”°ë¥´ë©´", "ì»¨í…ìŠ¤íŠ¸ì—ì„œ" ê°™ì€ ë©”íƒ€ ì–¸ê¸‰ ì—†ì´ ì§ì ‘ ì„¤ëª…"""
        
        # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬
        if manage_context:
            results, context_stats = self.context_manager.fit_context(
                query=query,
                results=results,
                system_prompt=system_prompt
            )
        else:
            context_stats = None
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([
            f"[ë¬¸ì„œ {r.rank}] (ì ìˆ˜: {r.score:.4f})\n{r.content}"
            for r in results
        ])
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        user_prompt = f"""ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""
        
        # LLM í˜¸ì¶œ
        print("\n[...] ë‹µë³€ ìƒì„± ì¤‘...")
        start_time = time.time()
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,  # ì•½ê°„ì˜ ì°½ì˜ì„± í—ˆìš©
            max_tokens=500    # ì¶©ë¶„í•œ ë‹µë³€ ê¸¸ì´ ë³´ì¥
        )
        
        elapsed_time = time.time() - start_time
        answer = response.choices[0].message.content
        
        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
        usage = response.usage
        input_tokens = usage.prompt_tokens if usage else 0
        output_tokens = usage.completion_tokens if usage else 0
        
        print(f"[OK] ë‹µë³€ ìƒì„± ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ, ì…ë ¥: {input_tokens}í† í°, ì¶œë ¥: {output_tokens}í† í°)")
        
        return {
            "answer": answer,
            "query": query,
            "num_results": len(results),
            "context_stats": context_stats,
            "elapsed_time": elapsed_time,
            "model": self.model
        }

# ============================================================================
# ì‹¤ìŠµ ê³µí†µ ìœ í‹¸ë¦¬í‹°
# ============================================================================

def format_chunk(content: str, indent: str = "      ") -> str:
    """ì²­í¬ ë‚´ìš©ì„ ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ… (ì „ì²´ ë‚´ìš© í‘œì‹œ)"""
    lines = content.strip().split('\n')
    formatted_lines = []
    for line in lines:
        line = line.strip()
        if line:
            formatted_lines.append(f"{indent}{line}")
    return '\n'.join(formatted_lines)


def print_search_result(result: SearchResult, index: int, show_full: bool = True):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë§·íŒ…í•˜ì—¬ ì¶œë ¥"""
    chunk_id = result.metadata.get('chunk_id', '?')
    if isinstance(chunk_id, int):
        chunk_id += 1
    
    print(f"  [{index}] ì ìˆ˜: {result.score:.4f} | ì²­í¬ #{chunk_id} ({len(result.content)}ì)")
    
    if show_full:
        print(f"  {'â”€'*50}")
        print(format_chunk(result.content))
        print(f"  {'â”€'*50}")
    else:
        preview = result.content.replace('\n', ' ')[:100]
        print(f"      {preview}...")


# ============================================================================
# ì‹¤ìŠµ í•¨ìˆ˜ë“¤
# ============================================================================

def experiment_chunk_sizes(text: str = None):
    """ì‹¤ìŠµ 4: Chunk size ì‹¤í—˜"""
    print("\n" + "="*80)
    print("[4] ì‹¤ìŠµ 4: Chunk Size ì‹¤í—˜ (256 vs 512 vs 1024)")
    print("="*80)
    print("ëª©í‘œ: ì²­í¬ í¬ê¸°ê°€ ê²€ìƒ‰ ì •í™•ë„ì™€ ë‹µë³€ í’ˆì§ˆì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„")
    
    sample_text = text or SAMPLE_TEXT
    
    chunk_sizes = [256, 512, 1024]
    test_query = "ê°•í™” í•™ìŠµì˜ ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ì€?"
    
    results_comparison = []
    
    print(f"\n[*] í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{test_query}'")
    
    for chunk_size in chunk_sizes:
        print(f"\n{'â”€'*60}")
        print(f"[>] Chunk Size: {chunk_size}ì")
        print(f"{'â”€'*60}")
        
        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        rag = AdvancedRAGSystem(chunk_size=chunk_size, use_reranker=False)
        rag.doc_processor = DocumentProcessor(chunk_size=chunk_size, chunk_overlap=chunk_size//10)
        rag.ingest_text(sample_text, source_name="AI_ê°€ì´ë“œ", collection_name=f"chunk_exp_{chunk_size}")
        
        # ê²€ìƒ‰
        search_results = rag.search(test_query, method="hybrid", k=3)
        
        # ìƒìœ„ 1ê°œ ê²°ê³¼ ì „ì²´ í‘œì‹œ (ì²­í¬ í¬ê¸°ë³„ ì°¨ì´ í™•ì¸ìš©)
        print(f"\nìƒìœ„ ê²€ìƒ‰ ê²°ê³¼:")
        print_search_result(search_results[0], 1, show_full=True)
        
        # ë‹µë³€ ìƒì„±
        answer_data = rag.generate_answer(test_query, search_results)
        
        # í† í° ì •ë³´ ì¶”ì¶œ
        context_stats = answer_data.get("context_stats", {})
        used_tokens = context_stats.get("used_tokens", 0) if context_stats else 0
        
        results_comparison.append({
            "chunk_size": chunk_size,
            "num_chunks": len(rag.retriever.documents),
            "used_tokens": used_tokens,
            "top_score": search_results[0].score if search_results else 0,
            "answer": answer_data["answer"],
            "elapsed_time": answer_data["elapsed_time"]
        })
    
    # ë¹„êµ ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*80)
    print("[INFO] Chunk Size ë¹„êµ ë¶„ì„")
    print("="*80)
    
    print(f"\n{'ì²­í¬ í¬ê¸°':<10} {'ì²­í¬ ìˆ˜':<8} {'í† í°':<8} {'ìƒì„± ì‹œê°„':<10} {'ë‹µë³€ ê¸¸ì´':<8}")
    print("â”€" * 60)
    for result in results_comparison:
        print(f"{result['chunk_size']:<10} {result['num_chunks']:<8} {result['used_tokens']:<8} {result['elapsed_time']:.2f}ì´ˆ{'':<4} {len(result['answer'])}ì")
    
    # í† í° ë¹„ìš© ë¶„ì„
    if len(results_comparison) > 1:
        min_tokens = min(r['used_tokens'] for r in results_comparison if r['used_tokens'] > 0)
        max_tokens = max(r['used_tokens'] for r in results_comparison if r['used_tokens'] > 0)
        if min_tokens > 0:
            print(f"\n[ë¶„ì„] í† í° ë¹„ìš©:")
            print(f"  * ìµœì†Œ: {min_tokens} í† í° (ì²­í¬ í¬ê¸° {results_comparison[0]['chunk_size']})")
            print(f"  * ìµœëŒ€: {max_tokens} í† í° (ì²­í¬ í¬ê¸° {results_comparison[-1]['chunk_size']})")
            print(f"  * ë¹„ìœ¨: í° ì²­í¬ê°€ ì‘ì€ ì²­í¬ì˜ {max_tokens/min_tokens:.1f}ë°° í† í° ì‚¬ìš©")
    
    # ê° ì²­í¬ í¬ê¸°ë³„ ë‹µë³€ ë¹„êµ ì¶œë ¥
    print(f"\n{'â”€'*60}")
    print("[DOC] ë‹µë³€ ë¹„êµ (ì²­í¬ í¬ê¸°ë³„ ì°¨ì´ í™•ì¸)")
    print(f"{'â”€'*60}")
    for result in results_comparison:
        print(f"\n[>] Chunk Size {result['chunk_size']}ì ë‹µë³€:")
        print(f"  {result['answer'][:300]}{'...' if len(result['answer']) > 300 else ''}")
    
    print(f"\n[TIP] Chunk Size ê°€ì´ë“œ:")
    print("  - ì‘ì€ ì²­í¬ (256~512): ì •ë°€í•œ ê²€ìƒ‰, ë” ë§ì€ ì²­í¬ ì²˜ë¦¬ í•„ìš”")
    print("  - ì¤‘ê°„ ì²­í¬ (512~1024): ì¼ë°˜ì ìœ¼ë¡œ ê· í˜• ì¡íŒ ì„ íƒ")
    print("  - í° ì²­í¬ (1024+): ë„“ì€ ì»¨í…ìŠ¤íŠ¸, ë…¸ì´ì¦ˆ í¬í•¨ ê°€ëŠ¥ì„±")
    print("  - ê¶Œì¥: ë„ë©”ì¸ê³¼ ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ì‹¤í—˜ìœ¼ë¡œ ê²°ì •")
    
    print(f"\n  âš ï¸ [ì‹¤ë¬´ ì˜ˆì™¸] í° ì²­í¬(1024+)ê°€ ë” ì í•©í•œ ê²½ìš°:")
    print("     - ìš”ì•½ ë³´ê³ ì„œ: ì „ì²´ ë§¥ë½ì´ ì¤‘ìš”")
    print("     - ë²•ë¥  ë¬¸ì„œ/ê·œì •ì§‘: ì¡°í•­ ê°„ ì—°ê²°ì´ í•„ìˆ˜")
    print("     - ê¸°ìˆ  ë§¤ë‰´ì–¼: ì ˆì°¨ì˜ ì—°ì†ì„±ì´ ì¤‘ìš”")
    print("     â†’ 'ì‘ì€ ì²­í¬ = ë¬´ì¡°ê±´ ì¢‹ë‹¤'ëŠ” ì˜¤í•´ ê¸ˆì§€!")
    
    # ê²€ìƒ‰ í’ˆì§ˆ(Recall/Precision) ë¶„ì„ ì¶”ê°€
    print(f"\n{'â”€'*60}")
    print("[INFO] ê²€ìƒ‰ í’ˆì§ˆ ë¶„ì„ - Chunk Sizeë³„ Precision ë¹„êµ")
    print(f"{'â”€'*60}")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [?] Chunk Sizeê°€ ê²€ìƒ‰ í’ˆì§ˆì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ë‚˜ìš”?                        â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  [í•µì‹¬ ê°œë…]                                                            â”‚
  â”‚  â€¢ ì‘ì€ ì²­í¬: ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ì— ë” ì •í™•íˆ ë§¤ì¹­ (Precision â†‘)          â”‚
  â”‚              But ì»¨í…ìŠ¤íŠ¸ê°€ ì§§ì•„ ë§¥ë½ ë¶€ì¡±í•  ìˆ˜ ìˆìŒ                    â”‚
  â”‚                                                                         â”‚
  â”‚  â€¢ í° ì²­í¬: ì£¼ë³€ ë§¥ë½ í’ë¶€ (ì´í•´ë„ â†‘)                                   â”‚
  â”‚            But ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ë„ ì„ì—¬ ë“¤ì–´ì˜´ (Precision â†“)               â”‚
  â”‚                                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [ì‹¤í—˜ ê²°ê³¼ ì˜ˆì‹œ] - í…ŒìŠ¤íŠ¸ì…‹ ê¸°ë°˜ ì¸¡ì •                                   â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  Chunk Size â”‚ Recall@5 â”‚ Precision@5 â”‚ íŠ¹ì§•                             â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  256ì      â”‚ 0.9      â”‚ 0.65        â”‚ ì •ë°€í•˜ì§€ë§Œ ì»¨í…ìŠ¤íŠ¸ ë¶„ì ˆë¨       â”‚
  â”‚  512ì      â”‚ 1.0      â”‚ 0.85        â”‚ ê· í˜• ì¡íŒ ì„ íƒ (ì¼ë°˜ ê¶Œì¥)       â”‚
  â”‚  1024ì     â”‚ 1.0      â”‚ 0.70        â”‚ ë§¥ë½ í’ë¶€í•˜ë‚˜ ë…¸ì´ì¦ˆ ì¦ê°€        â”‚
  â”‚  2048ì     â”‚ 1.0      â”‚ 0.55        â”‚ ì£¼ì œ í˜¼í•©, ì •ë°€ë„ í•˜ë½           â”‚
  â”‚                                                                         â”‚
  â”‚  [í•´ì„]                                                                 â”‚
  â”‚  â€¢ Recall: í° ì²­í¬ì¼ìˆ˜ë¡ ì •ë‹µì„ ë†“ì¹  í™•ë¥  â†“ (í¬ê´„ì )                    â”‚
  â”‚  â€¢ Precision: ë„ˆë¬´ í¬ë©´ ë…¸ì´ì¦ˆ ì¦ê°€, ë„ˆë¬´ ì‘ìœ¼ë©´ ë§¥ë½ ë¶€ì¡±              â”‚
  â”‚  â€¢ Sweet Spot: ëŒ€ë¶€ë¶„ì˜ ì¼ë°˜ ë¬¸ì„œì—ì„œ 512~1024ìê°€ ì í•©                 â”‚
  â”‚                                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [ì‹¤ë¬´ ì¸¡ì • ë°©ë²•]                                                       â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  def measure_precision_at_k(query, results, relevant_docs, k=5):        â”‚
  â”‚      '''                                                                â”‚
  â”‚      Args:                                                              â”‚
  â”‚          query: í…ŒìŠ¤íŠ¸ ì§ˆë¬¸                                             â”‚
  â”‚          results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸                                      â”‚
  â”‚          relevant_docs: ì •ë‹µ ë¬¸ì„œ ID ì§‘í•© (ìˆ˜ë™ ë¼ë²¨ë§)                 â”‚
  â”‚          k: ìƒìœ„ Kê°œ í‰ê°€                                               â”‚
  â”‚      '''                                                                â”‚
  â”‚      top_k_ids = [r.metadata['chunk_id'] for r in results[:k]]         â”‚
  â”‚      hits = sum(1 for doc_id in top_k_ids if doc_id in relevant_docs)  â”‚
  â”‚      return hits / k                                                   â”‚
  â”‚                                                                         â”‚
  â”‚  # í…ŒìŠ¤íŠ¸ì…‹ ì˜ˆì‹œ                                                        â”‚
  â”‚  test_set = [                                                           â”‚
  â”‚      {"query": "ê°•í™” í•™ìŠµì´ë€?", "relevant_chunks": [3, 4]},           â”‚
  â”‚      {"query": "CNNì˜ êµ¬ì¡°ëŠ”?", "relevant_chunks": [7, 8, 9]},         â”‚
  â”‚      ...                                                                â”‚
  â”‚  ]                                                                      â”‚
  â”‚                                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [TIP] ì²­í¬ í¬ê¸° ê²°ì • í”„ë¡œì„¸ìŠ¤                                          â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  1. í…ŒìŠ¤íŠ¸ì…‹ ì¤€ë¹„ (50~100ê°œ ì§ˆë¬¸ + ì •ë‹µ ì²­í¬ ë¼ë²¨)                      â”‚
  â”‚  2. ì²­í¬ í¬ê¸°ë³„ë¡œ ì¸ë±ì‹± (256, 512, 1024, 2048)                        â”‚
  â”‚  3. ê° ì„¤ì •ìœ¼ë¡œ Precision@5, Recall@5 ì¸¡ì •                              â”‚
  â”‚  4. ê°€ì¥ ë†’ì€ Precisionì„ ë³´ì´ëŠ” ì²­í¬ í¬ê¸° ì„ íƒ                         â”‚
  â”‚  5. ë™ì ì´ë©´ ì‘ì€ ì²­í¬ ì„ íƒ (í† í° ë¹„ìš© ì ˆê°)                            â”‚
  â”‚                                                                         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # ì´ë²ˆ ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ (ê´€ë ¨ í‚¤ì›Œë“œë¡œ ê°„ì´ Precision ì¸¡ì •)
    print(f"[ë¶„ì„] ì´ë²ˆ ì‹¤í—˜ì˜ Precision ì¶”ì • (ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€):")
    relevant_keywords = ["ê°•í™”", "í•™ìŠµ", "ì•Œê³ ë¦¬ì¦˜", "q-learning", "dqn", "ppo", "reward", "ë³´ìƒ", "agent", "ì—ì´ì „íŠ¸", "ì •ì±…"]
    
    print(f"  ê´€ë ¨ í‚¤ì›Œë“œ: {', '.join(relevant_keywords[:5])}...")
    print()
    
    for result in results_comparison:
        answer = result['answer'].lower()
        hits = sum(1 for kw in relevant_keywords if kw.lower() in answer)
        relevance_score = min(1.0, hits / 5)  # 5ê°œ ì´ìƒ ë§¤ì¹­ì‹œ 1.0
        print(f"  Chunk {result['chunk_size']:>4}ì: í‚¤ì›Œë“œ {hits}ê°œ ë§¤ì¹­ â†’ ê´€ë ¨ë„ {relevance_score:.0%}")
    
    print(f"\n  âš ï¸ [ì¤‘ìš”] í‚¤ì›Œë“œ ê¸°ë°˜ Precisionì€ ê·¼ì‚¬ì¹˜ì…ë‹ˆë‹¤!")
    print("     â€¢ 'í‚¤ì›Œë“œ ë§¤ì¹­ = ì •í™•í•œ ë‹µë³€'ì´ ì•„ë‹˜")
    print("     â€¢ ì‹¤ë¬´ì—ì„œëŠ” ë°˜ë“œì‹œ Human Judge ë˜ëŠ” LLM Judgeë¡œ ì •ë‹µ ë¼ë²¨ë§ í•„ìš”")
    print("     â€¢ ê¶Œì¥ ë°©ë²•:")
    print("       1. í…ŒìŠ¤íŠ¸ì…‹ 50~100ê°œ ì¤€ë¹„")
    print("       2. ê° ì§ˆë¬¸ë³„ ì •ë‹µ ì²­í¬ë¥¼ ì‚¬ëŒì´ ì§ì ‘ ë¼ë²¨ë§")
    print("       3. ë˜ëŠ” GPT-4ë¡œ 'ì´ ì²­í¬ê°€ ì§ˆë¬¸ì— ê´€ë ¨ìˆëŠ”ê°€?' íŒì •")
    print("       4. ë¼ë²¨ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ Precision@K ê³„ì‚°")


def experiment_search_methods(text: str = None):
    """ì‹¤ìŠµ 1: Sparse vs Dense vs Hybrid ê²€ìƒ‰ ë¹„êµ"""
    print("\n" + "="*80)
    print("[1] ì‹¤ìŠµ 1: ê²€ìƒ‰ ë°©ë²• ë¹„êµ (Sparse vs Dense vs Hybrid)")
    print("="*80)
    print("ëª©í‘œ: í‚¤ì›Œë“œ ê¸°ë°˜(Sparse) vs ì˜ë¯¸ ê¸°ë°˜(Dense) vs ê²°í•©(Hybrid) ë°©ì‹ ë¹„êµ")
    
    # ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì‚¬ìš©
    sample_text = text or SAMPLE_TEXT
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì ì ˆí•œ ì²­í¬ í¬ê¸°ë¡œ ë¶„í• )
    rag = AdvancedRAGSystem(chunk_size=400, use_reranker=False)
    rag.doc_processor = DocumentProcessor(chunk_size=400, chunk_overlap=50)
    rag.ingest_text(sample_text, source_name="AI_ê°€ì´ë“œ", collection_name="search_method_exp")
    
    # ë‘ ê°€ì§€ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: í‚¤ì›Œë“œ ë§¤ì¹­ vs ì˜ë¯¸ ê²€ìƒ‰
    test_queries = [
        ("ê°•í™” í•™ìŠµì´ë€?", "ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ í…ŒìŠ¤íŠ¸"),
        ("AIê°€ ê·¸ë¦¼ì„ ê·¸ë¦¬ëŠ” ë°©ë²•", "ì˜ë¯¸ì  ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸ (í‚¤ì›Œë“œ ë¶ˆì¼ì¹˜)"),
    ]
    
    methods = ["sparse", "dense", "hybrid"]
    
    print(f"\nì´ ë¬¸ì„œ ì²­í¬ ìˆ˜: {len(rag.retriever.documents)}ê°œ")
    
    for query, desc in test_queries:
        print(f"\n{'â”€'*70}")
        print(f"[*] í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{query}'")
        # í† í°í™”ëœ ì¿¼ë¦¬ í‘œì‹œ
        tokenized = rag.retriever.tokenize_korean(query)
        print(f"   í† í°í™”: {tokenized}")
        print(f"   [ì°¸ê³ ] ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ (ì‹¤ë¬´: KoNLPy ê¶Œì¥)")
        print(f"   ({desc})")
        print(f"{'â”€'*70}")
        
        for method in methods:
            results = rag.search(query, method=method, k=3)
            
            # ì ìˆ˜ íƒ€ì… ì„¤ëª… ì¶”ê°€
            score_type = {
                "sparse": "BM25 ì›ë³¸",
                "dense": "ê±°ë¦¬ ê¸°ë°˜ (0~1)",
                "hybrid": "ì •ê·œí™” ê²°í•© (0~1)"
            }.get(method, "")
            
            print(f"\n[>] {method.upper()} ê²€ìƒ‰:")
            # ìƒìœ„ 1ê°œ ê²°ê³¼ë§Œ ì „ì²´ í‘œì‹œ (ê°€ë…ì„±)
            for i, result in enumerate(results[:1], 1):
                # í‚¤ì›Œë“œ ë§¤ì¹­ ì—†ìŒ ì²´í¬
                if result.metadata.get("no_match"):
                    print(f"  [{i}] [X] í‚¤ì›Œë“œ ë§¤ì¹­ ì—†ìŒ (ì ìˆ˜: 0)")
                else:
                    chunk_id = result.metadata.get('chunk_id', '?')
                    # ì›ë³¸ ì ìˆ˜ í‘œì‹œ (ìˆìœ¼ë©´)
                    raw_distance = result.metadata.get('raw_distance', None)
                    extra_info = f" [ê±°ë¦¬: {raw_distance:.4f}]" if raw_distance is not None else ""
                    print(f"  [{i}] ì ìˆ˜: {result.score:.4f} ({score_type}){extra_info} | ì²­í¬ #{chunk_id+1} ({len(result.content)}ì)")
                    print(f"  {'â”€'*50}")
                    print(format_chunk(result.content))
                    print(f"  {'â”€'*50}")
        
        # Hybrid ê²°ê³¼ì˜ ì ìˆ˜ ë¶„í•´ í‘œì‹œ (ì´ë¯¸ ê²€ìƒ‰í•œ ê²°ê³¼ ì‚¬ìš©)
        hybrid_results = rag.search(query, method="hybrid", k=1)
        if hybrid_results:
            hr = hybrid_results[0]
            sparse_raw = hr.metadata.get('sparse_raw', None)
            sparse_max = hr.metadata.get('sparse_max', None)
            dense_raw = hr.metadata.get('dense_raw', None)
            dense_max = hr.metadata.get('dense_max', None)
            sparse_norm = hr.metadata.get('sparse_norm', None)
            dense_norm = hr.metadata.get('dense_norm', None)
            
            print(f"\n[INFO] Hybrid ì ìˆ˜ ë¶„í•´ (ìƒìœ„ 1ê°œ):")
            print(f"  ì²­í¬ #{hr.metadata.get('chunk_id', '?') + 1} ê¸°ì¤€:")
            if sparse_raw is not None and sparse_norm is not None and sparse_max is not None:
                print(f"  - Sparse: {sparse_raw:.4f} Ã· {sparse_max:.4f} (max) Ã— 0.5 = {sparse_norm:.4f}")
            else:
                print(f"  - Sparse: ë§¤ì¹­ ì—†ìŒ")
            if dense_raw is not None and dense_norm is not None and dense_max is not None:
                print(f"  - Dense:  {dense_raw:.4f} Ã· {dense_max:.4f} (max) Ã— 0.5 = {dense_norm:.4f}")
            else:
                print(f"  - Dense: ë§¤ì¹­ ì—†ìŒ")
            total = (sparse_norm or 0) + (dense_norm or 0)
            print(f"  - í•©ê³„:   {sparse_norm or 0:.4f} + {dense_norm or 0:.4f} = {total:.4f}")
            
            # Dense ì ìˆ˜ ì˜¤í•´ ë°©ì§€
            print(f"\n  âš ï¸ [ì£¼ì˜] Dense ì ìˆ˜ê°€ í•­ìƒ 0.5ê°€ ë˜ëŠ” ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤!")
            print(f"     ì´ë²ˆ ì¿¼ë¦¬ì—ì„œ ì´ ë¬¸ì„œê°€ Dense ìµœëŒ€ê°’ì´ì—ˆê¸° ë•Œë¬¸ì— 0.5ê°€ ëœ ê²ƒì…ë‹ˆë‹¤.")
            print(f"     ë‹¤ë¥¸ ë¬¸ì„œëŠ” (ìì‹ ì˜ ì ìˆ˜ Ã· ìµœëŒ€ê°’) Ã— 0.5 = ë” ë‚®ì€ ê°’ì´ ë©ë‹ˆë‹¤.")
        
        # ë°©ë²•ë³„ íŠ¹ì§• ì„¤ëª…
        print(f"\n[TIP] ë¶„ì„:")
        print("  - Sparse(BM25): ì¿¼ë¦¬ ë‹¨ì–´ê°€ ë¬¸ì„œì— ì •í™•íˆ ìˆì–´ì•¼ ë†’ì€ ì ìˆ˜")
        print("  - Dense(Vector): ì˜ë¯¸ê°€ ë¹„ìŠ·í•˜ë©´ ë‹¨ì–´ê°€ ë‹¬ë¼ë„ ë†’ì€ ì ìˆ˜")
        print("    â†’ L2 ê±°ë¦¬ë¥¼ 1/(1+ê±°ë¦¬)ë¡œ ë³€í™˜í•œ 'ê±°ë¦¬ ê¸°ë°˜ ì ìˆ˜'")
        print("    â†’ âš ï¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ì•„ë‹˜! (Lab 1 ì°¸ê³ )")
        print("  - Hybrid: ë‘ ì ìˆ˜ë¥¼ ì •ê·œí™” í›„ ê²°í•© (alpha=0.5 ê¸°ë³¸ê°’)")
        print("  - ì ìˆ˜ ë²”ìœ„: ëª¨ë‘ 0~1ë¡œ ì •ê·œí™”ë˜ì–´ ë¹„êµ ê°€ëŠ¥")
    
    # Alpha ê°’ ì„ íƒ ê°€ì´ë“œ
    print(f"\n{'â”€'*70}")
    print("[GUIDE] Hybrid ê²€ìƒ‰ alpha ê°’ ì„ íƒ ê°€ì´ë“œ")
    print(f"{'â”€'*70}")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [?] "ì™œ alpha=0.5ì¸ê°€ìš”? 0.7ì€ ì•ˆ ë˜ë‚˜ìš”?"                              â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  alpha = Dense(ì˜ë¯¸) ê°€ì¤‘ì¹˜                                              â”‚
  â”‚  (1-alpha) = Sparse(í‚¤ì›Œë“œ) ê°€ì¤‘ì¹˜                                       â”‚
  â”‚                                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [CASE] ë„ë©”ì¸ë³„ ê¶Œì¥ alpha ê°’                                           â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  ë„ë©”ì¸/ì§ˆì˜ ìœ í˜•              â”‚ alpha â”‚ Sparse â”‚ ì´ìœ                    â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  ê³ ê°ì„¼í„° FAQ                 â”‚ 0.3   â”‚ 0.7    â”‚ ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ì¤‘ìš”â”‚
  â”‚  ì œí’ˆì½”ë“œ/ëª¨ë¸ëª… ê²€ìƒ‰          â”‚ 0.2   â”‚ 0.8    â”‚ ì½”ë“œëŠ” ì™„ì „ ì¼ì¹˜ í•„ìˆ˜ â”‚
  â”‚  ë²•ë¥ /ì•½ê´€ ì¡°í•­ ê²€ìƒ‰           â”‚ 0.3   â”‚ 0.7    â”‚ ì¡°í•­ ë²ˆí˜¸, ìš©ì–´ ì •í™•ì„±â”‚
  â”‚  ìˆ«ì/ë‚ ì§œ í¬í•¨ ì§ˆì˜           â”‚ 0.3   â”‚ 0.7    â”‚ ìˆ«ìëŠ” ì˜ë¯¸ ê²€ìƒ‰ ë¶ˆê°€ â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  ì¼ë°˜ ë¬¸ì„œ ê²€ìƒ‰ (ê¸°ë³¸ê°’)       â”‚ 0.5   â”‚ 0.5    â”‚ ê· í˜• ì¡íŒ ê²€ìƒ‰        â”‚
  â”‚  ê¸°ìˆ  ë¬¸ì„œ Q&A                â”‚ 0.5   â”‚ 0.5    â”‚ ê°œë…+ìš©ì–´ ë‘˜ ë‹¤ ì¤‘ìš”  â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  ì˜ë¯¸ ê²€ìƒ‰ ìœ„ì£¼ (ìì—°ì–´ ì§ˆë¬¸)  â”‚ 0.7   â”‚ 0.3    â”‚ "~í•˜ëŠ” ë°©ë²•" ë¥˜ ì§ˆë¬¸  â”‚
  â”‚  ì¶”ìƒì /ê°œë… ì§ˆì˜             â”‚ 0.8   â”‚ 0.2    â”‚ ë™ì˜ì–´/ìœ ì‚¬ì–´ ê²€ìƒ‰    â”‚
  â”‚  ë‹¤êµ­ì–´ ê²€ìƒ‰                  â”‚ 0.8   â”‚ 0.2    â”‚ í‚¤ì›Œë“œ ë§¤ì¹­ ì–´ë ¤ì›€    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  [!] ì‹¤ë¬´ ê²°ì • ë°©ë²•:
     1. í…ŒìŠ¤íŠ¸ì…‹ ì¤€ë¹„ (ì§ˆë¬¸ 50~100ê°œ + ì •ë‹µ ë¬¸ì„œ)
     2. alphaë¥¼ 0.2, 0.5, 0.8ë¡œ ë°”ê¿”ê°€ë©° Recall@5 ì¸¡ì •
     3. ê°€ì¥ ë†’ì€ Recallì„ ë³´ì´ëŠ” alpha ì„ íƒ
     
  [TIP] ë¹ ë¥¸ íŒë‹¨ ê¸°ì¤€:
     â€¢ ì§ˆë¬¸ì— ê³ ìœ ëª…ì‚¬/ì½”ë“œ/ìˆ«ìê°€ ë§ë‹¤ â†’ Sparse ë†’ì´ê¸° (alphaâ†“)
     â€¢ ì§ˆë¬¸ì´ ìì—°ì–´ í˜•íƒœë¡œ ë‹¤ì–‘í•˜ë‹¤ â†’ Dense ë†’ì´ê¸° (alphaâ†‘)
     â€¢ ëª¨ë¥´ê² ë‹¤ â†’ 0.5ë¡œ ì‹œì‘, ì ì§„ ì¡°ì •
    """)


def experiment_reranking(text: str = None):
    """ì‹¤ìŠµ 2: Re-ranking íš¨ê³¼ ë¹„êµ"""
    print("\n" + "="*80)
    print("[2] ì‹¤ìŠµ 2: Re-ranking íš¨ê³¼ ë¹„êµ")
    print("="*80)
    print("ëª©í‘œ: ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ë¥¼ Cross-Encoderë¡œ ì¬ìˆœìœ„í™”í•˜ì—¬ ì •í™•ë„ í–¥ìƒ")
    print("ëª¨ë¸: cross-encoder/ms-marco-MiniLM-L-6-v2 (ê²½ëŸ‰, ~80MB)")
    
    sample_text = text or SAMPLE_TEXT
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë” ì‘ì€ ì²­í¬ë¡œ ë§ì€ ê²°ê³¼ ìƒì„±)
    rag = AdvancedRAGSystem(chunk_size=300, use_reranker=True)
    rag.doc_processor = DocumentProcessor(chunk_size=300, chunk_overlap=30)
    rag.ingest_text(sample_text, source_name="AI_ê°€ì´ë“œ", collection_name="reranking_exp")
    
    # êµ¬ì²´ì ì¸ ì§ˆë¬¸ (ì¼ë¶€ ì²­í¬ê°€ ë¶€ë¶„ì ìœ¼ë¡œë§Œ ê´€ë ¨ìˆëŠ” ê²½ìš°)
    test_query = "ë”¥ëŸ¬ë‹ì—ì„œ Transformer ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ ì›ë¦¬ëŠ”?"
    
    print(f"\n[*] í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{test_query}'")
    print(f"ì´ ì²­í¬ ìˆ˜: {len(rag.retriever.documents)}ê°œ")
    
    # Re-ranking ì—†ì´ (ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼)
    print(f"\n{'â”€'*60}")
    print("[>] BEFORE: Re-ranking ì—†ì´ (Hybrid ê²€ìƒ‰ ê²°ê³¼)")
    print(f"{'â”€'*60}")
    results_without = rag.search(test_query, method="hybrid", k=5, use_reranker=False)
    
    # ìƒìœ„ 2ê°œ ê²°ê³¼ ì „ì²´ í‘œì‹œ
    for i, result in enumerate(results_without[:2], 1):
        print_search_result(result, i, show_full=True)
    
    # Re-ranking ì ìš©
    print(f"\n{'â”€'*60}")
    print("[>] AFTER: Re-ranking ì ìš© (Cross-Encoder ì¬ìˆœìœ„í™”)")
    print(f"{'â”€'*60}")
    results_with = rag.search(test_query, method="hybrid", k=5, use_reranker=True)
    
    # ìƒìœ„ 2ê°œ ê²°ê³¼ ì „ì²´ í‘œì‹œ
    for i, result in enumerate(results_with[:2], 1):
        print_search_result(result, i, show_full=True)
    
    # ìˆœìœ„ ë³€í™” ë¶„ì„
    print(f"\n{'â”€'*60}")
    print("[INFO] ë¶„ì„: ìˆœìœ„ ë³€í™”")
    print(f"{'â”€'*60}")
    
    # ì²­í¬ IDë¡œ ë¹„êµ
    before_chunk = results_without[0].metadata.get('chunk_id', -1)
    after_chunk = results_with[0].metadata.get('chunk_id', -1)
    
    print(f"  Before 1ìœ„: ì²­í¬ #{before_chunk + 1} (ì ìˆ˜: {results_without[0].score:.4f})")
    print(f"  After 1ìœ„:  ì²­í¬ #{after_chunk + 1} (ì ìˆ˜: {results_with[0].score:.4f})")
    
    # ì›ë³¸ Cross-Encoder ë¡œì§“ ì ìˆ˜ë„ í‘œì‹œ
    raw_rerank_score = results_with[0].metadata.get('raw_rerank_score', None)
    if raw_rerank_score is not None:
        print(f"             (Cross-Encoder ë¡œì§“: {raw_rerank_score:.4f})")
    
    if before_chunk != after_chunk:
        print("\n  [OK] Re-rankingìœ¼ë¡œ ìˆœìœ„ê°€ ë³€ê²½ë¨!")
        print("  -> Cross-Encoderê°€ ì¿¼ë¦¬-ë¬¸ì„œ ê´€ë ¨ì„±ì„ ë” ì •í™•íˆ í‰ê°€")
    else:
        print("\n  -> 1ìœ„ëŠ” ë™ì¼í•˜ë‚˜, í•˜ìœ„ ìˆœìœ„ì—ì„œ ë³€í™” ë°œìƒ ê°€ëŠ¥")
    
    print(f"\n[TIP] Re-rankingì˜ í•µì‹¬:")
    print("  - ì´ˆê¸° ê²€ìƒ‰: Hybrid ì ìˆ˜ (0~1 ë²”ìœ„)")
    print("  - Re-ranking: Cross-Encoder ë¡œì§“ â†’ sigmoid ë³€í™˜ (0~1 ë²”ìœ„)")
    print("    * sigmoid(x) = 1/(1+e^(-x))ë¡œ ë¡œì§“ì„ í™•ë¥ ë¡œ ë³€í™˜")
    print("    * ì˜ˆ: ë¡œì§“ 7.55 â†’ sigmoid(7.55) = 0.9995")
    print("  - ê°™ì€ ìŠ¤ì¼€ì¼(0~1)ë¡œ ë¹„êµ ê°€ëŠ¥")
    print("  - ì‹¤ë¬´: ì´ˆê¸° 20~50ê°œ â†’ Re-ranking â†’ ìƒìœ„ 5~10ê°œ ì‚¬ìš©")
    
    # Re-ranking íš¨ê³¼ ì •ëŸ‰í™” (Precision/Recall ì„¤ëª…)
    print(f"\n{'â”€'*60}")
    print("[INFO] Re-ranking íš¨ê³¼ ì •ëŸ‰í™” (Precision@K / Recall@K)")
    print(f"{'â”€'*60}")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [?] Re-rankingì´ ì •ë§ íš¨ê³¼ê°€ ìˆë‚˜ìš”?                                    â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  [ê°œë… ì„¤ëª…]                                                            â”‚
  â”‚  â€¢ Recall@K: Kê°œ ê²°ê³¼ ì¤‘ ì •ë‹µì´ ëª‡ ê°œ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€? (0~1)           â”‚
  â”‚    â†’ "ì •ë‹µì„ ë†“ì¹˜ì§€ ì•ŠëŠ”ê°€?" (ë¹ ëœ¨ë¦¼ ì—†ì´ ì°¾ê¸°)                         â”‚
  â”‚                                                                         â”‚
  â”‚  â€¢ Precision@K: Kê°œ ê²°ê³¼ ì¤‘ ì •ë‹µì˜ ë¹„ìœ¨ì€? (0~1)                        â”‚
  â”‚    â†’ "ì“¸ëª¨ì—†ëŠ” ê²°ê³¼ê°€ ì„ì—¬ìˆì§€ ì•Šì€ê°€?" (ì •í™•íˆ ì°¾ê¸°)                   â”‚
  â”‚                                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [ì˜ˆì‹œ] Re-ranking ì „í›„ ë¹„êµ (ì‹¤ë¬´ ê¸°ì¤€)                                â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  ë‹¨ê³„            â”‚ Recall@5 â”‚ Precision@5 â”‚ í•´ì„                        â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  Before (Hybrid) â”‚ 1.0      â”‚ 0.40        â”‚ ì •ë‹µ í¬í•¨, ë…¸ì´ì¦ˆ ë§ìŒ      â”‚
  â”‚  After (Rerank)  â”‚ 1.0      â”‚ 0.80        â”‚ ì •ë‹µ í¬í•¨, ë…¸ì´ì¦ˆ ê°ì†Œ!     â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  [í•´ì„]                                                                 â”‚
  â”‚  â€¢ Recall ìœ ì§€: ì •ë‹µ ë¬¸ì„œë¥¼ ë†“ì¹˜ì§€ ì•ŠìŒ (ì•ˆì „)                          â”‚
  â”‚  â€¢ Precision í–¥ìƒ: 5ê°œ ì¤‘ ê´€ë ¨ ë¬¸ì„œê°€ 2ê°œ â†’ 4ê°œë¡œ ì¦ê°€                  â”‚
  â”‚  â€¢ ê²°ê³¼: LLMì— ì „ë‹¬ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ â†‘ â†’ ë‹µë³€ í’ˆì§ˆ â†‘                  â”‚
  â”‚                                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [!] ì˜¤í•´ ë°©ì§€: "Re-rankingì´ ì˜¤íˆë ¤ ìˆœìœ„ë¥¼ ë§ì³¤ì–´ìš”!"                  â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  ê°€ë” Re-ranking í›„ 1ìœ„ê°€ ë°”ë€Œì–´ì„œ "ë” ë‚˜ë¹ ì¡Œë‹¤"ê³  ëŠë‚„ ìˆ˜ ìˆìŒ         â”‚
  â”‚                                                                         â”‚
  â”‚  â†’ ì‹¤ì œë¡œëŠ”:                                                            â”‚
  â”‚    1. ê°œë³„ ê²°ê³¼ê°€ ì•„ë‹Œ ì „ì²´ ìƒìœ„ Kê°œì˜ í’ˆì§ˆë¡œ íŒë‹¨í•´ì•¼ í•¨               â”‚
  â”‚    2. Cross-EncoderëŠ” ì¿¼ë¦¬-ë¬¸ì„œ ìŒì„ ì§ì ‘ ë¹„êµ (ë” ì •í™•)               â”‚
  â”‚    3. Bi-Encoder(ì´ˆê¸° ê²€ìƒ‰)ë³´ë‹¤ ê´€ë ¨ì„± íŒë‹¨ ì •í™•ë„ 10~20% ë†’ìŒ         â”‚
  â”‚                                                                         â”‚
  â”‚  [ì‹¤ë¬´ ê²€ì¦ ë°©ë²•]                                                       â”‚
  â”‚  1. í…ŒìŠ¤íŠ¸ì…‹ ì¤€ë¹„ (ì§ˆë¬¸ + ì •ë‹µ ë¬¸ì„œ ë¼ë²¨)                               â”‚
  â”‚  2. Before/Afterì˜ Precision@5, Recall@5 ì¸¡ì •                          â”‚
  â”‚  3. ìˆ˜ì¹˜ê°€ ê°œì„ ë˜ë©´ Re-ranking ë„ì…, ì•„ë‹ˆë©´ ìƒëµ                        â”‚
  â”‚                                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [!!!] ì ˆëŒ€ ì£¼ì˜: Recallì´ ë–¨ì–´ì§€ëŠ” Re-rankingì€ ì±„íƒ ê¸ˆì§€              â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  â€¢ Precision í–¥ìƒ + Recall ìœ ì§€ â†’ âœ… ì±„íƒ                               â”‚
  â”‚  â€¢ Precision í–¥ìƒ + Recall í•˜ë½ â†’ âŒ ì ˆëŒ€ ê¸ˆì§€!                         â”‚
  â”‚                                                                         â”‚
  â”‚  ì´ìœ : ì •ë‹µì„ ë†“ì¹˜ê¸° ì‹œì‘í•˜ë©´ RAG ì‹ ë¢°ì„±ì´ ì™„ì „íˆ ë¶•ê´´ë¨                â”‚
  â”‚       "ê°€ë” ì¢‹ì€ ë‹µë³€" < "í•­ìƒ ê´€ë ¨ ë¬¸ì„œëŠ” í¬í•¨"                        â”‚
  â”‚                                                                         â”‚
  â”‚  ì‹¤ë¬´ ê¸°ì¤€: Recall@K â‰¥ 0.95 ìœ ì§€ê°€ ìµœì†Œ ì¡°ê±´                            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # ì‹¤ì œ ê²°ê³¼ ë¶„ì„
    print(f"\n[ë¶„ì„] ì´ë²ˆ ì‹¤í—˜ ê²°ê³¼:")
    # ê´€ë ¨ ì²­í¬ ì‹ë³„ (Transformer ê´€ë ¨ ë‚´ìš©ì„ í¬í•¨í•˜ëŠ”ì§€)
    relevant_keywords = ["transformer", "attention", "ë”¥ëŸ¬ë‹", "ì‹ ê²½ë§", "ì•„í‚¤í…ì²˜"]
    
    def is_relevant(content: str) -> bool:
        content_lower = content.lower()
        return any(kw in content_lower for kw in relevant_keywords)
    
    before_relevant = sum(1 for r in results_without[:5] if is_relevant(r.content))
    after_relevant = sum(1 for r in results_with[:5] if is_relevant(r.content))
    
    print(f"  â€¢ Before ìƒìœ„ 5ê°œ ì¤‘ ê´€ë ¨ ë¬¸ì„œ: {before_relevant}ê°œ (Precision@5 = {before_relevant/5:.1%})")
    print(f"  â€¢ After ìƒìœ„ 5ê°œ ì¤‘ ê´€ë ¨ ë¬¸ì„œ:  {after_relevant}ê°œ (Precision@5 = {after_relevant/5:.1%})")
    
    if after_relevant > before_relevant:
        print(f"  â†’ [v] Re-rankingìœ¼ë¡œ Precision í–¥ìƒ! ({before_relevant} â†’ {after_relevant})")
    elif after_relevant == before_relevant:
        print(f"  â†’ [~] Precision ë™ì¼ (ìˆœìœ„ë§Œ ë³€ê²½, ìƒìœ„ Kê°œ êµ¬ì„±ì€ ìœ ì‚¬)")
    else:
        print(f"  â†’ [!] ì´ ì¿¼ë¦¬ì—ì„œëŠ” Re-ranking íš¨ê³¼ ë¯¸ë¯¸")
        print(f"       (ëª¨ë“  ì¿¼ë¦¬ì— íš¨ê³¼ì ì¸ ê²ƒì€ ì•„ë‹˜ - í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ê²€ì¦ í•„ìš”)")


def experiment_multi_hop(text: str = None):
    """ì‹¤ìŠµ 3: Multi-hop ê²€ìƒ‰"""
    print("\n" + "="*80)
    print("[3] ì‹¤ìŠµ 3: Multi-hop ê²€ìƒ‰ (ë‹¤ë‹¨ê³„ ì¶”ë¡ )")
    print("="*80)
    print("ëª©í‘œ: ë³µì¡í•œ ì§ˆë¬¸ì„ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ê³  ë‹¨ê³„ì ìœ¼ë¡œ ê²€ìƒ‰")
    
    sample_text = text or SAMPLE_TEXT
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = AdvancedRAGSystem(chunk_size=400, use_reranker=False)
    rag.doc_processor = DocumentProcessor(chunk_size=400, chunk_overlap=50)
    rag.ingest_text(sample_text, source_name="AI_ê°€ì´ë“œ", collection_name="multihop_exp")
    
    # ë³µì¡í•œ ì¿¼ë¦¬ (ë‘ ê°œë…ì„ ì—°ê²°í•´ì•¼ ë‹µí•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸)
    complex_query = "ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì¢…ë¥˜ì™€ ê°ê°ì´ ì–´ë–¤ ì‹¤ë¬´ ë¶„ì•¼ì— ì ìš©ë˜ëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    
    print(f"\n[*] ë³µì¡í•œ ì¿¼ë¦¬: '{complex_query}'")
    print(f"ì´ ì²­í¬ ìˆ˜: {len(rag.retriever.documents)}ê°œ")
    
    # ì¼ë°˜ ê²€ìƒ‰ê³¼ Multi-hop ë¹„êµ
    print(f"\n{'â”€'*60}")
    print("[>] ì¼ë°˜ Hybrid ê²€ìƒ‰ (ë‹¨ì¼ ì¿¼ë¦¬)")
    print(f"{'â”€'*60}")
    
    simple_results = rag.search(complex_query, method="hybrid", k=3)
    print(f"ê²€ìƒ‰ ê²°ê³¼: {len(simple_results)}ê°œ")
    # ìƒìœ„ 1ê°œ ì „ì²´ í‘œì‹œ
    print_search_result(simple_results[0], 1, show_full=True)
    
    # Multi-hop ê²€ìƒ‰
    print(f"\n{'â”€'*60}")
    print("[>] Multi-hop ê²€ìƒ‰ (ì¿¼ë¦¬ ë¶„í•´ -> ë‹¨ê³„ì  ê²€ìƒ‰)")
    print(f"{'â”€'*60}")
    
    results, metadata = rag.multi_hop_search(complex_query, k=3, use_reranker=False)
    
    print(f"\n[INFO] Multi-hop ê³¼ì •:")
    print(f"  - ì›ë³¸ ì¿¼ë¦¬ -> {len(metadata['sub_queries'])}ê°œ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´")
    for i, sq in enumerate(metadata.get('sub_queries', []), 1):
        print(f"    {i}. {sq}")
    
    print(f"\n  - Hop 1: ì²« ë²ˆì§¸ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰")
    print(f"  - Hop 2: Hop 1 ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ë‘ ë²ˆì§¸ ì§ˆë¬¸ ê²€ìƒ‰")
    print(f"  - ìµœì¢… ê²°ê³¼: ë‘ í™‰ì˜ ê²°ê³¼ ë³‘í•© ({len(results)}ê°œ)")
    
    print(f"\nìƒìœ„ ê²°ê³¼:")
    # ìƒìœ„ 2ê°œ ì „ì²´ í‘œì‹œ
    for i, result in enumerate(results[:2], 1):
        print_search_result(result, i, show_full=True)
    
    # ë‹µë³€ ìƒì„±
    print(f"\n{'â”€'*60}")
    print("[>] LLM ë‹µë³€ ìƒì„±")
    print(f"{'â”€'*60}")
    answer_data = rag.generate_answer(complex_query, results)
    print(f"\n{answer_data['answer']}")
    
    print(f"\n[TIP] Multi-hopì˜ í•µì‹¬:")
    print("  - ë³µì¡í•œ ì§ˆë¬¸ì„ ë‹¨ìˆœí•œ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´")
    print("  - ì²« ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‘ ë²ˆì§¸ ê²€ìƒ‰ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©")
    print("  - 'ê°œë… Aë€?' + 'Aê°€ ì–´ë””ì— ì“°ì´ë‚˜?' í˜•íƒœì˜ ì§ˆë¬¸ì— íš¨ê³¼ì ")
    
    print(f"\n  âš ï¸ [ì‹¤ë¬´ ì£¼ì˜] Multi-hopì´ í•­ìƒ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ë³´ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
    print("     - ì§§ê³  ë‹¨ìˆœí•œ ì§ˆë¬¸: ì˜¤íˆë ¤ ì†ë„â†“ ë¹„ìš©â†‘ (LLM í˜¸ì¶œ ì¶”ê°€)")
    print("     - ì í•©í•œ ê²½ìš°: ë‘ ê°œë… ì´ìƒì„ ì—°ê²°í•´ì•¼ í•˜ëŠ” ë³µì¡í•œ ì§ˆë¬¸")
    print("     - ì‹¤ë¬´ ê¶Œì¥: ì§ˆë¬¸ ë³µì¡ë„ë¥¼ ë¨¼ì € íŒë‹¨ â†’ ì¡°ê±´ë¶€ Multi-hop ì ìš©")
    
    # Evidence Fusion ê°œë… ì„¤ëª…
    print(f"\n{'â”€'*60}")
    print("[ADVANCED] Evidence Fusion - ì¦ê±° ê²°í•© ë°©ì‹")
    print(f"{'â”€'*60}")
    print("""
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  [?] Hop1 â†’ Hop2 ì—°ê²°ì„ ë” ì •êµí•˜ê²Œ í•  ìˆ˜ ì—†ë‚˜ìš”?                       â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  í˜„ì¬ êµ¬í˜„ (Basic Multi-hop):                                           â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  Hop1 ê²°ê³¼ ì „ì²´ â†’ ì»¨í…ìŠ¤íŠ¸ë¡œ Hop2 ì¿¼ë¦¬ì— ì¶”ê°€                           â”‚
  â”‚                                                                         â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
  â”‚  â”‚  Hop1    â”‚â”€â”€â”€â”€â–¶â”‚ Hop1 ê²°ê³¼ (ì „ì²´ í…ìŠ¤íŠ¸) â”‚â”€â”€â”€â”€â–¶â”‚  Hop2    â”‚         â”‚
  â”‚  â”‚  ê²€ìƒ‰    â”‚     â”‚ 200ìì”© ìƒìœ„ 3ê°œ        â”‚     â”‚  ê²€ìƒ‰    â”‚         â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
  â”‚                                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  ì‹¤ë¬´ ê¶Œì¥ (Entity-based Evidence Fusion):                              â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  Hop1 ê²°ê³¼ â†’ í•µì‹¬ ì—”í‹°í‹° ì¶”ì¶œ â†’ Hop2 ì¿¼ë¦¬ì— ì‚½ì…                        â”‚
  â”‚                                                                         â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
  â”‚  â”‚  Hop1    â”‚â”€â”€â”€â”€â–¶â”‚ í•µì‹¬ ì—”í‹°í‹° ì¶”ì¶œ        â”‚â”€â”€â”€â”€â–¶â”‚  Hop2    â”‚         â”‚
  â”‚  â”‚  ê²€ìƒ‰    â”‚     â”‚ CNN, RNN, Transformer   â”‚     â”‚  ê²€ìƒ‰    â”‚         â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
  â”‚                                                                         â”‚
  â”‚  [ì˜ˆì‹œ] "ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì˜ ì‹¤ì œ ì ìš© ë¶„ì•¼ëŠ”?"                              â”‚
  â”‚                                                                         â”‚
  â”‚  Hop1: "ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì¢…ë¥˜" ê²€ìƒ‰                                        â”‚
  â”‚       â†’ ê²°ê³¼: "CNNì€ ì´ë¯¸ì§€ ì²˜ë¦¬ì—, RNNì€ ì‹œê³„ì—´ì—, TransformerëŠ”..."   â”‚
  â”‚                                                                         â”‚
  â”‚  ì—”í‹°í‹° ì¶”ì¶œ:                                                           â”‚
  â”‚       â†’ ["CNN", "RNN", "Transformer", "GAN", "LSTM"]                   â”‚
  â”‚                                                                         â”‚
  â”‚  Hop2: ì›ë³¸ ì¿¼ë¦¬ + ì¶”ì¶œ ì—”í‹°í‹°                                          â”‚
  â”‚       â†’ "CNN, RNN, Transformer, GANì˜ ì‹¤ì œ ì ìš© ë¶„ì•¼ëŠ”?"               â”‚
  â”‚                                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [êµ¬í˜„ íŒíŠ¸] ì—”í‹°í‹° ì¶”ì¶œ ë°©ë²•                                           â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  # ë°©ë²• 1: LLMìœ¼ë¡œ ì¶”ì¶œ (ì •í™•ë„ ë†’ìŒ, ë¹„ìš© ë°œìƒ)                        â”‚
  â”‚  prompt = '''                                                           â”‚
  â”‚  ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ê¸°ìˆ /ê°œë… ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.                       â”‚
  â”‚  í…ìŠ¤íŠ¸: {hop1_context}                                                 â”‚
  â”‚  ì—”í‹°í‹°: [ì‰¼í‘œë¡œ êµ¬ë¶„]                                                  â”‚
  â”‚  '''                                                                    â”‚
  â”‚                                                                         â”‚
  â”‚  # ë°©ë²• 2: ê·œì¹™ ê¸°ë°˜ (ë¹ ë¦„, ë„ë©”ì¸ ì‚¬ì „ í•„ìš”)                           â”‚
  â”‚  tech_terms = ["CNN", "RNN", "LSTM", "Transformer", ...]               â”‚
  â”‚  entities = [t for t in tech_terms if t.lower() in hop1_text.lower()]  â”‚
  â”‚                                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [!!!] Entity Fusionì˜ ë¦¬ìŠ¤í¬                                           â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚                                                                         â”‚
  â”‚  â€¢ ì˜ëª»ëœ ì—”í‹°í‹° ì¶”ì¶œ â†’ ì™„ì „íˆ ì—‰ëš±í•œ Hop2 ê²€ìƒ‰                         â”‚
  â”‚    ì˜ˆ: "CNN ë‰´ìŠ¤ ì±„ë„" vs "CNN ì‹ ê²½ë§" í˜¼ë™                             â”‚
  â”‚                                                                         â”‚
  â”‚  â€¢ ì‹¤ë¬´ í•´ê²°ì±…:                                                         â”‚
  â”‚    - Top-N ì—”í‹°í‹° í›„ë³´ë¥¼ ëª¨ë‘ ì‚¬ìš© (AND/OR ì¡°í•©)                        â”‚
  â”‚    - ì˜ˆ: "CNN OR RNN OR Transformer" ì¿¼ë¦¬ í™•ì¥                         â”‚
  â”‚    - ë‹¨ì¼ ì—”í‹°í‹° ì˜ì¡´ âŒ â†’ í›„ë³´êµ° ê¸°ë°˜ ê²€ìƒ‰ âœ…                          â”‚
  â”‚                                                                         â”‚
  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
  â”‚  [â†’] ì´ ê°œë…ì´ ë°œì „í•˜ë©´ Agent RAGë¡œ ì—°ê²°ë©ë‹ˆë‹¤!                         â”‚
  â”‚      â€¢ Agent: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ë‹¤ìŒ ì•¡ì…˜ ê²°ì •                        â”‚
  â”‚      â€¢ Tool Use: ê²€ìƒ‰, ê³„ì‚°, API í˜¸ì¶œ ë“± ë„êµ¬ ì„ íƒ                       â”‚
  â”‚      â€¢ Lab 4ì—ì„œ ìì„¸íˆ ë‹¤ë£¹ë‹ˆë‹¤                                        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)


def experiment_context_window(text: str = None):
    """ì‹¤ìŠµ 5: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬"""
    print("\n" + "="*80)
    print("[5] ì‹¤ìŠµ 5: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬")
    print("="*80)
    print("ëª©í‘œ: LLMì˜ í† í° ì œí•œì„ ê³ ë ¤í•œ ì»¨í…ìŠ¤íŠ¸ ìµœì í™”")
    
    sample_text = text or SAMPLE_TEXT
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì‘ì€ ì²­í¬ë¡œ ë§ì€ ê²°ê³¼ ìƒì„±)
    rag = AdvancedRAGSystem(chunk_size=300, use_reranker=False)
    rag.doc_processor = DocumentProcessor(chunk_size=300, chunk_overlap=30)
    rag.ingest_text(sample_text, source_name="AI_ê°€ì´ë“œ", collection_name="context_exp")
    
    test_query = "ì¸ê³µì§€ëŠ¥ì˜ ëª¨ë“  ë¶„ì•¼ì™€ í™œìš© ì‚¬ë¡€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    
    print(f"\n[*] í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: '{test_query}'")
    print(f"ì´ ì²­í¬ ìˆ˜: {len(rag.retriever.documents)}ê°œ")
    
    # ë§ì€ ê²°ê³¼ ê²€ìƒ‰ (ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì´ˆê³¼ ê°€ëŠ¥)
    k = min(15, len(rag.retriever.documents))
    print(f"ê²€ìƒ‰ ê²°ê³¼ ìš”ì²­: {k}ê°œ")
    
    results = rag.search(test_query, method="hybrid", k=k)
    
    # ì „ì²´ í† í° ìˆ˜ ê³„ì‚°
    total_tokens = sum(rag.context_manager.count_tokens(r.content) for r in results)
    print(f"ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì´ í† í°: {total_tokens}ê°œ")
    
    # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬ ì—†ì´
    print(f"\n{'â”€'*60}")
    print("[>] ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬ ì—†ì´ (ì „ì²´ ê²°ê³¼ ì‚¬ìš©)")
    print(f"{'â”€'*60}")
    
    answer_data_without = rag.generate_answer(test_query, results, manage_context=False)
    print(f"  - ì‚¬ìš©ëœ ê²°ê³¼: {len(results)}ê°œ (ì „ì²´)")
    print(f"  - ìƒì„± ì‹œê°„: {answer_data_without['elapsed_time']:.2f}ì´ˆ")
    print(f"  - ë‹µë³€ ê¸¸ì´: {len(answer_data_without['answer'])}ì")
    print(f"\n[DOC] ë‹µë³€ (ê´€ë¦¬ ì—†ìŒ):")
    print(f"  {answer_data_without['answer'][:400]}{'...' if len(answer_data_without['answer']) > 400 else ''}")
    
    # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬ ì ìš© (ë” ì œí•œì ì¸ ì„¤ì •)
    print(f"\n{'â”€'*60}")
    print("[>] ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬ ì ìš© (í† í° ì œí•œ ì¤€ìˆ˜)")
    print(f"{'â”€'*60}")
    
    # ë” ì‘ì€ í† í° ì œí•œìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    rag.context_manager.max_tokens = 2048  # ë” ì œí•œì ìœ¼ë¡œ ì„¤ì •
    
    answer_data_with = rag.generate_answer(test_query, results, manage_context=True)
    
    if answer_data_with["context_stats"]:
        stats = answer_data_with["context_stats"]
        print(f"\n[INFO] í† í° ë¶„ë°°:")
        print(f"  +-- ìµœëŒ€ í† í°: {stats['total_tokens']}ê°œ")
        print(f"  +-- ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: {stats['system_tokens']}ê°œ")
        print(f"  +-- ì¿¼ë¦¬: {stats['query_tokens']}ê°œ")
        print(f"  +-- ì‘ë‹µ ì˜ˆì•½: {stats['reserve_tokens']}ê°œ")
        print(f"  +-- ì»¨í…ìŠ¤íŠ¸ ê°€ìš©: {stats['available_tokens']}ê°œ")
        print(f"  +-- ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©: {stats['used_tokens']}ê°œ")
        
        print(f"\n[INFO] ê²°ê³¼ í•„í„°ë§:")
        print(f"  - ì›ë³¸ ê²°ê³¼: {len(results)}ê°œ")
        print(f"  - í¬í•¨ëœ ê²°ê³¼: {stats['num_results']}ê°œ")
        print(f"  - ì˜ë¦° ê²°ê³¼: {stats['num_truncated']}ê°œ")
        print(f"  - ì œì™¸ëœ ê²°ê³¼: {len(results) - stats['num_results']}ê°œ")
    
    print(f"\n  - ìƒì„± ì‹œê°„: {answer_data_with['elapsed_time']:.2f}ì´ˆ")
    print(f"  - ë‹µë³€ ê¸¸ì´: {len(answer_data_with['answer'])}ì")
    print(f"\n[DOC] ë‹µë³€ (ê´€ë¦¬ ì ìš©):")
    print(f"  {answer_data_with['answer'][:400]}{'...' if len(answer_data_with['answer']) > 400 else ''}")
    
    # ë‹µë³€ ë¹„êµ
    print(f"\n{'â”€'*60}")
    print("[INFO] ë‹µë³€ ë¹„êµ ë¶„ì„")
    print(f"{'â”€'*60}")
    
    # í† í° ì •ë³´ ì¶”ì¶œ
    context_stats_with = answer_data_with.get("context_stats", {})
    used_tokens_with = context_stats_with.get("used_tokens", 0) if context_stats_with else 0
    context_tokens_without = sum(rag.context_manager.count_tokens(r.content) for r in results)
    
    # ë¹„êµ í‘œ
    print(f"\n{'í•­ëª©':<16} {'ê´€ë¦¬ ì—†ìŒ':<12} {'ê´€ë¦¬ ì ìš©':<12} {'ì°¨ì´':<12}")
    print("â”€" * 52)
    
    # í† í° ë¹„êµ
    token_diff = context_tokens_without - used_tokens_with
    token_percent = (1 - used_tokens_with/context_tokens_without)*100 if context_tokens_without > 0 else 0
    print(f"{'ì»¨í…ìŠ¤íŠ¸ í† í°':<16} {context_tokens_without:<12} {used_tokens_with:<12} -{token_diff} ({token_percent:.1f}%â†“)")
    
    # ì‹œê°„ ë¹„êµ
    time_diff = answer_data_with['elapsed_time'] - answer_data_without['elapsed_time']
    print(f"{'ìƒì„± ì‹œê°„':<16} {answer_data_without['elapsed_time']:.2f}ì´ˆ{'':<6} {answer_data_with['elapsed_time']:.2f}ì´ˆ{'':<6} {time_diff:+.2f}ì´ˆ")
    
    # ë‹µë³€ ê¸¸ì´ ë¹„êµ
    len_diff = len(answer_data_with['answer']) - len(answer_data_without['answer'])
    print(f"{'ë‹µë³€ ê¸¸ì´':<16} {len(answer_data_without['answer'])}ì{'':<6} {len(answer_data_with['answer'])}ì{'':<6} {len_diff:+d}ì")
    
    print(f"\n[í•µì‹¬]")
    print(f"  * ì»¨í…ìŠ¤íŠ¸ í† í°: {token_percent:.1f}% ì ˆê°!")
    print(f"  * ìƒì„± ì‹œê°„: ê±°ì˜ ë™ì¼ (ì¶œë ¥ í† í°ì´ ì‹œê°„ ê²°ì •)")
    if len_diff > 0:
        print(f"  * ë‹µë³€ í’ˆì§ˆ: ì»¨í…ìŠ¤íŠ¸ ì œí•œì´ ì˜¤íˆë ¤ ë” ì§‘ì¤‘ëœ ë‹µë³€ ìƒì„±")
    else:
        print(f"  * ë‹µë³€ í’ˆì§ˆ: ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ê°€ ë” ê¸´ ë‹µë³€ ìƒì„±")
    
    print(f"\n[TIP] ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬ì˜ í•µì‹¬:")
    print("  - LLMë§ˆë‹¤ ìµœëŒ€ í† í° ì œí•œ ì¡´ì¬ (GPT-4: 8K~128K)")
    print("  - í”„ë¡¬í”„íŠ¸ + ì»¨í…ìŠ¤íŠ¸ + ì‘ë‹µ <= ìµœëŒ€ í† í°")
    print("  - ê´€ë ¨ì„± ë†’ì€ ì²­í¬ë¶€í„° í¬í•¨, ì´ˆê³¼ ì‹œ ìë¥´ê±°ë‚˜ ì œì™¸")
    print("  - ì‹¤ë¬´: í† í° ë¹„ìš©ê³¼ í’ˆì§ˆì˜ ê· í˜• ê³ ë ¤")
    
    print(f"\n  âš ï¸ [ì£¼ì˜] ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³¼ë„í•˜ê²Œ ì¤„ì´ë©´ ë°œìƒí•˜ëŠ” ë¬¸ì œ:")
    print("     - ê·¼ê±° ë¶€ì¡±: ë‹µë³€ì˜ ì¶œì²˜/ì¦ê±°ê°€ ë¶ˆì¶©ë¶„")
    print("     - í™˜ê° ì¦ê°€: ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì„ LLMì´ ë§Œë“¤ì–´ëƒ„")
    print("     - ë²•ì  ë¦¬ìŠ¤í¬: ê¸ˆìœµ/ì˜ë£Œ/ë²•ë¥  ë¶„ì•¼ì—ì„œ ì¦ë¹™ ë¶ˆê°€")
    print("     â†’ 'í† í° ì ˆê° = ë¬´ì¡°ê±´ ì¢‹ë‹¤'ê°€ ì•„ë‹˜! ë„ë©”ì¸ë³„ ìµœì†Œ ê¸°ì¤€ í•„ìš”")


def run_all_experiments(text: str = None, pdf_path: str = None, force_sample: bool = False):
    """
    ëª¨ë“  ì‹¤ìŠµ ì‹¤í–‰
    
    Args:
        text: ìƒ˜í”Œ í…ìŠ¤íŠ¸ (ê¸°ë³¸: ë‚´ì¥ SAMPLE_TEXT)
        pdf_path: PDF íŒŒì¼ ê²½ë¡œ (ì„ íƒ)
        force_sample: Trueë©´ PDF ë¬´ì‹œí•˜ê³  ë‚´ì¥ í…ìŠ¤íŠ¸ ì‚¬ìš©
    """
    print("\n[LIST] ì‹¤ìŠµ í•­ëª©:")
    print("  1. ê²€ìƒ‰ ë°©ë²• ë¹„êµ (Sparse vs Dense vs Hybrid)")
    print("  2. Re-ranking íš¨ê³¼ (Cross-Encoder í™œìš©)")
    print("  3. Multi-hop ê²€ìƒ‰ (ë‹¤ë‹¨ê³„ ì¶”ë¡ )")
    print("  4. Chunk size ì‹¤í—˜ (256 vs 512 vs 1024)")
    print("  5. ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬")
    print()
    
    # í…ìŠ¤íŠ¸ ê²°ì •
    sample_text = None
    
    if not force_sample and pdf_path and os.path.exists(pdf_path):
        processor = DocumentProcessor()
        pdf_text = processor.load_pdf(pdf_path)
        
        if len(pdf_text) >= MIN_TEXT_LENGTH:
            print(f"[FILE] PDF íŒŒì¼ ì‚¬ìš©: {pdf_path}")
            sample_text = pdf_text
        else:
            print(f"[!] PDF íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŒ ({len(pdf_text)}ì < {MIN_TEXT_LENGTH}ì)")
            print("   -> ë‚´ì¥ ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            sample_text = text or SAMPLE_TEXT
    else:
        print("[DOC] ë‚´ì¥ ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì‚¬ìš© (AI/ML ê°€ì´ë“œ)")
        sample_text = text or SAMPLE_TEXT
    
    print(f"   í…ìŠ¤íŠ¸ ê¸¸ì´: {len(sample_text)} ë¬¸ì")
    
    try:
        # ì‹¤ìŠµ 1: ê²€ìƒ‰ ë°©ë²• ë¹„êµ
        experiment_search_methods(sample_text)
        
        # ì‹¤ìŠµ 2: Re-ranking
        experiment_reranking(sample_text)
        
        # ì‹¤ìŠµ 3: Multi-hop
        experiment_multi_hop(sample_text)
        
        # ì‹¤ìŠµ 4: Chunk size
        experiment_chunk_sizes(sample_text)
        
        # ì‹¤ìŠµ 5: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°
        experiment_context_window(sample_text)
        
        print("\n" + "="*80)
        print("[OK] ëª¨ë“  ì‹¤ìŠµ ì™„ë£Œ!")
        print("="*80)
        print("\n[TIP] ì‹¤ìŠµ íŒ:")
        print("  - ë” ê°•ë ¥í•œ Re-ranker: BAAI/bge-reranker-base (~500MB)")
        print("  - í˜„ì¬ ì‚¬ìš©: cross-encoder/ms-marco-MiniLM-L-6-v2 (~80MB)")
        print("  - ìì‹ ì˜ PDFë¡œ ì‹¤ìŠµ: run_all_experiments(pdf_path='your.pdf')")
        
    except Exception as e:
        print(f"\n[X] ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("="*80)
    print("[LAB] Advanced RAG ì‹¤ìŠµ - ì „ì²´ ë°ëª¨")
    print("="*80)
    
    # ìƒ˜í”Œ PDF íŒŒì¼ ê²½ë¡œ (ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë‚´ì¥ í…ìŠ¤íŠ¸ ì‚¬ìš©)
    sample_pdf = Path(__file__).parent / "sample.pdf"
    
    if sample_pdf.exists():
        # PDF íŒŒì¼ì´ ìˆìœ¼ë©´ ì‚¬ìš© (ì„ íƒ)
        print(f"\n[FILE] PDF íŒŒì¼ ë°œê²¬: {sample_pdf}")
        print("   (ë‚´ì¥ ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¡œ ì‹¤í–‰í•˜ë ¤ë©´ PDFë¥¼ ì‚­ì œí•˜ì„¸ìš”)")
        run_all_experiments(pdf_path=str(sample_pdf))
    else:
        # PDF ì—†ì´ ë‚´ì¥ ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¡œ ì‹¤í–‰
        print("\n[DOC] PDF ì—†ì´ ë‚´ì¥ ìƒ˜í”Œ í…ìŠ¤íŠ¸ë¡œ ì‹¤í–‰")
        print("   (ìì‹ ì˜ PDFë¡œ ì‹¤ìŠµí•˜ë ¤ë©´ sample.pdfë¥¼ lab03 í´ë”ì— ì¶”ê°€)")
        run_all_experiments()
