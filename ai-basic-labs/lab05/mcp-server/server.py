from fastmcp import FastMCP
import asyncio
import logging
import re
from typing import Dict, Any
from datetime import datetime

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ë¶ˆí•„ìš”í•œ ë””ë²„ê·¸ ë¡œê·¸ ìˆ¨ê¸°ê¸°
logging.getLogger("mcp.server").setLevel(logging.INFO)
logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
logging.getLogger("sse_starlette").setLevel(logging.WARNING)
logging.getLogger("mcp.server.lowlevel").setLevel(logging.WARNING)

mcp = FastMCP(name="AriProcessingServer")

# Health check endpoint (MCP tool)
@mcp.tool
def health_check() -> Dict[str, Any]:
    """
    ARI Processing Server í—¬ìŠ¤ì²´í¬
    - BeautifulSoup ì„í¬íŠ¸ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    """
    logger.info("[MCP] health_check called")
    soup_ok = False

    # BeautifulSoup import í™•ì¸
    try:
        import importlib
        importlib.import_module("bs4")
        soup_ok = True
    except Exception as e:
        logger.warning(f"BeautifulSoup import failed: {e}")

    status = "healthy" if soup_ok else "unhealthy"
    return {
        "success": soup_ok,
        "status": status,
        "service": "ari-processing-server",
        "dependencies": {
            "beautifulsoup": soup_ok,
        }
    }


# ============================================================================
# ARI CONTENT PROCESSING TOOLS (HTML êµ¬ì¡°í™” ë° ì „ìš© íŒŒì‹±)
# ============================================================================

def _process_nested_table(table, depth=0) -> str:
    """
    ì¤‘ì²©ëœ í‘œë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
    
    Args:
        table: BeautifulSoup table ê°ì²´
        depth: ì¤‘ì²© ê¹Šì´ (ë“¤ì—¬ì“°ê¸°ìš©)
        
    Returns:
        í‘œë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•œ ë¬¸ìì—´
    """
    from bs4 import BeautifulSoup
    import logging
    import copy
    logger = logging.getLogger(__name__)
    
    indent = "  " * depth  # ì¤‘ì²© ìˆ˜ì¤€ì— ë”°ë¥¸ ë“¤ì—¬ì“°ê¸°
    result_lines = []
    
    try:
        # ëª¨ë“  í–‰ ê°€ì ¸ì˜¤ê¸°
        all_rows = table.find_all('tr')
        
        if not all_rows:
            return ""
        
        # ê° í–‰ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (í—¤ë”/ë°ì´í„° êµ¬ë¶„ ì—†ì´)
        for row_idx, row in enumerate(all_rows):
            cells = row.find_all(['td', 'th'])
            
            if not cells:
                continue
                
            row_items = []
            
            for cell in cells:
                # ì¤‘ì²©ëœ í‘œ í™•ì¸
                nested_tables = cell.find_all('table')
                
                if nested_tables:
                    # ì¤‘ì²©ëœ í‘œê°€ ìˆëŠ” ê²½ìš°
                    # ë¨¼ì € í‘œë¥¼ ì œì™¸í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    cell_copy = copy.copy(cell)
                    for nt in cell_copy.find_all('table'):
                        nt.decompose()
                    
                    cell_text = cell_copy.get_text(strip=True)
                    if cell_text:
                        row_items.append(cell_text)
                    
                    # ì¤‘ì²©ëœ í‘œë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
                    for nt in nested_tables:
                        nested_result = _process_nested_table(nt, depth + 1)
                        if nested_result:
                            row_items.append(f"\n{indent}  [ì¤‘ì²©ëœ í‘œ]\n" + nested_result)
                else:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°
                    cell_text = cell.get_text(strip=True)
                    if cell_text:
                        row_items.append(cell_text)
            
            # í–‰ì— ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if row_items:
                # ê° í•­ëª©ì„ ê°œë³„ ë¼ì¸ìœ¼ë¡œ ì¶”ê°€
                for item in row_items:
                    if item and item.strip():
                        result_lines.append(f"{indent}- {item}")
    
    except Exception as e:
        logger.warning(f"í‘œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        logger.debug(traceback.format_exc())
    
    return "\n".join(result_lines)

def _optimize_markdown_for_rag(markdown_text: str) -> tuple:
    """
    RAG ì‹œìŠ¤í…œì„ ìœ„í•œ ë§ˆí¬ë‹¤ìš´ ìµœì í™”
    
    Args:
        markdown_text: ì›ë³¸ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
        
    Returns:
        (ìµœì í™”ëœ ë§ˆí¬ë‹¤ìš´, ì¶”ì²œ RAG ì„¤ì •) íŠœí”Œ
    """
    import re
    
    lines = markdown_text.split('\n')
    enhanced_lines = []
    
    # í‘œ ì„¹ì…˜ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„
    in_table = False
    table_content = []
    
    for i, line in enumerate(lines):
        # ìƒˆë¡œìš´ í‘œ ì‹œì‘
        if line.startswith('[í‘œ '):
            # ì´ì „ í‘œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì²˜ë¦¬
            if table_content:
                enhanced_lines.extend(table_content)
                enhanced_lines.append('')  # ë¹ˆ ì¤„ë¡œ êµ¬ë¶„
                table_content = []
            
            enhanced_lines.append('\n---\n')  # í‘œ êµ¬ë¶„ì
            enhanced_lines.append(line)
            in_table = True
            
        # ì¤‘ì²©ëœ í‘œ ì‹œì‘
        elif '[ì¤‘ì²©ëœ í‘œ]' in line:
            enhanced_lines.append('\n~~~ ì¤‘ì²© ì‹œì‘ ~~~')
            enhanced_lines.append(line)
            
        # ê¸´ í…ìŠ¤íŠ¸ í•­ëª© ë¶„í•  (200ì ì´ìƒ)
        elif line.startswith('- ') and len(line) > 200:
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            text = line[2:]  # '- ' ì œê±°
            
            # ë§ˆì¹¨í‘œ, ì‰¼í‘œ ë“±ìœ¼ë¡œ ë¶„í• 
            sentences = re.split(r'(?<=[.!?]) (?=[A-Zê°€-í£])', text)
            
            if len(sentences) > 1:
                enhanced_lines.append('- ' + sentences[0])
                for sent in sentences[1:]:
                    if sent.strip():
                        enhanced_lines.append('  ' + sent.strip())
            else:
                # ì‰¼í‘œë¡œë„ ë¶„í•  ì‹œë„
                parts = text.split(', ')
                if len(parts) > 3:  # ì¶©ë¶„íˆ ê¸´ ê²½ìš°ë§Œ
                    enhanced_lines.append('- ' + parts[0] + ',')
                    for part in parts[1:]:
                        if part.strip():
                            enhanced_lines.append('  ' + part.strip() + ',')
                else:
                    enhanced_lines.append(line)
        else:
            if in_table and line.strip() == '':
                in_table = False
            enhanced_lines.append(line)
    
    # ë§ˆì§€ë§‰ í‘œ ë‚´ìš© ì²˜ë¦¬
    if table_content:
        enhanced_lines.extend(table_content)
    
    # ìµœì í™”ëœ ë§ˆí¬ë‹¤ìš´
    optimized_markdown = '\n'.join(enhanced_lines)
    
    # RAG ì„¤ì • ì¶”ì²œ
    # í‘œê°€ ë§ì€ ê²½ìš°ì™€ ì¼ë°˜ í…ìŠ¤íŠ¸ê°€ ë§ì€ ê²½ìš°ë¥¼ êµ¬ë¶„
    table_count = optimized_markdown.count('[í‘œ ')
    avg_line_length = sum(len(line) for line in enhanced_lines) / max(len(enhanced_lines), 1)
    
    if table_count > 5:  # í‘œê°€ ë§ì€ ë¬¸ì„œ
        recommended_config = {
            "separators": ["\n---\n", "\n~~~ ì¤‘ì²©", "\n\n", "\n- ", "\n", ". ", ", "],
            "chunk_size": 2500,  # í‘œë¥¼ ìœ„í•´ ë” í° í¬ê¸°
            "chunk_overlap": min(500, int(2500 * 0.2)),  # ìµœëŒ€ 500, chunk_sizeì˜ 20%
            "strategy": "table_aware"
        }
    elif avg_line_length > 100:  # ê¸´ í…ìŠ¤íŠ¸ê°€ ë§ì€ ë¬¸ì„œ
        recommended_config = {
            "separators": ["\n\n", "\n- ", ". ", ", ", "\n"],
            "chunk_size": 2000,
            "chunk_overlap": min(500, int(2000 * 0.2)),  # ìµœëŒ€ 500, chunk_sizeì˜ 20%
            "strategy": "sentence_aware"
        }
    else:  # ì¼ë°˜ì ì¸ ê²½ìš°
        recommended_config = {
            "separators": ["\n- ", "\n\n", "\n", ". ", ", "],
            "chunk_size": 2000,
            "chunk_overlap": min(500, int(2000 * 0.2)),  # ìµœëŒ€ 500, chunk_sizeì˜ 20%
            "strategy": "balanced"
        }
    
    return optimized_markdown, recommended_config

def _extract_cell_parts_by_html_structure(cell_obj) -> list:
    """
    HTML ì…€ ë‚´ë¶€ì˜ êµ¬ì¡°(p, ul, li, br ë“±)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• 
    
    Args:
        cell_obj: BeautifulSoup ì…€ ê°ì²´
        
    Returns:
        ë¶„í• ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    parts = []
    
    # ì „ì²´ í…ìŠ¤íŠ¸ ë¨¼ì € ì¶”ì¶œ
    full_text = cell_obj.get_text(separator=' ', strip=True)
    
    # ì „ì²´ í…ìŠ¤íŠ¸ê°€ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ë¶„í•  ë¶ˆí•„ìš”)
    if len(full_text) <= 500:
        return [full_text] if full_text else []
    
    # 1. <ul>/<ol> ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„í•  ì‹œë„ (ê°€ì¥ ëª…í™•í•œ êµ¬ì¡°)
    list_tags = cell_obj.find_all(['ul', 'ol'], recursive=False)
    if list_tags:
        # ë¦¬ìŠ¤íŠ¸ í•­ëª© ì¶”ì¶œ
        for list_tag in list_tags:
            li_tags = list_tag.find_all('li')
            for li in li_tags:
                text = li.get_text(separator=' ', strip=True)
                # ì˜ë¯¸ ìˆëŠ” ê¸¸ì´ë§Œ (20ì ì´ìƒ)
                if text and len(text) > 20:
                    parts.append(text)
        
        # ë¦¬ìŠ¤íŠ¸ ì™¸ í…ìŠ¤íŠ¸ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€: ì´ë¯¸ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì œì™¸)
        # ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ì‹œë¡œ ì œê±°í•œ ë³µì‚¬ë³¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        from bs4 import BeautifulSoup
        temp_cell = BeautifulSoup(str(cell_obj), 'html.parser')
        for list_tag in temp_cell.find_all(['ul', 'ol']):
            list_tag.decompose()  # ë¦¬ìŠ¤íŠ¸ ì œê±°
        
        remaining_text = temp_cell.get_text(separator=' ', strip=True)
        if remaining_text and len(remaining_text) > 20:
            # ì´ë¯¸ ì¶”ì¶œëœ ë‚´ìš©ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸
            is_duplicate = any(remaining_text in part or part in remaining_text for part in parts)
            if not is_duplicate:
                parts.insert(0, remaining_text)  # ë¦¬ìŠ¤íŠ¸ ì „ì— ë‚˜ì˜¨ í…ìŠ¤íŠ¸ì´ë¯€ë¡œ ì•ì— ì¶”ê°€
        
        if parts:
            return parts
    
    # 2. <br> íƒœê·¸ë¡œ ë¶„í•  ì‹œë„
    html_str = str(cell_obj)
    if '<br' in html_str.lower():
        from bs4 import BeautifulSoup
        temp_soup = BeautifulSoup(html_str, 'html.parser')
        for br in temp_soup.find_all('br'):
            br.replace_with('\n')
        text = temp_soup.get_text()
        parts = [line.strip() for line in text.split('\n') if line.strip() and len(line.strip()) > 20]
        if len(parts) > 1:
            return parts
    
    # 3. <p> íƒœê·¸ë¡œ ë¶„í•  ì‹œë„ (ë‹¨, ì˜ë¯¸ ìˆëŠ” í¬ê¸°ë§Œ)
    p_tags = cell_obj.find_all('p', recursive=False)
    if p_tags and len(p_tags) > 1:  # 2ê°œ ì´ìƒì¼ ë•Œë§Œ
        for p in p_tags:
            text = p.get_text(separator=' ', strip=True)
            # ì˜ë¯¸ ìˆëŠ” ê¸¸ì´ë§Œ (50ì ì´ìƒ)
            if text and len(text) > 50:
                parts.append(text)
        if parts:
            return parts
    
    # 4. êµ¬ì¡°ê°€ ì—†ê±°ë‚˜ ë„ˆë¬´ ì‘ì€ êµ¬ì¡°ë©´ 600ì ë‹¨ìœ„ë¡œ ë¶„í• 
    if len(full_text) > 600:
        chunk_size = 600
        for i in range(0, len(full_text), chunk_size):
            chunk = full_text[i:i + chunk_size]
            
            # ë‹¨ì–´ ê²½ê³„ ê³ ë ¤
            if i + chunk_size < len(full_text):
                last_space = chunk.rfind(' ')
                if last_space > chunk_size * 0.7:
                    chunk = chunk[:last_space]
            
            if chunk.strip():
                parts.append(chunk.strip())
        return parts
    
    # 5. ê·¸ëŒ€ë¡œ ë°˜í™˜
    return [full_text] if full_text else []


@mcp.tool
def ari_html_to_markdown(html_content: str, extract_tables: bool = True, use_trafilatura: bool = True) -> Dict[str, Any]:
    """
    HTMLì„ RAG ì¹œí™”ì ì¸ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬ (trafilatura + BeautifulSoup + markdownify)
    
    - HTMLì—ì„œ ìˆœìˆ˜ ì»¨í…ì¸ ë§Œ ì¶”ì¶œ (ë¶ˆí•„ìš”í•œ HTML ì½”ë“œ ì œê±°)
    - trafilaturaë¡œ ë…¸ì´ì¦ˆ ì œê±° (ê´‘ê³ , ë„¤ë¹„ê²Œì´ì…˜ ë“±)
    - BeautifulSoupìœ¼ë¡œ í‘œ(table) êµ¬ì¡° íŒŒì‹± ë° êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    - RAG ì‹œìŠ¤í…œì— ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¹”ë”í•œ Markdown ì¶œë ¥
    
    Args:
        html_content: HTML ë³¸ë¬¸ ë¬¸ìì—´
        extract_tables: í‘œë¥¼ ë³„ë„ë¡œ ì¶”ì¶œí• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        use_trafilatura: trafilatura ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
    
    Returns:
        ë³€í™˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (success, result í¬í•¨)
    """
    logger.info(f"[MCP] ari_html_to_markdown í˜¸ì¶œë¨ - HTML í¬ê¸°: {len(html_content)} chars")
    logger.info(f"[MCP] ì˜µì…˜: extract_tables={extract_tables}, use_trafilatura={use_trafilatura}")
    
    try:
        from bs4 import BeautifulSoup, Tag
        import trafilatura
        from markdownify import markdownify as md
        from datetime import datetime
        
        # 1. BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
        soup = BeautifulSoup(html_content, 'lxml')
        
        # 1-1. ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±° (Confluence íŠ¹í™”)
        # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ë„¤ë¹„ê²Œì´ì…˜, ê´‘ê³  ë“± ì œê±°
        for selector in ['script', 'style', 'nav', 'header', 'footer', 'aside', 
                        '.aui-page-header-actions', '.page-actions', '.aui-toolbar2',
                        '.comment-container', '.like-button-container', '.page-labels',
                        'svg', '.aui-icon']:
            for element in soup.select(selector):
                element.decompose()
        
        logger.info("[MCP] ë¶ˆí•„ìš”í•œ HTML ìš”ì†Œ ì œê±° ì™„ë£Œ")
        
        # 1-2. í˜ì´ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì‘ì„±ì, ë‚ ì§œ ë“±)
        metadata_parts = []
        
        # ì œëª© ì¶”ì¶œ
        title = soup.find('h1', {'id': 'title-text'}) or soup.find('title')
        if title:
            metadata_parts.append(f"# {title.get_text(strip=True)}\n")
        
        # ì‘ì„±ì/ìˆ˜ì •ì ì •ë³´ ì¶”ì¶œ
        page_metadata = soup.find('div', class_='page-metadata') or soup.find('div', id='page-metadata')
        if page_metadata:
            metadata_text = page_metadata.get_text(separator=' ', strip=True)
            if metadata_text:
                metadata_parts.append(f"**ë©”íƒ€ë°ì´í„°**: {metadata_text}\n")
        
        # ë¸Œë ˆë“œí¬ëŸ¼ ì¶”ì¶œ
        breadcrumbs = soup.find('ol', {'id': 'breadcrumbs'}) or soup.find('div', class_='breadcrumbs')
        if breadcrumbs:
            breadcrumb_text = breadcrumbs.get_text(separator=' > ', strip=True)
            if breadcrumb_text:
                metadata_parts.append(f"**ê²½ë¡œ**: {breadcrumb_text}\n")
        
        metadata_content = "\n".join(metadata_parts) if metadata_parts else ""
        logger.info(f"[MCP] ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ: {len(metadata_content)} characters")
        
        # 2. í‘œ ì¶”ì¶œ (trafilatura ì „ì— ë¨¼ì € ì¶”ì¶œ)
        tables_text = ""
        tables_count = 0
        
        if extract_tables:
            tables_text_list = []
            # ëª¨ë“  í‘œë¥¼ ì²˜ë¦¬í•˜ë˜, ì¤‘ì²©ëœ í‘œëŠ” ë¶€ëª¨ í‘œ ì²˜ë¦¬ ì‹œì—ë§Œ ì²˜ë¦¬
            all_tables = soup.find_all('table')
            logger.info(f"[MCP] ë°œê²¬ëœ ì „ì²´ í‘œ ê°œìˆ˜: {len(all_tables)}")
            
            # ì´ë¯¸ ì²˜ë¦¬ëœ í‘œë¥¼ ì¶”ì 
            processed_tables = set()
            
            table_idx = 0
            for table in all_tables:
                # ì´ë¯¸ ì²˜ë¦¬ëœ í‘œëŠ” ê±´ë„ˆë›°ê¸°
                if id(table) in processed_tables:
                    continue
                    
                # ì¤‘ì²©ëœ í‘œì¸ì§€ í™•ì¸ (ë¶€ëª¨ê°€ tableì´ë©´ ì¤‘ì²©ëœ í‘œ)
                if table.find_parent('table'):
                    # ì¤‘ì²©ëœ í‘œëŠ” ë¶€ëª¨ í‘œ ì²˜ë¦¬ ì‹œ ì²˜ë¦¬ë˜ë¯€ë¡œ ê±´ë„ˆë›°ê¸°
                    continue
                
                table_idx += 1
                
                try:
                    # ì´ í‘œëŠ” ìµœìƒìœ„ í‘œ
                    # ë‚´ë¶€ì— ì¤‘ì²©ëœ í‘œê°€ ìˆëŠ”ì§€ í™•ì¸
                    nested_tables = table.find_all('table')
                    has_nested_tables = len(nested_tables) > 0
                    if has_nested_tables:
                        logger.info(f"[MCP] í‘œ {table_idx}: ë‚´ë¶€ì— {len(nested_tables)}ê°œì˜ ì¤‘ì²©ëœ í‘œ í¬í•¨")
                        # ì¤‘ì²©ëœ í‘œë“¤ì„ processed ëª©ë¡ì— ì¶”ê°€
                        for nt in nested_tables:
                            processed_tables.add(id(nt))
                    
                    # í‘œ ì œëª© ì°¾ê¸°
                    table_title = None
                    caption = table.find('caption')
                    if caption:
                        table_title = caption.get_text(strip=True)
                    
                    # í—¤ë” ì¶”ì¶œ
                    headers = []
                    has_thead = False
                    used_first_row_as_header = False
                    
                    thead = table.find('thead')
                    if thead:
                        has_thead = True
                        header_row = thead.find('tr')
                        if header_row:
                            # ëª¨ë“  thì™€ td íƒœê·¸ ì¶”ì¶œ (ë³‘í•©ëœ ì…€ ê³ ë ¤)
                            header_cells = header_row.find_all(['th', 'td'])
                            for cell in header_cells:
                                # colspan ì²´í¬
                                colspan = int(cell.get('colspan', 1))
                                cell_text = cell.get_text(strip=True)
                                
                                # colspanì´ ìˆìœ¼ë©´ í•´ë‹¹ ìˆ˜ë§Œí¼ í—¤ë” ì¶”ê°€ (ë¹ˆ í—¤ë”ë¡œ)
                                if colspan > 1:
                                    # ì²« ë²ˆì§¸ëŠ” ì‹¤ì œ í…ìŠ¤íŠ¸, ë‚˜ë¨¸ì§€ëŠ” ë¹ˆ ë¬¸ìì—´
                                    headers.append(cell_text)
                                    for _ in range(colspan - 1):
                                        headers.append('')  # ë³‘í•©ëœ ì…€ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„
                                else:
                                    headers.append(cell_text)
                            
                            logger.info(f"[MCP] í‘œ {table_idx} í—¤ë” ì¶”ì¶œ (thead): {len(headers)}ê°œ - {headers}")
                    
                    # theadê°€ ì—†ê±°ë‚˜ í—¤ë”ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì²« ë²ˆì§¸ í–‰ ì‚¬ìš©
                    if not headers:
                        first_row = table.find('tr')
                        if first_row:
                            header_cells = first_row.find_all(['th', 'td'])
                            for cell in header_cells:
                                colspan = int(cell.get('colspan', 1))
                                cell_text = cell.get_text(strip=True)
                                
                                if colspan > 1:
                                    headers.append(cell_text)
                                    for _ in range(colspan - 1):
                                        headers.append('')
                                else:
                                    headers.append(cell_text)
                            
                            used_first_row_as_header = True
                            logger.info(f"[MCP] í‘œ {table_idx} í—¤ë” ì¶”ì¶œ (ì²« í–‰): {len(headers)}ê°œ - {headers}")
                    
                    # ë°ì´í„° í–‰ ì¶”ì¶œ
                    rows = []
                    tbody = table.find('tbody')
                    if tbody:
                        data_rows = tbody.find_all('tr')
                    else:
                        # tbodyê°€ ì—†ëŠ” ê²½ìš°
                        all_rows = table.find_all('tr')
                        # ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ì‚¬ìš©í–ˆë‹¤ë©´ ë‘ ë²ˆì§¸ í–‰ë¶€í„°, ì•„ë‹ˆë©´ ì²« ë²ˆì§¸ í–‰ë¶€í„°
                        start_idx = 1 if used_first_row_as_header else 0
                        data_rows = all_rows[start_idx:]
                    
                    for row in data_rows:
                        # ì…€ ê°ì²´ ìì²´ë¥¼ ì €ì¥ (ë‚˜ì¤‘ì— HTML êµ¬ì¡° ê¸°ë°˜ ë¶„í•  ìœ„í•´)
                        cell_objects = row.find_all(['td', 'th'])
                        if cell_objects:
                            rows.append(cell_objects)
                    
                    # êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ ìƒì„±
                    if rows:  # í—¤ë”ê°€ ì—†ì–´ë„ í–‰ì´ ìˆìœ¼ë©´ ì²˜ë¦¬
                        # í—¤ë”ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ê¸°ë³¸ê°’ ìƒì„±
                        if not headers:
                            # ì²« ë²ˆì§¸ í–‰ì˜ ì…€ ìˆ˜ë¥¼ í™•ì¸
                            first_row_cell_count = len(rows[0]) if rows else 0
                            # ê¸°ë³¸ í—¤ë” ìƒì„±
                            headers = [f"ì—´{i+1}" for i in range(first_row_cell_count)]
                            logger.info(f"[MCP] í‘œ {table_idx}: í—¤ë” ì—†ìŒ, ê¸°ë³¸ í—¤ë” ìƒì„± - {headers}")
                        
                        # ë¹ˆ í—¤ë” ì œê±° ë° ì‹¤ì œ í—¤ë”ë§Œ ì‚¬ìš©
                        actual_headers = [h for h in headers if h.strip()]
                        if not actual_headers:
                            # ëª¨ë“  í—¤ë”ê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
                            actual_headers = [f"ì—´{i+1}" for i in range(len(headers))]
                        
                        logger.info(f"[MCP] í‘œ {table_idx} ì‹¤ì œ í—¤ë”: {len(actual_headers)}ê°œ - {actual_headers}")
                        
                        table_lines = []
                        if table_title:
                            table_lines.append(f"\n[í‘œ: {table_title}]")
                        else:
                            table_lines.append(f"\n[í‘œ {table_idx}]")
                        
                        for row_idx, row in enumerate(rows):
                            # ë””ë²„ê¹…: ì›ë³¸ ì…€ ê°œìˆ˜ í™•ì¸
                            original_cell_count = len(row)
                            if original_cell_count != len(actual_headers):
                                logger.warning(f"[MCP] í‘œ {table_idx} í–‰ {row_idx+1}: ì…€ ê°œìˆ˜({original_cell_count}) != ì‹¤ì œ í—¤ë” ê°œìˆ˜({len(actual_headers)})")
                            
                            # ì‹¤ì œ í—¤ë” ìˆ˜ì— ë§ì¶° ì…€ ìˆ˜ ì¡°ì •
                            # ì…€ì´ ë¶€ì¡±í•˜ë©´ None ì¶”ê°€
                            while len(row) < len(actual_headers):
                                row.append(None)
                            
                            row_items = []
                            # ì‹¤ì œ í—¤ë”ì™€ ì…€ ë§¤í•‘
                            for col_idx, (header, cell_obj) in enumerate(zip(actual_headers, row)):
                                if cell_obj is None:
                                    logger.debug(f"[MCP] í‘œ {table_idx} í–‰ {row_idx+1} ì—´ {col_idx+1}({header}): None ì…€")
                                    continue
                                    
                                # ì…€ ë‚´ë¶€ì— ì¤‘ì²©ëœ í‘œê°€ ìˆëŠ”ì§€ í™•ì¸
                                nested_tables_in_cell = cell_obj.find_all('table')
                                if nested_tables_in_cell:
                                    # ì¤‘ì²©ëœ í‘œê°€ ìˆëŠ” ê²½ìš°
                                    logger.debug(f"[MCP] í‘œ {table_idx} í–‰ {row_idx+1} ì—´ {col_idx+1}({header}): {len(nested_tables_in_cell)}ê°œì˜ ì¤‘ì²©ëœ í‘œ í¬í•¨")
                                    
                                    # ì¤‘ì²©ëœ í‘œë¥¼ ì œì™¸í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                                    from bs4 import BeautifulSoup
                                    temp_cell = BeautifulSoup(str(cell_obj), 'html.parser')
                                    
                                    # ì¤‘ì²©ëœ í‘œë“¤ì„ ì²˜ë¦¬í•˜ê³  ì œê±°
                                    nested_results = []
                                    for nested_table in temp_cell.find_all('table'):
                                        # ì¤‘ì²©ëœ í‘œë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
                                        nested_result = _process_nested_table(nested_table, depth=1)
                                        if nested_result:
                                            nested_results.append(nested_result)
                                        nested_table.decompose()  # temp_cellì—ì„œ í‘œ ì œê±°
                                    
                                    # ì›ë³¸ cell_objì—ì„œë„ ì¤‘ì²©ëœ í‘œ ì œê±° (ì¤‘ë³µ ë°©ì§€)
                                    for nested_table in cell_obj.find_all('table'):
                                        nested_table.decompose()
                                    
                                    # í‘œë¥¼ ì œê±°í•œ í›„ ë‚¨ì€ í…ìŠ¤íŠ¸
                                    cell_text = temp_cell.get_text(separator=' ', strip=True)
                                    
                                    # ì…€ í…ìŠ¤íŠ¸ì™€ ì¤‘ì²©ëœ í‘œ ê²°ê³¼ ê²°í•©
                                    cell_parts = []
                                    if cell_text:
                                        cell_parts.append(cell_text)
                                    for nested_result in nested_results:
                                        cell_parts.append("\n    [ì¤‘ì²©ëœ í‘œ]\n" + "\n".join("    " + line for line in nested_result.split("\n")))
                                    
                                    if not cell_parts:
                                        cell_parts = []
                                else:
                                    # ì¤‘ì²©ëœ í‘œê°€ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
                                    # ë¨¼ì € ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë””ë²„ê¹… ë° í´ë°±)
                                    full_text = cell_obj.get_text(separator=' ', strip=True)
                                    
                                    # HTML êµ¬ì¡° ê¸°ë°˜ ë¶„í•  ì‹œë„
                                    try:
                                        cell_parts = _extract_cell_parts_by_html_structure(cell_obj)
                                    except Exception as e:
                                        logger.debug(f"[MCP] ì…€ ë¶„í•  ì‹¤íŒ¨: {e}")
                                        cell_parts = []
                                    
                                    # ë¶„í• ì´ ì‹¤íŒ¨í•˜ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
                                    if not cell_parts and full_text:
                                        cell_parts = [full_text]
                                        logger.debug(f"[MCP] í‘œ {table_idx} í–‰ {row_idx+1} ì—´ {col_idx+1}({header}): ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©")
                                
                                # ì…€ ë‚´ìš© ì¶”ê°€
                                for part in cell_parts:
                                    if part.strip():
                                        # ì¤‘ì²©ëœ í‘œëŠ” ë“¤ì—¬ì“°ê¸°ë¡œ êµ¬ë¶„
                                        if "[ì¤‘ì²©ëœ í‘œ]" in part:
                                            row_items.append(part)  # ì¤‘ì²©ëœ í‘œëŠ” ê·¸ëŒ€ë¡œ ì¶”ê°€
                                        else:
                                            row_items.append(f"{header}: {part.strip()}")
                            
                            if row_items:
                                # ê° í•­ëª©ì„ ê°œë³„ ì¤„ë¡œ ë¶„ë¦¬ (ì²­í‚¹ ê°œì„ )
                                for item in row_items:
                                    table_lines.append(f"- {item}")
                        
                        tables_text_list.append("\n".join(table_lines))
                        logger.info(f"[MCP] í‘œ {table_idx} ë³€í™˜ ì™„ë£Œ: {len(headers)}ê°œ ì—´, {len(rows)}ê°œ í–‰")
                    
                    # í˜„ì¬ í‘œë¥¼ ì²˜ë¦¬ëœ ëª©ë¡ì— ì¶”ê°€
                    processed_tables.add(id(table))
                
                except Exception as e:
                    logger.warning(f"[MCP] í‘œ {table_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            tables_text = "\n\n".join(tables_text_list)
            tables_count = table_idx  # ì‹¤ì œ ì²˜ë¦¬ëœ ìµœìƒìœ„ í‘œ ê°œìˆ˜
            
            # í‘œë¥¼ HTMLì—ì„œ ì œê±° (ì¤‘ë³µ ë°©ì§€)
            for table in soup.find_all('table'):
                table.decompose()
            logger.info(f"[MCP] í‘œ {tables_count}ê°œ ì¶”ì¶œ í›„ HTMLì—ì„œ ì œê±°")
        
        # 3. trafilaturaë¡œ ì£¼ìš” í…ìŠ¤íŠ¸ ì¶”ì¶œ (í‘œ ì œê±°ëœ HTML ì‚¬ìš©)
        main_text = ""
        if use_trafilatura:
            try:
                logger.info("[MCP] trafilaturaë¡œ ì£¼ìš” í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
                html_without_tables = str(soup)
                main_text = trafilatura.extract(
                    html_without_tables,
                    include_tables=False,
                    include_comments=False,
                    include_formatting=True,
                    include_links=False,  # ë§í¬ ì œì™¸
                    no_fallback=False,
                    favor_precision=False,
                    favor_recall=True,  # ë” ë§ì€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    output_format='txt'
                )
                if main_text:
                    logger.info(f"[MCP] trafilatura ì¶”ì¶œ ì™„ë£Œ: {len(main_text):,} characters")
                else:
                    logger.warning("[MCP] trafilatura ì¶”ì¶œ ì‹¤íŒ¨, BeautifulSoupìœ¼ë¡œ í´ë°±")
            except Exception as e:
                logger.warning(f"[MCP] trafilatura ì˜¤ë¥˜: {e}, BeautifulSoupìœ¼ë¡œ í´ë°±")
        
        # 4. ìµœì¢… Markdown ìƒì„±
        final_markdown = ""
        method = ""
        
        # ë©”íƒ€ë°ì´í„° ë¨¼ì € ì¶”ê°€
        if metadata_content:
            final_markdown = metadata_content + "\n---\n\n"
        
        # ë³¸ë¬¸ ì½˜í…ì¸  ì¶”ê°€
        if main_text:
            final_markdown += main_text
            method = "trafilatura + BeautifulSoup"
        else:
            logger.info("[MCP] BeautifulSoupìœ¼ë¡œ ì „ì²´ HTML ì²˜ë¦¬ ì¤‘...")
            
            # main-content ë˜ëŠ” wiki-content div ì°¾ê¸°
            main_content = soup.find('div', {'id': 'main-content'}) or soup.find('div', class_='wiki-content')
            if main_content:
                content_html = str(main_content)
            else:
                content_html = str(soup)
            
            final_markdown += md(
                content_html,
                heading_style="ATX",
                bullets="-",
                strip=['script', 'style']
            )
            method = "BeautifulSoup + markdownify"
        
        # í‘œ ì¶”ê°€
        if tables_text:
            final_markdown += "\n\n---\n\n## ì¶”ì¶œëœ í‘œ ë°ì´í„°\n\n" + tables_text
        
        # 5. ì •ë¦¬ ë° í¬ë§·íŒ… (ì—°ì†ëœ ë¹ˆ ì¤„ ì œê±°)
        lines = final_markdown.split('\n')
        cleaned_lines = []
        prev_empty = False
        for line in lines:
            is_empty = not line.strip()
            if is_empty and prev_empty:
                continue
            cleaned_lines.append(line)
            prev_empty = is_empty
        
        final_markdown = '\n'.join(cleaned_lines).strip()
        
        # RAG ìµœì í™” ì ìš©
        optimized_markdown, rag_config = _optimize_markdown_for_rag(final_markdown)
        
        # RAG ì„¤ì •ì„ ê¸°ì¡´ ì„¤ì •ê³¼ ë³‘í•©
        # ì‚¬ìš©ìê°€ ì œê³µí•œ separatorsê°€ ë‹¨ì¼ ë¬¸ìì—´ì¸ ê²½ìš°ë¥¼ ê³ ë ¤
        if rag_config['strategy'] == 'table_aware':
            # í‘œ ì¤‘ì‹¬ ë¬¸ì„œëŠ” ë” í° ì²­í¬ ì‚¬ìš©
            rag_config['primary_separator'] = "\n---\n"  # í‘œ êµ¬ë¶„ì
            rag_config['fallback_separator'] = "\n- "     # ë¦¬ìŠ¤íŠ¸ í•­ëª©
        elif rag_config['strategy'] == 'sentence_aware':
            # ê¸´ í…ìŠ¤íŠ¸ëŠ” ë¬¸ì¥ ë‹¨ìœ„ ë¶„í• 
            rag_config['primary_separator'] = ". "
            rag_config['fallback_separator'] = "\n"
        else:
            # ê· í˜•ì¡íŒ ì ‘ê·¼
            rag_config['primary_separator'] = "\n- "
            rag_config['fallback_separator'] = "\n\n"
        
        result = {
            'success': True,
            'result': {
                'markdown': optimized_markdown,  # ìµœì í™”ëœ ë§ˆí¬ë‹¤ìš´ ì‚¬ìš©
                'stats': {
                    'original_size': len(html_content),
                    'markdown_size': len(optimized_markdown),
                    'original_markdown_size': len(final_markdown),
                    'tables_found': tables_count,
                    'method': method,
                    'optimization_applied': True
                },
                'rag_config': rag_config,  # RAG ì¶”ì²œ ì„¤ì • ì¶”ê°€
                'converted_at': datetime.now().isoformat()
            }
        }
        
        logger.info(f"[MCP] ari_html_to_markdown ì™„ë£Œ - ë§ˆí¬ë‹¤ìš´ í¬ê¸°: {len(final_markdown)} chars, í‘œ {tables_count}ê°œ")
        return result
        
    except Exception as e:
        logger.error(f"[MCP] Enhanced HTML to Markdown ë³€í™˜ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}




@mcp.tool
def ari_extract_navigation(html_content: str) -> Dict[str, Any]:
    """
    HTMLì—ì„œ Confluence í˜ì´ì§€ íŠ¸ë¦¬ ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ë¥¼ ì¶”ì¶œí•˜ëŠ” ë„êµ¬
    
    - í˜„ì¬ í˜ì´ì§€ ID ë° ë¶€ëª¨ í˜ì´ì§€ ID ì¶”ì¶œ
    - í˜ì´ì§€ íŠ¸ë¦¬ êµ¬ì¡° íŒŒì‹± (ìµœìƒìœ„ í˜ì´ì§€ ë° ì „ì²´ ê³„ì¸µ êµ¬ì¡°)
    - ê° í˜ì´ì§€ì˜ ì œëª©, URL, ë ˆë²¨, í•˜ìœ„ í˜ì´ì§€ ì¡´ì¬ ì—¬ë¶€ ë“± ë©”íƒ€ë°ì´í„° í¬í•¨
    
    Args:
        html_content: HTML ë³¸ë¬¸ ë¬¸ìì—´
    
    Returns:
        ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ ì •ë³´ ë”•ì…”ë„ˆë¦¬ (success, result í¬í•¨)
    """
    logger.info(f"[MCP] ari_extract_navigation í˜¸ì¶œë¨ - HTML í¬ê¸°: {len(html_content)} chars")
    
    try:
        from bs4 import BeautifulSoup
        import re
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # í˜„ì¬ í˜ì´ì§€ ID ì¶”ì¶œ (meta íƒœê·¸ì—ì„œ)
        current_page_id = None
        page_id_meta = soup.find('meta', {'name': 'ajs-page-id'})
        if page_id_meta and page_id_meta.get('content'):
            current_page_id = page_id_meta.get('content')
        
        # ë¶€ëª¨ í˜ì´ì§€ ID ì¶”ì¶œ
        parent_page_id = None
        parent_id_meta = soup.find('meta', {'name': 'ajs-parent-page-id'})
        if parent_id_meta and parent_id_meta.get('content'):
            parent_page_id = parent_id_meta.get('content')
        
        logger.info(f"[MCP] í˜ì´ì§€ ì •ë³´: current_page_id={current_page_id}, parent_page_id={parent_page_id}")
        
        # í˜ì´ì§€ íŠ¸ë¦¬ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
        page_tree_container = soup.find('div', class_='plugin_pagetree_children_list')
        
        if not page_tree_container:
            logger.warning("[MCP] í˜ì´ì§€ íŠ¸ë¦¬ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {
                'success': True,
                'result': {
                    'current_page_id': current_page_id,
                    'parent_page_id': parent_page_id,
                    'root_pages': [],
                    'all_pages': []
                }
            }
        
        # ìµœìƒìœ„ ul íƒœê·¸ ì°¾ê¸°
        root_ul = page_tree_container.find('ul', class_='plugin_pagetree_children_list')
        
        all_pages = []
        root_pages = []
        
        if root_ul:
            # ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  í˜ì´ì§€ ì•„ì´í…œ ì¶”ì¶œ
            all_pages = _extract_page_items_recursive(root_ul, level=0)
            
            # ìµœìƒìœ„ í˜ì´ì§€ë§Œ í•„í„°ë§ (level 0)
            root_pages = [page for page in all_pages if page['level'] == 0]
        
        logger.info(f"[MCP] ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ ì¶”ì¶œ ì™„ë£Œ: ì´ {len(all_pages)}ê°œ í˜ì´ì§€, ìµœìƒìœ„ {len(root_pages)}ê°œ")
        
        return {
            'success': True,
            'result': {
                'current_page_id': current_page_id,
                'parent_page_id': parent_page_id,
                'root_pages': root_pages,
                'all_pages': all_pages
            }
        }
        
    except Exception as e:
        logger.error(f"[MCP] ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}


def _extract_page_items_recursive(ul_element, level: int = 0) -> list:
    """
    ì¬ê·€ì ìœ¼ë¡œ í˜ì´ì§€ íŠ¸ë¦¬ì—ì„œ ëª¨ë“  ì•„ì´í…œì„ ì¶”ì¶œ
    
    Args:
        ul_element: BeautifulSoup ul ì—˜ë¦¬ë¨¼íŠ¸
        level: í˜„ì¬ ê³„ì¸µ ê¹Šì´
        
    Returns:
        í˜ì´ì§€ ì•„ì´í…œ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    import re
    items = []
    
    # ì§ì ‘ ìì‹ li íƒœê·¸ë§Œ ì²˜ë¦¬
    for li in ul_element.find_all('li', recursive=False):
        try:
            # í˜ì´ì§€ ID ì¶”ì¶œ
            toggle_link = li.find('a', class_='plugin_pagetree_childtoggle')
            page_id = None
            is_expanded = False
            
            if toggle_link:
                page_id = toggle_link.get('data-page-id')
                aria_expanded = toggle_link.get('aria-expanded', 'false')
                is_expanded = aria_expanded == 'true'
            
            # í˜ì´ì§€ ì œëª©ê³¼ URL ì¶”ì¶œ
            content_span = li.find('span', class_='plugin_pagetree_children_span')
            title = None
            url = None
            
            if content_span:
                link = content_span.find('a')
                if link:
                    title = link.get_text(strip=True)
                    url = link.get('href')
                    
                    # pageIdê°€ ì—†ìœ¼ë©´ URLì—ì„œ ì¶”ì¶œ ì‹œë„
                    if not page_id and url:
                        match = re.search(r'pageId=(\d+)', url)
                        if match:
                            page_id = match.group(1)
            
            # í•˜ìœ„ í˜ì´ì§€ ì»¨í…Œì´ë„ˆ í™•ì¸
            children_container = li.find('div', class_='plugin_pagetree_children_container')
            has_children = False
            
            if children_container:
                child_ul = children_container.find('ul', class_='plugin_pagetree_children_list', recursive=False)
                has_children = child_ul is not None and len(child_ul.find_all('li', recursive=False)) > 0
            
            # í˜ì´ì§€ ì•„ì´í…œ ìƒì„±
            if page_id and title:
                item = {
                    'page_id': page_id,
                    'title': title,
                    'url': url,
                    'level': level,
                    'has_children': has_children,
                    'is_expanded': is_expanded
                }
                items.append(item)
                
                # í•˜ìœ„ í˜ì´ì§€ê°€ ìˆê³  í¼ì³ì ¸ ìˆìœ¼ë©´ ì¬ê·€ í˜¸ì¶œ
                if has_children and children_container:
                    child_ul = children_container.find('ul', class_='plugin_pagetree_children_list', recursive=False)
                    if child_ul:
                        child_items = _extract_page_items_recursive(child_ul, level + 1)
                        items.extend(child_items)
        
        except Exception as e:
            logger.warning(f"[MCP] í˜ì´ì§€ ì•„ì´í…œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ (level {level}): {e}")
            continue
    
    return items


async def main():
    # Start ARI Processing MCP server
    logger.info("ğŸš€ ARI Processing MCP Server ì‹œì‘ ì¤‘...")
    logger.info("ğŸ“ ì„œë²„ ì£¼ì†Œ: http://0.0.0.0:4200/my-custom-path")
    logger.info("ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: health_check, ari_html_to_markdown, ari_extract_navigation")
    
    await mcp.run_async(
        transport="http",
        host="0.0.0.0",
        port=4200,
        path="/my-custom-path",
        log_level="info",
    )

if __name__ == "__main__":
    asyncio.run(main())