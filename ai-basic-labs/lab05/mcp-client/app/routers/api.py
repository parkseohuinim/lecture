"""API routes for ARI Processing"""
from fastapi import APIRouter, HTTPException, UploadFile, File, Form, BackgroundTasks, Response, WebSocket, WebSocketDisconnect
from fastapi.responses import FileResponse, StreamingResponse
from typing import List, Optional, AsyncGenerator
import tempfile
import os
import logging
from datetime import datetime
import json
import yaml
import asyncio

from app.models import (
    HealthResponse, RagConfig, ProcessingMetadata, DocumentAnalysis, 
    NavigationMenu, NavigationItem,
    RAGUploadResponse, RAGQueryRequest, RAGQueryResponse, 
    RAGDocumentInfo, RAGStatsResponse, RAGSourceInfo
)
from app.infrastructure.mcp.mcp_service import mcp_service
from app.infrastructure.llm.llm_service import llm_service
from app.application.conference.service import conference_service
from app.application.rag.rag_service import get_rag_service

logger = logging.getLogger(__name__)

# Create API router
router = APIRouter()

@router.get("/", tags=["root"])
async def root():
    """Root endpoint"""
    return {"message": "ARI Processing Server is running"}

@router.get("/health", response_model=HealthResponse, tags=["health"])
async def health_check():
    """Health check endpoint"""
    try:
        health_data = await mcp_service.health_check()
        
        return HealthResponse(
            status="healthy" if health_data["connected"] else "unhealthy",
            mcp_connected=health_data["connected"],
            tools_available=health_data["tools_available"],
            details=health_data
        )
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return HealthResponse(
            status="error",
            mcp_connected=False,
            tools_available=0,
            details={"error": str(e)}
        )



def analyze_markdown_structure(markdown_content: str) -> dict:
    """
    ë§ˆí¬ë‹¤ìš´ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ RAG ì„¤ì •ì„ ì¶”ì²œ
    
    Returns:
        dict: {
            'separators': List[str],
            'chunk_size': int,
            'chunk_overlap': int,
            'analysis': dict  # ë¶„ì„ ìƒì„¸ ì •ë³´
        }
    """
    lines = markdown_content.split('\n')
    total_length = len(markdown_content)
    
    # íŒ¨í„´ ë¶„ì„
    has_headers = any(line.strip().startswith('#') for line in lines)
    has_horizontal_rules = any(line.strip() == '---' for line in lines)
    has_lists = any(line.strip().startswith(('-', '*', '+')) for line in lines)
    
    # í‘œ ê°ì§€: ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹ ë˜ëŠ” "[í‘œ" íŒ¨í„´
    has_markdown_tables = '|' in markdown_content and any('---' in line and '|' in line for line in lines)
    has_list_tables = any('[í‘œ' in line or '[Table' in line.lower() for line in lines)
    has_tables = has_markdown_tables or has_list_tables
    
    # ë‹¨ë½ êµ¬ë¶„ ë¶„ì„
    empty_line_count = sum(1 for line in lines if not line.strip())
    double_newline_count = markdown_content.count('\n\n')
    
    # í‰ê·  ë‹¨ë½ ê¸¸ì´ ê³„ì‚°
    paragraphs = [p.strip() for p in markdown_content.split('\n\n') if p.strip()]
    avg_paragraph_length = sum(len(p) for p in paragraphs) / len(paragraphs) if paragraphs else 0
    
    # Separator ìš°ì„ ìˆœìœ„ ê²°ì •
    separators = []
    
    # 1. í—¤ë” ê¸°ë°˜ ë¶„í•  (ê°€ì¥ í° ë‹¨ìœ„)
    if has_headers:
        # í—¤ë” ë ˆë²¨ë³„ë¡œ ë¶„í• 
        separators.extend(["\n### ", "\n## ", "\n# "])
    
    # 2. ìˆ˜í‰ì„  ê¸°ë°˜ ë¶„í• 
    if has_horizontal_rules:
        separators.append("\n---\n")
    
    # 3. ì´ì¤‘ ê°œí–‰ (ë‹¨ë½ êµ¬ë¶„) - í•­ìƒ í¬í•¨
    separators.append("\n\n")
    
    # 4. ë‹¨ì¼ ê°œí–‰ - í•­ìƒ í¬í•¨
    separators.append("\n")
    
    # 5. ë¬¸ì¥ ë‹¨ìœ„ ë¶„í•  (í‘œ ë°ì´í„° ë“± ê¸´ ì¤„ ëŒ€ì‘)
    # ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ ë’¤ ê³µë°±ìœ¼ë¡œ ë¬¸ì¥ êµ¬ë¶„
    separators.extend([". ", "? ", "! "])
    
    # 6. ì‰¼í‘œ/ì„¸ë¯¸ì½œë¡  ë‹¨ìœ„ ë¶„í•  (ë” ì„¸ë°€í•œ ë¶„í• )
    separators.extend([", ", "; ", ": "])
    
    # 7. ê³µë°±ì€ ì¡°ê±´ë¶€ ì¶”ê°€ (í‘œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ)
    if has_tables:
        separators.append(" ")
    
    # 8. ë¹ˆ ë¬¸ìì—´ì€ ì œê±° (ê³¼ë„í•œ ë¶„í•  ë°©ì§€)
    # separators.append("")
    
    # Chunk Size ê²°ì •
    # í‘œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë” í° ì²­í¬ ì‚¬ìš©
    if has_tables:
        # í‘œê°€ ìˆìœ¼ë©´ í° ì²­í¬ ì‚¬ìš© (í‘œê°€ ì˜ë¦¬ì§€ ì•Šë„ë¡)
        chunk_size = 3000
    elif avg_paragraph_length > 0:
        # ë‹¨ë½ì´ ì§§ìœ¼ë©´ ì—¬ëŸ¬ ë‹¨ë½ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
        if avg_paragraph_length < 300:
            chunk_size = 2000
        elif avg_paragraph_length < 600:
            chunk_size = 1500
        else:
            chunk_size = 1200
    else:
        chunk_size = 1500  # ê¸°ë³¸ê°’
    
    # Chunk Overlap ê²°ì • (chunk_sizeì˜ 15-20%, ìµœëŒ€ 500)
    chunk_overlap = min(500, int(chunk_size * 0.2))
    
    analysis = {
        'total_length': total_length,
        'total_lines': len(lines),
        'has_headers': has_headers,
        'has_horizontal_rules': has_horizontal_rules,
        'has_lists': has_lists,
        'has_tables': has_tables,
        'paragraph_count': len(paragraphs),
        'avg_paragraph_length': int(avg_paragraph_length),
        'empty_line_count': empty_line_count,
        'double_newline_count': double_newline_count
    }
    
    return {
        'separators': separators,
        'chunk_size': chunk_size,
        'chunk_overlap': chunk_overlap,
        'analysis': analysis
    }


@router.post("/chat", tags=["chat"])
async def chat_endpoint(
    background_tasks: BackgroundTasks,
    question: str = Form(..., description="ì‚¬ìš©ì ë©”ì‹œì§€ ë˜ëŠ” ì§ˆë¬¸"),
    files: List[UploadFile] = File(default=[], description="HTML íŒŒì¼ë“¤ (ì„ íƒì‚¬í•­)")
):
    """
    í†µí•© ì±„íŒ… API - ì¼ë°˜ ëŒ€í™” + HTML íŒŒì¼ ë¶„ì„
    
    **ë™ì‘ ë°©ì‹:**
    
    1. **ì¼ë°˜ ëŒ€í™”** (íŒŒì¼ ì—†ìŒ):
       - ì§ˆë¬¸: "ì•ˆë…•í•˜ì„¸ìš”"
       - ì‘ë‹µ: AI ì¸ì‚¬ (ë„êµ¬ í˜¸ì¶œ ì—†ìŒ)
    
    2. **HTML íŒŒì¼ ë¶„ì„** (íŒŒì¼ ìˆìŒ + ì²˜ë¦¬ í‚¤ì›Œë“œ):
       - ì§ˆë¬¸: "ì´ HTML ë‚´ìš©ì„ ì¶”ì¶œí•´ì¤˜"
       - ì‘ë‹µ: Markdown ê²°ê³¼ + **Frontmatter íŒŒì¼ ìë™ ìƒì„±**
       - download_url í•„ë“œì— ë‹¤ìš´ë¡œë“œ ë§í¬ í¬í•¨
    
    3. **ì¼ë°˜ ì§ˆë¬¸** (íŒŒì¼ ìˆìŒ + ì¼ë°˜ í‚¤ì›Œë“œ):
       - ì§ˆë¬¸: "HTMLì´ë€?"
       - ì‘ë‹µ: AI ì„¤ëª… (ë„êµ¬ í˜¸ì¶œ ì—†ìŒ)
    
    **ì‘ë‹µ í˜•ì‹:**
    ```markdown
    ---json
    {
      "rag_config": {
        "separators": ["\n\n", "\n", " ", ""],
        "chunk_size": 1000,
        "chunk_overlap": 200,
        "document_type": "confluence_page"
      },
      "metadata": {
        "processed_at": "2025-10-29T...",
        "html_size": 123456,
        "markdown_size": 45678,
        "tools_used": ["ari_html_to_markdown"]
      },
      "navigation_menu": {
        "current_page_id": "180192188",
        "parent_page_id": "180192092",
        ...
      }
    }
    ---
    
    <!-- RAG_CONTENT_START -->
    
    # ì‹¤ì œ ë§ˆí¬ë‹¤ìš´ ë‚´ìš©
    ...
    ```
    
    **êµ¬ì¡° ì„¤ëª…:**
    - `---json ... ---`: ë©”íƒ€ë°ì´í„° ì˜ì—­ (RAG ì„¤ì •, ì²˜ë¦¬ ì •ë³´, ë„¤ë¹„ê²Œì´ì…˜ ë“±)
    - `<!-- RAG_CONTENT_START -->`: RAG ì¸ë±ì‹± ì‹œì‘ ì§€ì  ë§ˆì»¤
    - ë§ˆì»¤ ì´í›„: ì‹¤ì œ RAG ì¸ë±ì‹± ëŒ€ìƒ ì½˜í…ì¸ 
    
    **RAG íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ë°©ë²•:**
    ```python
    # íŒŒì¼ ì½ê¸°
    with open('content.md', 'r') as f:
        content = f.read()
    
    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    if '---json' in content:
        _, frontmatter, rest = content.split('---', 2)
        metadata = json.loads(frontmatter.replace('json', '', 1))
    
    # RAG ì½˜í…ì¸ ë§Œ ì¶”ì¶œ
    if '<!-- RAG_CONTENT_START -->' in rest:
        rag_content = rest.split('<!-- RAG_CONTENT_START -->', 1)[1].strip()
    else:
        rag_content = rest.strip()
    
    # rag_contentë§Œ ì„ë² ë”© ë° ì¸ë±ì‹±
    ```
    """
    try:
        if not question.strip():
            raise HTTPException(status_code=400, detail="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
        
        # 1. HTML íŒŒì¼ ì½ê¸° (ì„ íƒì‚¬í•­)
        html_content = None
        content_length = 0
        file_info = []
        
        if files:
            html_files = [f for f in files if f.filename and f.filename.endswith('.html')]
            
            if html_files:
                logger.info(f"ğŸ“ {len(html_files)}ê°œ HTML íŒŒì¼ ê°ì§€")
                
                # ì²« ë²ˆì§¸ HTML íŒŒì¼ë§Œ ì²˜ë¦¬
                first_file = html_files[0]
                try:
                    content = await first_file.read()
                    html_content = content.decode('utf-8', errors='ignore')
                    content_length = len(html_content)
                    file_info.append({
                        "filename": first_file.filename,
                        "size": content_length
                    })
                    logger.info(f"ğŸ“„ HTML íŒŒì¼: {first_file.filename} ({content_length:,} bytes)")
                except Exception as e:
                    logger.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
                    raise HTTPException(status_code=400, detail=f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
        
        # 2. MCP ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        available_tools = mcp_service.available_tools
        
        if not available_tools:
            raise HTTPException(status_code=503, detail="ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # 3. LLM ì˜ë„ ë¶„ì„ ë° ì²˜ë¦¬
        if html_content:
            # HTML íŒŒì¼ì´ ìˆëŠ” ê²½ìš°
            logger.info("ğŸ¤– HTML íŒŒì¼ê³¼ í•¨ê»˜ ì§ˆë¬¸ ì²˜ë¦¬")
            try:
                answer, tools_used = await llm_service.query_with_raw_result_and_html(
                    question=question,
                    available_tools=available_tools,
                    html_content=html_content
                )
            except Exception as llm_error:
                logger.error(f"âŒ LLM ì²˜ë¦¬ ì‹¤íŒ¨: {llm_error}")
                raise HTTPException(
                    status_code=503,
                    detail=f"LLM ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {str(llm_error)}. API í‚¤ê°€ ìœ íš¨í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
                )
        else:
            # ì¼ë°˜ ëŒ€í™” (íŒŒì¼ ì—†ìŒ)
            logger.info("ğŸ’¬ ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬")
            answer = await llm_service.generate_response(question)
            tools_used = []
        
        logger.info(f"âœ… LLM ì²˜ë¦¬ ì™„ë£Œ: {len(answer):,} characters")
        logger.info(f"ğŸ”§ ì‚¬ìš©ëœ ë„êµ¬: {', '.join(tools_used) if tools_used else 'ì—†ìŒ'}")
        
        # HTML ì²˜ë¦¬ ì˜ë„ê°€ ì•„ë‹ˆë©´ (ì¼ë°˜ ì§ˆë¬¸ì´ë©´) JSON ì‘ë‹µë§Œ
        if not tools_used or 'ari_html_to_markdown' not in tools_used:
            logger.info("ğŸ’¬ ì¼ë°˜ ì§ˆë¬¸ - JSON ì‘ë‹µë§Œ")
            return {
                "success": True,
                "answer": answer,
                "tools_used": [],
                "has_markdown": False,
                "file_info": file_info
            }
        
        # HTML ì²˜ë¦¬ ì˜ë„ì¼ ê²½ìš° - Frontmatter íŒŒì¼ ìƒì„± + JSON ì‘ë‹µ
        # 4. ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë¶„ì„ ë° ìµœì  RAG ì„¤ì • ìë™ ì¶”ì²œ
        logger.info("ğŸ” ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë¶„ì„ ì¤‘...")
        rag_analysis = analyze_markdown_structure(answer)
        
        logger.info(f"ğŸ“Š [Frontmatter] ë¶„ì„ ê²°ê³¼:")
        logger.info(f"   - ì´ ê¸¸ì´: {rag_analysis['analysis']['total_length']:,} characters")
        logger.info(f"   - ë‹¨ë½ ìˆ˜: {rag_analysis['analysis']['paragraph_count']}")
        logger.info(f"   - í‰ê·  ë‹¨ë½ ê¸¸ì´: {rag_analysis['analysis']['avg_paragraph_length']} characters")
        logger.info(f"   - í—¤ë” ì¡´ì¬: {rag_analysis['analysis']['has_headers']}")
        logger.info(f"   - í…Œì´ë¸” ì¡´ì¬: {rag_analysis['analysis']['has_tables']}")
        logger.info(f"   - ê¶Œì¥ ì²­í¬ í¬ê¸°: {rag_analysis['chunk_size']}")
        logger.info(f"   - ê¶Œì¥ ì²­í¬ ì¤‘ë³µ: {rag_analysis['chunk_overlap']}")
        logger.info(f"   - ê¶Œì¥ Separators: {rag_analysis['separators'][:3]}...")
        
        # 5. ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ ì¶”ì¶œ (HTML íŒŒì¼ì´ ìˆì„ ë•Œë§Œ)
        navigation_menu = None
        
        if html_content:
            logger.info("ğŸ—‚ï¸ ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ ì¶”ì¶œ ì¤‘...")
            
            try:
                nav_result = await mcp_service.call_tool(
                    tool_name="ari_extract_navigation",
                    arguments={"html_content": html_content}
                )
                
                # MCP ê²°ê³¼ íŒŒì‹±
                if hasattr(nav_result, 'content') and nav_result.content:
                    nav_json = json.loads(nav_result.content[0].text)
                    if nav_json.get('success') and 'result' in nav_json:
                        nav_data = nav_json['result']
                        
                        # NavigationItem ë¦¬ìŠ¤íŠ¸ ìƒì„±
                        root_pages = [NavigationItem(**page) for page in nav_data.get('root_pages', [])]
                        all_pages = [NavigationItem(**page) for page in nav_data.get('all_pages', [])]
                        
                        navigation_menu = NavigationMenu(
                            current_page_id=nav_data.get('current_page_id'),
                            parent_page_id=nav_data.get('parent_page_id'),
                            root_pages=root_pages,
                            all_pages=all_pages
                        )
                        
                        logger.info(f"   - í˜„ì¬ í˜ì´ì§€ ID: {navigation_menu.current_page_id}")
                        logger.info(f"   - ë¶€ëª¨ í˜ì´ì§€ ID: {navigation_menu.parent_page_id}")
                        logger.info(f"   - ìµœìƒìœ„ í˜ì´ì§€ ìˆ˜: {len(navigation_menu.root_pages)}")
                        logger.info(f"   - ì „ì²´ í˜ì´ì§€ ìˆ˜: {len(navigation_menu.all_pages)}")
            except Exception as e:
                logger.warning(f"   - ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # 6. RAG ì„¤ì • ìƒì„± (ì„œë²„ì—ì„œ ì¶”ì²œí•œ ì„¤ì • ì‚¬ìš©)
        # MCP ì„œë²„ê°€ ìµœì í™”ëœ ì„¤ì •ì„ ì œê³µí•˜ëŠ”ì§€ í™•ì¸
        server_rag_config = None
        if tools_used and 'ari_html_to_markdown' in tools_used:
            # MCP ê²°ê³¼ì—ì„œ RAG ì„¤ì • ì¶”ì¶œ ì‹œë„
            try:
                # answerê°€ JSON í˜•ì‹ì¸ ê²½ìš° íŒŒì‹±
                if '{"success":' in answer:
                    import re
                    json_match = re.search(r'\{"success":.*\}', answer, re.DOTALL)
                    if json_match:
                        result_json = json.loads(json_match.group())
                        if result_json.get('success') and 'result' in result_json:
                            server_rag_config = result_json['result'].get('rag_config')
            except:
                pass
        
        # ì„œë²„ ì¶”ì²œ ì„¤ì •ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¶„ì„ ê¸°ë°˜ ì„¤ì • ì‚¬ìš©
        if server_rag_config:
            # ì„œë²„ê°€ ì¶”ì²œí•œ ë‹¨ì¼ separator ì‚¬ìš©
            primary_sep = server_rag_config.get('primary_separator', '\n- ')
            fallback_sep = server_rag_config.get('fallback_separator', '\n\n')
            
            # ë‹¨ì¼ separatorë§Œ ì§€ì›í•˜ëŠ” ì‹œìŠ¤í…œì„ ìœ„í•œ ì„¤ì •
            rag_config = RagConfig(
                separators=[primary_sep],  # ë‹¨ì¼ separator
                chunk_size=server_rag_config.get('chunk_size', 2000),
                chunk_overlap=server_rag_config.get('chunk_overlap', 400),
                document_type="confluence_page",
                strategy=server_rag_config.get('strategy', 'balanced')
            )
            
            logger.info(f"ğŸ¯ [Frontmatter] ì„œë²„ ì¶”ì²œ RAG ì„¤ì • ì‚¬ìš©:")
            logger.info(f"   - Primary Separator: {repr(primary_sep)}")
            logger.info(f"   - Chunk Size: {rag_config.chunk_size}")
            logger.info(f"   - Chunk Overlap: {rag_config.chunk_overlap}")
            logger.info(f"   - Strategy: {rag_config.strategy}")
        else:
            # ê¸°ë³¸ ë¶„ì„ ê¸°ë°˜ ì„¤ì • (ë‹¨ìˆœí™”)
            # ë¦¬ìŠ¤íŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            has_lists = rag_analysis['analysis'].get('has_lists', False)
            
            rag_config = RagConfig(
                separators=['\n- '] if has_lists else ['\n\n'],  # ë‹¨ì¼ separator
                chunk_size=rag_analysis['chunk_size'],
                chunk_overlap=rag_analysis['chunk_overlap'],
            document_type="confluence_page"
        )
        
        metadata = ProcessingMetadata(
            processed_at=datetime.now().isoformat(),
            html_size=content_length,
            markdown_size=len(answer),
            tools_used=tools_used if tools_used else [],
            document_analysis=DocumentAnalysis(**rag_analysis['analysis'])
        )
        
        # 7. í”„ë¡ íŠ¸ë§¤í„° ìƒì„± (JSON í˜•ì‹ - YAMLë³´ë‹¤ ì•ˆì •ì )
        frontmatter_data = {
            "rag_config": rag_config.model_dump(),
            "metadata": metadata.model_dump()
        }
        
        # ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if navigation_menu:
            frontmatter_data["navigation_menu"] = navigation_menu.model_dump()
        
        # JSONìœ¼ë¡œ ì§ë ¬í™” (ê°œí–‰ ë¬¸ì ë“±ì´ ì´ìŠ¤ì¼€ì´í”„ë¨)
        frontmatter_json = json.dumps(
            frontmatter_data,
            ensure_ascii=False,
            indent=2
        )
        
        # 7. í”„ë¡ íŠ¸ë§¤í„° + ë§ˆí¬ë‹¤ìš´ ê²°í•© (RAG ì½˜í…ì¸  ì‹œì‘ ë§ˆì»¤ ì¶”ê°€)
        final_content = f"""---json
{frontmatter_json}
---

<!-- RAG_CONTENT_START -->

{answer}
"""
        
        # 8. Markdown íŒŒì¼ ì €ì¥ (ì •ì  íŒŒì¼ë¡œ ì €ì¥)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"content_frontmatter_{timestamp}.md"
        
        # ì •ì  íŒŒì¼ ë””ë ‰í† ë¦¬ ìƒì„±
        static_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'static', 'downloads')
        os.makedirs(static_dir, exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        file_path = os.path.join(static_dir, filename)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(final_content)
        
        logger.info(f"ğŸ’¾ Frontmatter íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename} ({len(final_content):,} bytes)")
        logger.info(f"ğŸ¯ RAG ì½˜í…ì¸  êµ¬ë¶„ ë§ˆì»¤ ì¶”ê°€ë¨: <!-- RAG_CONTENT_START -->")
        
        # 9. ë‹¤ìš´ë¡œë“œ URL ìƒì„±
        download_url = f"/downloads/{filename}"
        
        # 10. íŒŒì¼ ìë™ ì‚­ì œ ì„¤ì • (1ì‹œê°„ í›„)
        def cleanup_file():
            try:
                import time
                time.sleep(3600)  # 1ì‹œê°„ í›„
                if os.path.exists(file_path):
                    os.unlink(file_path)
                    logger.info(f"ğŸ—‘ï¸ íŒŒì¼ ìë™ ì‚­ì œ ì™„ë£Œ: {filename}")
            except Exception as e:
                logger.warning(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        background_tasks.add_task(cleanup_file)
        
        # 11. JSON ì‘ë‹µ (ì±„íŒ… + ë‹¤ìš´ë¡œë“œ URL)
        return {
            "success": True,
            "answer": answer,
            "tools_used": tools_used,
            "has_markdown": True,
            "file_info": file_info,
            "frontmatter_file": {
                "filename": filename,
                "download_url": download_url,
                "size": len(final_content),
                "rag_config": rag_config.model_dump()
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "success": False,
            "answer": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "tools_used": [],
            "has_markdown": False
        }


# ============================================================================
# Multi-Agent Conference Endpoints
# ============================================================================

@router.get("/conference/patterns", tags=["conference"])
async def get_conference_patterns():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ë©€í‹° ì—ì´ì „íŠ¸ íŒ¨í„´ ëª©ë¡ ì¡°íšŒ
    
    Returns:
        List[Dict]: íŒ¨í„´ ëª©ë¡
    """
    return {
        "success": True,
        "patterns": conference_service.get_available_patterns()
    }


@router.websocket("/ws/conference")
async def conference_websocket(websocket: WebSocket):
    """
    ë©€í‹° ì—ì´ì „íŠ¸ íšŒì˜ WebSocket ì—”ë“œí¬ì¸íŠ¸ (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°)
    
    **ì—°ê²° íë¦„:**
    1. í´ë¼ì´ì–¸íŠ¸ê°€ WebSocket ì—°ê²°
    2. í´ë¼ì´ì–¸íŠ¸ê°€ íšŒì˜ ì„¤ì • ì „ì†¡:
       ```json
       {
         "pattern": "sequential",
         "topic": "AI ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ",
         "max_rounds": 3,
         "num_agents": 5
       }
       ```
    3. ì„œë²„ê°€ ì‹¤ì‹œê°„ìœ¼ë¡œ ì—ì´ì „íŠ¸ ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë°:
       ```json
       {
         "type": "agent_message",
         "node": "summarizer",
         "content": "ìš”ì•½ ë‚´ìš©...",
         "status": "completed"
       }
       ```
    4. ì™„ë£Œ ì‹œ:
       ```json
       {
         "type": "conference_complete",
         "pattern": "sequential",
         "status": "completed"
       }
       ```
    """
    await websocket.accept()
    logger.info("ğŸ”Œ WebSocket ì—°ê²°ë¨")
    
    try:
        # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° íšŒì˜ ì„¤ì • ë°›ê¸°
        data = await websocket.receive_json()
        
        pattern = data.get("pattern")
        topic = data.get("topic")
        
        if not pattern or not topic:
            await websocket.send_json({
                "type": "error",
                "error": "patternê³¼ topicì€ í•„ìˆ˜ì…ë‹ˆë‹¤",
                "status": "error"
            })
            await websocket.close()
            return
        
        logger.info(f"ğŸ¯ íšŒì˜ ì‹œì‘: pattern={pattern}, topic={topic}")
        
        # íŒ¨í„´ë³„ ì¶”ê°€ ì˜µì…˜
        kwargs = {}
        if pattern == "debate":
            kwargs["max_rounds"] = data.get("max_rounds", 3)
        elif pattern == "swarm":
            kwargs["num_agents"] = data.get("num_agents", 5)
        
        # íšŒì˜ ì‹¤í–‰ (WebSocket ìŠ¤íŠ¸ë¦¬ë°)
        result = await conference_service.run_conference(
            pattern=pattern,
            topic=topic,
            websocket=websocket,
            **kwargs
        )
        
        logger.info(f"âœ… íšŒì˜ ì™„ë£Œ: pattern={pattern}")
    
    except WebSocketDisconnect:
        logger.info("ğŸ”Œ WebSocket ì—°ê²° ëŠê¹€")
    
    except Exception as e:
        logger.error(f"âŒ íšŒì˜ ì˜¤ë¥˜: {e}", exc_info=True)
        
        try:
            await websocket.send_json({
                "type": "error",
                "error": str(e),
                "status": "error"
            })
        except:
            pass
    
    finally:
        try:
            await websocket.close()
        except:
            pass


@router.websocket("/ws/hitl")
async def websocket_hitl_conference(websocket: WebSocket):
    """
    HITL (Human-in-the-Loop) íŒ¨í„´ ì „ìš© WebSocket ì—”ë“œí¬ì¸íŠ¸
    
    ì‹¤ì œ ì‚¬ëŒì´ ê°œì…í•  ìˆ˜ ìˆëŠ” 3ë‹¨ ë¶„ê¸° ì›Œí¬í”Œë¡œìš°:
    - âœ… APPROVE: ì œì•ˆ ìŠ¹ì¸
    - ğŸŸ¡ REVISION: ìˆ˜ì • ìš”ì²­ (í”¼ë“œë°± ë°˜ì˜ í›„ ì¬ìƒì„±)
    - â›” REJECT: ì œì•ˆ ê±°ë¶€
    
    **í´ë¼ì´ì–¸íŠ¸ â†’ ì„œë²„ ë©”ì‹œì§€:**
    
    1. ì„¸ì…˜ ì‹œì‘:
    ```json
    {"action": "start", "topic": "AI ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ ì„¤ê³„"}
    ```
    
    2. ì‚¬ëŒ ê²°ì • ì œì¶œ:
    ```json
    {
        "action": "decision",
        "session_id": "abc123",
        "decision": "revision",  // approve, revision, reject
        "feedback": "ë¹„ìš© ë¶„ì„ ë¶€ë¶„ì„ ë” ìƒì„¸íˆ ì‘ì„±í•´ì£¼ì„¸ìš”"
    }
    ```
    
    **ì„œë²„ â†’ í´ë¼ì´ì–¸íŠ¸ ë©”ì‹œì§€:**
    
    1. ì„¸ì…˜ ì‹œì‘ë¨:
    ```json
    {"type": "hitl_session_start", "session_id": "abc123", ...}
    ```
    
    2. ì‚¬ëŒ ì…ë ¥ ëŒ€ê¸°:
    ```json
    {"type": "hitl_awaiting_input", "proposal": "...", "revision_count": 0, ...}
    ```
    
    3. ì—ì´ì „íŠ¸ ë©”ì‹œì§€:
    ```json
    {"type": "agent_message", "node": "proposal_generator", "content": "...", ...}
    ```
    
    4. ì™„ë£Œ:
    ```json
    {"type": "conference_complete", "pattern": "hitl", ...}
    ```
    """
    await websocket.accept()
    logger.info("ğŸ”Œ [HITL] WebSocket ì—°ê²°ë¨")
    
    session_id = None
    
    try:
        while True:
            # í´ë¼ì´ì–¸íŠ¸ ë©”ì‹œì§€ ìˆ˜ì‹ 
            data = await websocket.receive_json()
            action = data.get("action")
            
            if action == "start":
                # ìƒˆ HITL ì„¸ì…˜ ì‹œì‘
                topic = data.get("topic")
                if not topic:
                    await websocket.send_json({
                        "type": "error",
                        "error": "topicì€ í•„ìˆ˜ì…ë‹ˆë‹¤",
                        "status": "error"
                    })
                    continue
                
                logger.info(f"ğŸš€ [HITL] ì„¸ì…˜ ì‹œì‘ ìš”ì²­: topic={topic}")
                
                # ì„¸ì…˜ ì‹œì‘
                result = await conference_service.start_hitl_session(
                    topic=topic,
                    websocket=websocket,
                    max_revisions=data.get("max_revisions", 3)
                )
                
                # session_id ì €ì¥
                session_id = result.get("session_id")
                
                logger.info(f"âœ… [HITL] ì„¸ì…˜ ì‹œì‘ë¨: {session_id}")
            
            elif action == "decision":
                # ì‚¬ëŒì˜ ê²°ì • ì²˜ë¦¬
                decision = data.get("decision")  # approve, revision, reject
                feedback = data.get("feedback", "")
                req_session_id = data.get("session_id") or session_id
                
                if not req_session_id:
                    await websocket.send_json({
                        "type": "error",
                        "error": "session_idê°€ í•„ìš”í•©ë‹ˆë‹¤",
                        "status": "error"
                    })
                    continue
                
                if not decision:
                    await websocket.send_json({
                        "type": "error",
                        "error": "decisionì€ í•„ìˆ˜ì…ë‹ˆë‹¤ (approve, revision, reject)",
                        "status": "error"
                    })
                    continue
                
                logger.info(f"ğŸ‘¤ [HITL] ì‚¬ëŒ ê²°ì •: {decision}, feedback={feedback[:50]}...")
                
                # ê²°ì • ì²˜ë¦¬ ë° ë‹¤ìŒ ë‹¨ê³„ ì‹¤í–‰
                result = await conference_service.run_hitl_step(
                    session_id=req_session_id,
                    human_decision=decision,
                    human_feedback=feedback,
                    websocket=websocket
                )
                
                # ì™„ë£Œ ì²´í¬
                if result.get("status") == "completed" or result.get("workflow_status") == "completed":
                    logger.info(f"âœ… [HITL] ì›Œí¬í”Œë¡œìš° ì™„ë£Œ")
                    break
            
            else:
                await websocket.send_json({
                    "type": "error",
                    "error": f"ì•Œ ìˆ˜ ì—†ëŠ” action: {action}. 'start' ë˜ëŠ” 'decision'ì„ ì‚¬ìš©í•˜ì„¸ìš”.",
                    "status": "error"
                })
    
    except WebSocketDisconnect:
        logger.info("ğŸ”Œ [HITL] WebSocket ì—°ê²° ëŠê¹€")
        # ì„¸ì…˜ ì •ë¦¬
        if session_id and session_id in conference_service.active_sessions:
            del conference_service.active_sessions[session_id]
    
    except Exception as e:
        logger.error(f"âŒ [HITL] ì˜¤ë¥˜: {e}", exc_info=True)
        
        try:
            await websocket.send_json({
                "type": "error",
                "error": str(e),
                "status": "error"
            })
        except:
            pass
    
    finally:
        try:
            await websocket.close()
        except:
            pass


@router.post("/conference/run", tags=["conference"])
async def run_conference(
    pattern: str = Form(..., description="íŒ¨í„´ ì´ë¦„"),
    topic: str = Form(..., description="íšŒì˜ ì£¼ì œ"),
    max_rounds: Optional[int] = Form(3, description="Debate íŒ¨í„´ì˜ ìµœëŒ€ ë¼ìš´ë“œ ìˆ˜"),
    num_agents: Optional[int] = Form(5, description="Swarm íŒ¨í„´ì˜ ì—ì´ì „íŠ¸ ìˆ˜")
):
    """
    ë©€í‹° ì—ì´ì „íŠ¸ íšŒì˜ ì‹¤í–‰ (ì¼ë°˜ POST, ìŠ¤íŠ¸ë¦¬ë° ì—†ìŒ)
    
    **ì§€ì› íŒ¨í„´:**
    - `sequential`: ìˆœì°¨ íŒŒì´í”„ë¼ì¸ (A â†’ B â†’ C)
    - `planner_executor`: ê³„íš-ì‹¤í–‰ íŒ¨í„´
    - `role_based`: ì—­í•  ê¸°ë°˜ í˜‘ì—…
    - `hierarchical`: ê³„ì¸µ êµ¬ì¡° (Manager-Workers)
    - `debate`: í† ë¡  íŒ¨í„´ (Proposer â†” Critic)
    - `swarm`: êµ°ì§‘ íŒ¨í„´ (ê²½ìŸ ê¸°ë°˜ ì„ íƒ)
    
    **ì˜ˆì‹œ:**
    ```bash
    curl -X POST "http://localhost:8000/api/conference/run" \
      -F "pattern=sequential" \
      -F "topic=AI ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ"
    ```
    """
    try:
        logger.info(f"ğŸ¯ íšŒì˜ ì‹œì‘ (POST): pattern={pattern}, topic={topic}")
        
        # íŒ¨í„´ë³„ ì¶”ê°€ ì˜µì…˜
        kwargs = {}
        if pattern == "debate":
            kwargs["max_rounds"] = max_rounds
        elif pattern == "swarm":
            kwargs["num_agents"] = num_agents
        
        # íšŒì˜ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë° ì—†ìŒ)
        result = await conference_service.run_conference(
            pattern=pattern,
            topic=topic,
            websocket=None,
            **kwargs
        )
        
        logger.info(f"âœ… íšŒì˜ ì™„ë£Œ (POST): pattern={pattern}")
        
        return {
            "success": True,
            **result
        }
    
    except Exception as e:
        logger.error(f"âŒ íšŒì˜ ì˜¤ë¥˜: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# RAG (ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ) ì—”ë“œí¬ì¸íŠ¸
# ============================================================================

@router.post("/rag/upload", response_model=RAGUploadResponse, tags=["rag"])
async def upload_document(
    file: UploadFile = File(..., description="ì—…ë¡œë“œí•  ë¬¸ì„œ (PDF, MD, JSON, TXT)")
):
    """
    ë¬¸ì„œ ì—…ë¡œë“œ ë° ì¸ë±ì‹±
    
    **ì§€ì› íŒŒì¼ í˜•ì‹:**
    - PDF (.pdf)
    - Markdown (.md, .markdown)
    - JSON (.json)
    - Text (.txt, .text)
    
    **ì˜ˆì‹œ:**
    ```bash
    curl -X POST "http://localhost:8000/api/rag/upload" \
      -F "file=@document.pdf"
    ```
    
    **ì²˜ë¦¬ ê³¼ì •:**
    1. íŒŒì¼ ë‚´ìš© ì¶”ì¶œ (PDF â†’ í…ìŠ¤íŠ¸, JSON â†’ ë¬¸ìì—´ ë“±)
    2. ì§€ëŠ¥í˜• ì²­í‚¹ (ë¬¸ì¥/ë‹¨ë½ ê²½ê³„ ê³ ë ¤)
    3. í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ì‹± (Vector DB + BM25)
    """
    try:
        # íŒŒì¼ í˜•ì‹ ê²€ì¦
        filename = file.filename
        extension = filename.lower().split('.')[-1] if '.' in filename else ''
        
        supported = {'pdf', 'md', 'markdown', 'json', 'txt', 'text'}
        if extension not in supported:
            raise HTTPException(
                status_code=400,
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: .{extension}. ì§€ì› í˜•ì‹: {', '.join(supported)}"
            )
        
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        content = await file.read()
        
        # RAG ì„œë¹„ìŠ¤ë¡œ ì²˜ë¦¬
        rag = get_rag_service()
        doc_info = await rag.upload_document(content, filename)
        
        return RAGUploadResponse(
            success=True,
            doc_id=doc_info.doc_id,
            filename=doc_info.filename,
            file_type=doc_info.file_type,
            total_chunks=doc_info.total_chunks,
            message=f"ë¬¸ì„œ '{filename}'ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ({doc_info.total_chunks}ê°œ ì²­í¬ ìƒì„±)"
        )
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë¬¸ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/rag/query", response_model=RAGQueryResponse, tags=["rag"])
async def query_rag(request: RAGQueryRequest):
    """
    ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ
    
    **ê²€ìƒ‰ ë°©ë²•:**
    - `sparse`: BM25 í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
    - `dense`: ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
    - `hybrid`: Sparse + Dense ê²°í•© (ê¶Œì¥)
    
    **ì˜ˆì‹œ:**
    ```bash
    curl -X POST "http://localhost:8000/api/rag/query" \
      -H "Content-Type: application/json" \
      -d '{
        "question": "ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "k": 5,
        "search_method": "hybrid",
        "alpha": 0.5
      }'
    ```
    
    **alpha íŒŒë¼ë¯¸í„°:**
    - 0.0: 100% Sparse (í‚¤ì›Œë“œ ì™„ì „ ë§¤ì¹­)
    - 0.5: 50/50 ê· í˜• (ê¸°ë³¸ê°’)
    - 1.0: 100% Dense (ì˜ë¯¸ ê¸°ë°˜)
    
    **íŒ:**
    - ì „ë¬¸ ìš©ì–´/ì½”ë“œ: alpha=0.3 (í‚¤ì›Œë“œ ì¤‘ì‹¬)
    - ìì—°ì–´ ì§ˆë¬¸: alpha=0.7 (ì˜ë¯¸ ì¤‘ì‹¬)
    """
    try:
        rag = get_rag_service()
        
        response = await rag.query(
            question=request.question,
            k=request.k,
            search_method=request.search_method,
            alpha=request.alpha,
            use_reranker=request.use_reranker,
            doc_filter=request.doc_filter
        )
        
        # ì¶œì²˜ ì •ë³´ ë³€í™˜
        sources = [
            RAGSourceInfo(
                content=s["content"],
                score=s["score"],
                rank=s["rank"],
                filename=s["filename"],
                chunk_id=s["chunk_id"]
            )
            for s in response.sources
        ]
        
        return RAGQueryResponse(
            success=True,
            answer=response.answer,
            sources=sources,
            search_method=response.search_method,
            total_sources=response.total_sources,
            confidence=response.confidence
        )
    
    except Exception as e:
        logger.error(f"âŒ RAG ì§ˆì˜ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/rag/stream", tags=["rag"])
async def query_rag_stream(request: RAGQueryRequest):
    """
    ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ (í† í° ìŠ¤íŠ¸ë¦¬ë°, SSE)
    
    **ChatGPT ìŠ¤íƒ€ì¼ í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ**
    
    **SSE ì´ë²¤íŠ¸ í˜•ì‹:**
    - `sources`: ê²€ìƒ‰ëœ ì¶œì²˜ ì •ë³´ (ë‹µë³€ ìƒì„± ì „)
    - `token`: ê°œë³„ í† í° (íƒ€ìì¹˜ë“¯ ì¶œë ¥)
    - `done`: ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ
    - `error`: ì˜¤ë¥˜ ë°œìƒ
    
    **ì˜ˆì‹œ (JavaScript):**
    ```javascript
    const eventSource = new EventSource('/api/rag/stream?...');
    eventSource.onmessage = (event) => {
      const data = JSON.parse(event.data);
      if (data.type === 'token') {
        // í† í°ì„ í™”ë©´ì— ì¶”ê°€
        appendText(data.data);
      }
    };
    ```
    
    **fetch ì‚¬ìš© ì˜ˆì‹œ:**
    ```javascript
    const response = await fetch('/api/rag/stream', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ question: 'ì§ˆë¬¸...' })
    });
    
    const reader = response.body.getReader();
    const decoder = new TextDecoder();
    
    while (true) {
      const { value, done } = await reader.read();
      if (done) break;
      const text = decoder.decode(value);
      // SSE íŒŒì‹± ë° ì²˜ë¦¬
    }
    ```
    """
    async def generate_sse():
        try:
            rag = get_rag_service()
            
            async for event in rag.query_stream(
                question=request.question,
                k=request.k,
                search_method=request.search_method,
                alpha=request.alpha,
                use_reranker=request.use_reranker,
                doc_filter=request.doc_filter
            ):
                # SSE í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                yield f"data: {json.dumps(event, ensure_ascii=False)}\n\n"
                
        except Exception as e:
            logger.error(f"âŒ RAG ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'data': str(e)}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate_sse(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Nginx ë²„í¼ë§ ë¹„í™œì„±í™”
        }
    )


@router.post("/chat/stream", tags=["chat"])
async def chat_stream(
    question: str = Form(..., description="ì‚¬ìš©ì ë©”ì‹œì§€")
):
    """
    ì¼ë°˜ ì±„íŒ… (í† í° ìŠ¤íŠ¸ë¦¬ë°, SSE)
    
    **ChatGPT ìŠ¤íƒ€ì¼ í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ**
    
    **SSE ì´ë²¤íŠ¸ í˜•ì‹:**
    - `token`: ê°œë³„ í† í° (íƒ€ìì¹˜ë“¯ ì¶œë ¥)
    - `done`: ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ
    - `error`: ì˜¤ë¥˜ ë°œìƒ
    """
    async def generate_sse():
        try:
            logger.info(f"ğŸŒŠ ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {question[:50]}...")
            
            async for token in llm_service.generate_response_stream(question):
                yield f"data: {json.dumps({'type': 'token', 'data': token}, ensure_ascii=False)}\n\n"
            
            yield f"data: {json.dumps({'type': 'done', 'data': None}, ensure_ascii=False)}\n\n"
            
            logger.info("âœ… ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì±„íŒ… ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}", exc_info=True)
            yield f"data: {json.dumps({'type': 'error', 'data': str(e)}, ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate_sse(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@router.get("/rag/documents", response_model=List[RAGDocumentInfo], tags=["rag"])
async def list_documents():
    """
    ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
    
    **ì˜ˆì‹œ:**
    ```bash
    curl -X GET "http://localhost:8000/api/rag/documents"
    ```
    """
    try:
        rag = get_rag_service()
        documents = rag.list_documents()
        
        return [
            RAGDocumentInfo(
                doc_id=doc["doc_id"],
                filename=doc["filename"],
                file_type=doc["file_type"],
                total_chunks=doc["total_chunks"],
                uploaded_at=doc["uploaded_at"],
                metadata=doc["metadata"]
            )
            for doc in documents
        ]
    
    except Exception as e:
        logger.error(f"âŒ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/rag/documents/{doc_id}", tags=["rag"])
async def delete_document(doc_id: str):
    """
    ë¬¸ì„œ ì‚­ì œ
    
    **ì˜ˆì‹œ:**
    ```bash
    curl -X DELETE "http://localhost:8000/api/rag/documents/abc123"
    ```
    """
    try:
        rag = get_rag_service()
        success = rag.delete_document(doc_id)
        
        if not success:
            raise HTTPException(status_code=404, detail=f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_id}")
        
        return {"success": True, "message": f"ë¬¸ì„œ '{doc_id}'ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/rag/documents", tags=["rag"])
async def clear_all_documents():
    """
    ëª¨ë“  ë¬¸ì„œ ì‚­ì œ
    
    **ì£¼ì˜:** ëª¨ë“  ì¸ë±ì‹±ëœ ë¬¸ì„œê°€ ì‚­ì œë©ë‹ˆë‹¤!
    
    **ì˜ˆì‹œ:**
    ```bash
    curl -X DELETE "http://localhost:8000/api/rag/documents"
    ```
    """
    try:
        rag = get_rag_service()
        rag.clear_all_documents()
        
        return {"success": True, "message": "ëª¨ë“  ë¬¸ì„œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
    
    except Exception as e:
        logger.error(f"âŒ ì „ì²´ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/rag/stats", response_model=RAGStatsResponse, tags=["rag"])
async def get_rag_stats():
    """
    RAG ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ
    
    **ì˜ˆì‹œ:**
    ```bash
    curl -X GET "http://localhost:8000/api/rag/stats"
    ```
    """
    try:
        rag = get_rag_service()
        stats = rag.get_stats()
        
        return RAGStatsResponse(
            success=True,
            collection_name=stats["collection_name"],
            total_documents=stats["total_documents"],
            total_chunks=stats.get("chroma_count", 0),
            reranker_enabled=stats["reranker_enabled"],
            document_list=stats["document_list"]
        )
    
    except Exception as e:
        logger.error(f"âŒ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))