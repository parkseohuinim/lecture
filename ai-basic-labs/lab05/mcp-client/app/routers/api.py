"""API routes for ARI Processing"""
from fastapi import APIRouter, HTTPException, UploadFile, File, Form, BackgroundTasks, Response
from fastapi.responses import FileResponse
from typing import List, Optional
import tempfile
import os
import logging
from datetime import datetime
import json
import yaml

from app.models import HealthResponse, RagConfig, ProcessingMetadata, DocumentAnalysis, NavigationMenu, NavigationItem
from app.infrastructure.mcp.mcp_service import mcp_service
from app.infrastructure.llm.llm_service import llm_service

logger = logging.getLogger(__name__)

# Create API router
router = APIRouter()

@router.get("/", tags=["root"])
async def root():
    """Root endpoint"""
    return {"message": "ARI Processing Server is running"}

@router.get("/health", response_model=HealthResponse, tags=["health"])
async def health_check():
    """Health check endpoint"""
    try:
        health_data = await mcp_service.health_check()
        
        return HealthResponse(
            status="healthy" if health_data["connected"] else "unhealthy",
            mcp_connected=health_data["connected"],
            tools_available=health_data["tools_available"],
            details=health_data
        )
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return HealthResponse(
            status="error",
            mcp_connected=False,
            tools_available=0,
            details={"error": str(e)}
        )



def analyze_markdown_structure(markdown_content: str) -> dict:
    """
    ë§ˆí¬ë‹¤ìš´ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ RAG ì„¤ì •ì„ ì¶”ì²œ
    
    Returns:
        dict: {
            'separators': List[str],
            'chunk_size': int,
            'chunk_overlap': int,
            'analysis': dict  # ë¶„ì„ ìƒì„¸ ì •ë³´
        }
    """
    lines = markdown_content.split('\n')
    total_length = len(markdown_content)
    
    # íŒ¨í„´ ë¶„ì„
    has_headers = any(line.strip().startswith('#') for line in lines)
    has_horizontal_rules = any(line.strip() == '---' for line in lines)
    has_lists = any(line.strip().startswith(('-', '*', '+')) for line in lines)
    
    # í‘œ ê°ì§€: ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹ ë˜ëŠ” "[í‘œ" íŒ¨í„´
    has_markdown_tables = '|' in markdown_content and any('---' in line and '|' in line for line in lines)
    has_list_tables = any('[í‘œ' in line or '[Table' in line.lower() for line in lines)
    has_tables = has_markdown_tables or has_list_tables
    
    # ë‹¨ë½ êµ¬ë¶„ ë¶„ì„
    empty_line_count = sum(1 for line in lines if not line.strip())
    double_newline_count = markdown_content.count('\n\n')
    
    # í‰ê·  ë‹¨ë½ ê¸¸ì´ ê³„ì‚°
    paragraphs = [p.strip() for p in markdown_content.split('\n\n') if p.strip()]
    avg_paragraph_length = sum(len(p) for p in paragraphs) / len(paragraphs) if paragraphs else 0
    
    # Separator ìš°ì„ ìˆœìœ„ ê²°ì •
    separators = []
    
    # 1. í—¤ë” ê¸°ë°˜ ë¶„í•  (ê°€ì¥ í° ë‹¨ìœ„)
    if has_headers:
        # í—¤ë” ë ˆë²¨ë³„ë¡œ ë¶„í• 
        separators.extend(["\n### ", "\n## ", "\n# "])
    
    # 2. ìˆ˜í‰ì„  ê¸°ë°˜ ë¶„í• 
    if has_horizontal_rules:
        separators.append("\n---\n")
    
    # 3. ì´ì¤‘ ê°œí–‰ (ë‹¨ë½ êµ¬ë¶„) - í•­ìƒ í¬í•¨
    separators.append("\n\n")
    
    # 4. ë‹¨ì¼ ê°œí–‰ - í•­ìƒ í¬í•¨
    separators.append("\n")
    
    # 5. ë¬¸ì¥ ë‹¨ìœ„ ë¶„í•  (í‘œ ë°ì´í„° ë“± ê¸´ ì¤„ ëŒ€ì‘)
    # ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ ë’¤ ê³µë°±ìœ¼ë¡œ ë¬¸ì¥ êµ¬ë¶„
    separators.extend([". ", "? ", "! "])
    
    # 6. ì‰¼í‘œ/ì„¸ë¯¸ì½œë¡  ë‹¨ìœ„ ë¶„í•  (ë” ì„¸ë°€í•œ ë¶„í• )
    separators.extend([", ", "; ", ": "])
    
    # 7. ê³µë°±ì€ ì¡°ê±´ë¶€ ì¶”ê°€ (í‘œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ)
    if has_tables:
        separators.append(" ")
    
    # 8. ë¹ˆ ë¬¸ìì—´ì€ ì œê±° (ê³¼ë„í•œ ë¶„í•  ë°©ì§€)
    # separators.append("")
    
    # Chunk Size ê²°ì •
    # í‘œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë” í° ì²­í¬ ì‚¬ìš©
    if has_tables:
        # í‘œê°€ ìˆìœ¼ë©´ í° ì²­í¬ ì‚¬ìš© (í‘œê°€ ì˜ë¦¬ì§€ ì•Šë„ë¡)
        chunk_size = 3000
    elif avg_paragraph_length > 0:
        # ë‹¨ë½ì´ ì§§ìœ¼ë©´ ì—¬ëŸ¬ ë‹¨ë½ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
        if avg_paragraph_length < 300:
            chunk_size = 2000
        elif avg_paragraph_length < 600:
            chunk_size = 1500
        else:
            chunk_size = 1200
    else:
        chunk_size = 1500  # ê¸°ë³¸ê°’
    
    # Chunk Overlap ê²°ì • (chunk_sizeì˜ 15-20%, ìµœëŒ€ 500)
    chunk_overlap = min(500, int(chunk_size * 0.2))
    
    analysis = {
        'total_length': total_length,
        'total_lines': len(lines),
        'has_headers': has_headers,
        'has_horizontal_rules': has_horizontal_rules,
        'has_lists': has_lists,
        'has_tables': has_tables,
        'paragraph_count': len(paragraphs),
        'avg_paragraph_length': int(avg_paragraph_length),
        'empty_line_count': empty_line_count,
        'double_newline_count': double_newline_count
    }
    
    return {
        'separators': separators,
        'chunk_size': chunk_size,
        'chunk_overlap': chunk_overlap,
        'analysis': analysis
    }


@router.post("/chat", tags=["chat"])
async def chat_endpoint(
    background_tasks: BackgroundTasks,
    question: str = Form(..., description="ì‚¬ìš©ì ë©”ì‹œì§€ ë˜ëŠ” ì§ˆë¬¸"),
    files: List[UploadFile] = File(default=[], description="HTML íŒŒì¼ë“¤ (ì„ íƒì‚¬í•­)")
):
    """
    í†µí•© ì±„íŒ… API - ì¼ë°˜ ëŒ€í™” + HTML íŒŒì¼ ë¶„ì„
    
    **ë™ì‘ ë°©ì‹:**
    
    1. **ì¼ë°˜ ëŒ€í™”** (íŒŒì¼ ì—†ìŒ):
       - ì§ˆë¬¸: "ì•ˆë…•í•˜ì„¸ìš”"
       - ì‘ë‹µ: AI ì¸ì‚¬ (ë„êµ¬ í˜¸ì¶œ ì—†ìŒ)
    
    2. **HTML íŒŒì¼ ë¶„ì„** (íŒŒì¼ ìˆìŒ + ì²˜ë¦¬ í‚¤ì›Œë“œ):
       - ì§ˆë¬¸: "ì´ HTML ë‚´ìš©ì„ ì¶”ì¶œí•´ì¤˜"
       - ì‘ë‹µ: Markdown ê²°ê³¼ + **Frontmatter íŒŒì¼ ìë™ ìƒì„±**
       - download_url í•„ë“œì— ë‹¤ìš´ë¡œë“œ ë§í¬ í¬í•¨
    
    3. **ì¼ë°˜ ì§ˆë¬¸** (íŒŒì¼ ìˆìŒ + ì¼ë°˜ í‚¤ì›Œë“œ):
       - ì§ˆë¬¸: "HTMLì´ë€?"
       - ì‘ë‹µ: AI ì„¤ëª… (ë„êµ¬ í˜¸ì¶œ ì—†ìŒ)
    
    **ì‘ë‹µ í˜•ì‹:**
    ```markdown
    ---json
    {
      "rag_config": {
        "separators": ["\n\n", "\n", " ", ""],
        "chunk_size": 1000,
        "chunk_overlap": 200,
        "document_type": "confluence_page"
      },
      "metadata": {
        "processed_at": "2025-10-29T...",
        "html_size": 123456,
        "markdown_size": 45678,
        "tools_used": ["ari_html_to_markdown"]
      },
      "navigation_menu": {
        "current_page_id": "180192188",
        "parent_page_id": "180192092",
        ...
      }
    }
    ---
    
    <!-- RAG_CONTENT_START -->
    
    # ì‹¤ì œ ë§ˆí¬ë‹¤ìš´ ë‚´ìš©
    ...
    ```
    
    **êµ¬ì¡° ì„¤ëª…:**
    - `---json ... ---`: ë©”íƒ€ë°ì´í„° ì˜ì—­ (RAG ì„¤ì •, ì²˜ë¦¬ ì •ë³´, ë„¤ë¹„ê²Œì´ì…˜ ë“±)
    - `<!-- RAG_CONTENT_START -->`: RAG ì¸ë±ì‹± ì‹œì‘ ì§€ì  ë§ˆì»¤
    - ë§ˆì»¤ ì´í›„: ì‹¤ì œ RAG ì¸ë±ì‹± ëŒ€ìƒ ì½˜í…ì¸ 
    
    **RAG íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ë°©ë²•:**
    ```python
    # íŒŒì¼ ì½ê¸°
    with open('content.md', 'r') as f:
        content = f.read()
    
    # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    if '---json' in content:
        _, frontmatter, rest = content.split('---', 2)
        metadata = json.loads(frontmatter.replace('json', '', 1))
    
    # RAG ì½˜í…ì¸ ë§Œ ì¶”ì¶œ
    if '<!-- RAG_CONTENT_START -->' in rest:
        rag_content = rest.split('<!-- RAG_CONTENT_START -->', 1)[1].strip()
    else:
        rag_content = rest.strip()
    
    # rag_contentë§Œ ì„ë² ë”© ë° ì¸ë±ì‹±
    ```
    """
    try:
        if not question.strip():
            raise HTTPException(status_code=400, detail="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
        
        # 1. HTML íŒŒì¼ ì½ê¸° (ì„ íƒì‚¬í•­)
        html_content = None
        content_length = 0
        file_info = []
        
        if files:
            html_files = [f for f in files if f.filename and f.filename.endswith('.html')]
            
            if html_files:
                logger.info(f"ğŸ“ {len(html_files)}ê°œ HTML íŒŒì¼ ê°ì§€")
                
                # ì²« ë²ˆì§¸ HTML íŒŒì¼ë§Œ ì²˜ë¦¬
                first_file = html_files[0]
                try:
                    content = await first_file.read()
                    html_content = content.decode('utf-8', errors='ignore')
                    content_length = len(html_content)
                    file_info.append({
                        "filename": first_file.filename,
                        "size": content_length
                    })
                    logger.info(f"ğŸ“„ HTML íŒŒì¼: {first_file.filename} ({content_length:,} bytes)")
                except Exception as e:
                    logger.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
                    raise HTTPException(status_code=400, detail=f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
        
        # 2. MCP ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        available_tools = mcp_service.available_tools
        
        if not available_tools:
            raise HTTPException(status_code=503, detail="ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # 3. LLM ì˜ë„ ë¶„ì„ ë° ì²˜ë¦¬
        if html_content:
            # HTML íŒŒì¼ì´ ìˆëŠ” ê²½ìš°
            logger.info("ğŸ¤– HTML íŒŒì¼ê³¼ í•¨ê»˜ ì§ˆë¬¸ ì²˜ë¦¬")
            try:
                answer, tools_used = await llm_service.query_with_raw_result_and_html(
                    question=question,
                    available_tools=available_tools,
                    html_content=html_content
                )
            except Exception as llm_error:
                logger.error(f"âŒ LLM ì²˜ë¦¬ ì‹¤íŒ¨: {llm_error}")
                raise HTTPException(
                    status_code=503,
                    detail=f"LLM ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {str(llm_error)}. API í‚¤ê°€ ìœ íš¨í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
                )
        else:
            # ì¼ë°˜ ëŒ€í™” (íŒŒì¼ ì—†ìŒ)
            logger.info("ğŸ’¬ ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬")
            answer = await llm_service.generate_response(question)
            tools_used = []
        
        logger.info(f"âœ… LLM ì²˜ë¦¬ ì™„ë£Œ: {len(answer):,} characters")
        logger.info(f"ğŸ”§ ì‚¬ìš©ëœ ë„êµ¬: {', '.join(tools_used) if tools_used else 'ì—†ìŒ'}")
        
        # HTML ì²˜ë¦¬ ì˜ë„ê°€ ì•„ë‹ˆë©´ (ì¼ë°˜ ì§ˆë¬¸ì´ë©´) JSON ì‘ë‹µë§Œ
        if not tools_used or 'ari_html_to_markdown' not in tools_used:
            logger.info("ğŸ’¬ ì¼ë°˜ ì§ˆë¬¸ - JSON ì‘ë‹µë§Œ")
            return {
                "success": True,
                "answer": answer,
                "tools_used": [],
                "has_markdown": False,
                "file_info": file_info
            }
        
        # HTML ì²˜ë¦¬ ì˜ë„ì¼ ê²½ìš° - Frontmatter íŒŒì¼ ìƒì„± + JSON ì‘ë‹µ
        # 4. ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë¶„ì„ ë° ìµœì  RAG ì„¤ì • ìë™ ì¶”ì²œ
        logger.info("ğŸ” ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë¶„ì„ ì¤‘...")
        rag_analysis = analyze_markdown_structure(answer)
        
        logger.info(f"ğŸ“Š [Frontmatter] ë¶„ì„ ê²°ê³¼:")
        logger.info(f"   - ì´ ê¸¸ì´: {rag_analysis['analysis']['total_length']:,} characters")
        logger.info(f"   - ë‹¨ë½ ìˆ˜: {rag_analysis['analysis']['paragraph_count']}")
        logger.info(f"   - í‰ê·  ë‹¨ë½ ê¸¸ì´: {rag_analysis['analysis']['avg_paragraph_length']} characters")
        logger.info(f"   - í—¤ë” ì¡´ì¬: {rag_analysis['analysis']['has_headers']}")
        logger.info(f"   - í…Œì´ë¸” ì¡´ì¬: {rag_analysis['analysis']['has_tables']}")
        logger.info(f"   - ê¶Œì¥ ì²­í¬ í¬ê¸°: {rag_analysis['chunk_size']}")
        logger.info(f"   - ê¶Œì¥ ì²­í¬ ì¤‘ë³µ: {rag_analysis['chunk_overlap']}")
        logger.info(f"   - ê¶Œì¥ Separators: {rag_analysis['separators'][:3]}...")
        
        # 5. ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ ì¶”ì¶œ (HTML íŒŒì¼ì´ ìˆì„ ë•Œë§Œ)
        navigation_menu = None
        
        if html_content:
            logger.info("ğŸ—‚ï¸ ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ ì¶”ì¶œ ì¤‘...")
            
            try:
                nav_result = await mcp_service.call_tool(
                    tool_name="ari_extract_navigation",
                    arguments={"html_content": html_content}
                )
                
                # MCP ê²°ê³¼ íŒŒì‹±
                if hasattr(nav_result, 'content') and nav_result.content:
                    nav_json = json.loads(nav_result.content[0].text)
                    if nav_json.get('success') and 'result' in nav_json:
                        nav_data = nav_json['result']
                        
                        # NavigationItem ë¦¬ìŠ¤íŠ¸ ìƒì„±
                        root_pages = [NavigationItem(**page) for page in nav_data.get('root_pages', [])]
                        all_pages = [NavigationItem(**page) for page in nav_data.get('all_pages', [])]
                        
                        navigation_menu = NavigationMenu(
                            current_page_id=nav_data.get('current_page_id'),
                            parent_page_id=nav_data.get('parent_page_id'),
                            root_pages=root_pages,
                            all_pages=all_pages
                        )
                        
                        logger.info(f"   - í˜„ì¬ í˜ì´ì§€ ID: {navigation_menu.current_page_id}")
                        logger.info(f"   - ë¶€ëª¨ í˜ì´ì§€ ID: {navigation_menu.parent_page_id}")
                        logger.info(f"   - ìµœìƒìœ„ í˜ì´ì§€ ìˆ˜: {len(navigation_menu.root_pages)}")
                        logger.info(f"   - ì „ì²´ í˜ì´ì§€ ìˆ˜: {len(navigation_menu.all_pages)}")
            except Exception as e:
                logger.warning(f"   - ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # 6. RAG ì„¤ì • ìƒì„± (ì„œë²„ì—ì„œ ì¶”ì²œí•œ ì„¤ì • ì‚¬ìš©)
        # MCP ì„œë²„ê°€ ìµœì í™”ëœ ì„¤ì •ì„ ì œê³µí•˜ëŠ”ì§€ í™•ì¸
        server_rag_config = None
        if tools_used and 'ari_html_to_markdown' in tools_used:
            # MCP ê²°ê³¼ì—ì„œ RAG ì„¤ì • ì¶”ì¶œ ì‹œë„
            try:
                # answerê°€ JSON í˜•ì‹ì¸ ê²½ìš° íŒŒì‹±
                if '{"success":' in answer:
                    import re
                    json_match = re.search(r'\{"success":.*\}', answer, re.DOTALL)
                    if json_match:
                        result_json = json.loads(json_match.group())
                        if result_json.get('success') and 'result' in result_json:
                            server_rag_config = result_json['result'].get('rag_config')
            except:
                pass
        
        # ì„œë²„ ì¶”ì²œ ì„¤ì •ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¶„ì„ ê¸°ë°˜ ì„¤ì • ì‚¬ìš©
        if server_rag_config:
            # ì„œë²„ê°€ ì¶”ì²œí•œ ë‹¨ì¼ separator ì‚¬ìš©
            primary_sep = server_rag_config.get('primary_separator', '\n- ')
            fallback_sep = server_rag_config.get('fallback_separator', '\n\n')
            
            # ë‹¨ì¼ separatorë§Œ ì§€ì›í•˜ëŠ” ì‹œìŠ¤í…œì„ ìœ„í•œ ì„¤ì •
            rag_config = RagConfig(
                separators=[primary_sep],  # ë‹¨ì¼ separator
                chunk_size=server_rag_config.get('chunk_size', 2000),
                chunk_overlap=server_rag_config.get('chunk_overlap', 400),
                document_type="confluence_page",
                strategy=server_rag_config.get('strategy', 'balanced')
            )
            
            logger.info(f"ğŸ¯ [Frontmatter] ì„œë²„ ì¶”ì²œ RAG ì„¤ì • ì‚¬ìš©:")
            logger.info(f"   - Primary Separator: {repr(primary_sep)}")
            logger.info(f"   - Chunk Size: {rag_config.chunk_size}")
            logger.info(f"   - Chunk Overlap: {rag_config.chunk_overlap}")
            logger.info(f"   - Strategy: {rag_config.strategy}")
        else:
            # ê¸°ë³¸ ë¶„ì„ ê¸°ë°˜ ì„¤ì • (ë‹¨ìˆœí™”)
            # ë¦¬ìŠ¤íŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            has_lists = rag_analysis['analysis'].get('has_lists', False)
            
            rag_config = RagConfig(
                separators=['\n- '] if has_lists else ['\n\n'],  # ë‹¨ì¼ separator
                chunk_size=rag_analysis['chunk_size'],
                chunk_overlap=rag_analysis['chunk_overlap'],
            document_type="confluence_page"
        )
        
        metadata = ProcessingMetadata(
            processed_at=datetime.now().isoformat(),
            html_size=content_length,
            markdown_size=len(answer),
            tools_used=tools_used if tools_used else [],
            document_analysis=DocumentAnalysis(**rag_analysis['analysis'])
        )
        
        # 7. í”„ë¡ íŠ¸ë§¤í„° ìƒì„± (JSON í˜•ì‹ - YAMLë³´ë‹¤ ì•ˆì •ì )
        frontmatter_data = {
            "rag_config": rag_config.model_dump(),
            "metadata": metadata.model_dump()
        }
        
        # ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if navigation_menu:
            frontmatter_data["navigation_menu"] = navigation_menu.model_dump()
        
        # JSONìœ¼ë¡œ ì§ë ¬í™” (ê°œí–‰ ë¬¸ì ë“±ì´ ì´ìŠ¤ì¼€ì´í”„ë¨)
        frontmatter_json = json.dumps(
            frontmatter_data,
            ensure_ascii=False,
            indent=2
        )
        
        # 7. í”„ë¡ íŠ¸ë§¤í„° + ë§ˆí¬ë‹¤ìš´ ê²°í•© (RAG ì½˜í…ì¸  ì‹œì‘ ë§ˆì»¤ ì¶”ê°€)
        final_content = f"""---json
{frontmatter_json}
---

<!-- RAG_CONTENT_START -->

{answer}
"""
        
        # 8. Markdown íŒŒì¼ ì €ì¥ (ì •ì  íŒŒì¼ë¡œ ì €ì¥)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"content_frontmatter_{timestamp}.md"
        
        # ì •ì  íŒŒì¼ ë””ë ‰í† ë¦¬ ìƒì„±
        static_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'static', 'downloads')
        os.makedirs(static_dir, exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        file_path = os.path.join(static_dir, filename)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(final_content)
        
        logger.info(f"ğŸ’¾ Frontmatter íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename} ({len(final_content):,} bytes)")
        logger.info(f"ğŸ¯ RAG ì½˜í…ì¸  êµ¬ë¶„ ë§ˆì»¤ ì¶”ê°€ë¨: <!-- RAG_CONTENT_START -->")
        
        # 9. ë‹¤ìš´ë¡œë“œ URL ìƒì„±
        download_url = f"/downloads/{filename}"
        
        # 10. íŒŒì¼ ìë™ ì‚­ì œ ì„¤ì • (1ì‹œê°„ í›„)
        def cleanup_file():
            try:
                import time
                time.sleep(3600)  # 1ì‹œê°„ í›„
                if os.path.exists(file_path):
                    os.unlink(file_path)
                    logger.info(f"ğŸ—‘ï¸ íŒŒì¼ ìë™ ì‚­ì œ ì™„ë£Œ: {filename}")
            except Exception as e:
                logger.warning(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        background_tasks.add_task(cleanup_file)
        
        # 11. JSON ì‘ë‹µ (ì±„íŒ… + ë‹¤ìš´ë¡œë“œ URL)
        return {
            "success": True,
            "answer": answer,
            "tools_used": tools_used,
            "has_markdown": True,
            "file_info": file_info,
            "frontmatter_file": {
                "filename": filename,
                "download_url": download_url,
                "size": len(final_content),
                "rag_config": rag_config.model_dump()
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "success": False,
            "answer": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "tools_used": [],
            "has_markdown": False
        }