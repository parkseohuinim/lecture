"""LLM Service - Handles OpenAI API interactions"""
from typing import List, Dict, Any, AsyncGenerator, Optional
import json
import logging
import asyncio
import httpx
import urllib3
from datetime import datetime
import tiktoken
import numpy as np
from openai import AsyncOpenAI
from app.config import settings
from app.infrastructure.mcp.mcp_service import mcp_service
from app.exceptions.base import LLMQueryError

logger = logging.getLogger(__name__)

# ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - OpenAI APIë§Œ ì‚¬ìš©
EMBEDDING_AVAILABLE = False
logger.info("ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© ì•ˆí•¨ - OpenAI APIë§Œìœ¼ë¡œ ì˜ë„ ë¶„ë¥˜")

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
no_ssl_httpx = httpx.AsyncClient(verify=False)


class LLMService:
    """Service class for managing LLM interactions"""
    
    def __init__(self):
        # Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ìš°ì„ ìˆœìœ„)
        self._azure_client = None
        self._openai_client = None
        self.azure_available = False
        
        if settings.azure_openai_enabled:
            logger.info("ğŸ”µ Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Standard OpenAI ë°©ì‹)")
            # Azure OpenAIë¥¼ Standard OpenAI ë°©ì‹ìœ¼ë¡œ ì‚¬ìš© (/openai/v1/ ê²½ë¡œ í¬í•¨)
            azure_base_url = f"{settings.azure_openai_api_base}/openai/v1/"
            logger.info(f"ğŸ”— Azure Base URL: {azure_base_url}")
            
            self._azure_client = AsyncOpenAI(
                api_key=settings.azure_openai_api_key,
                base_url=azure_base_url,
                http_client=no_ssl_httpx
            )
            self.azure_model = settings.azure_openai_model
            self.azure_available = True
        
        # ì¼ë°˜ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í´ë°±ìš©)
        if settings.openai_api_key:
            logger.info("ğŸŸ¢ ì¼ë°˜ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í´ë°±ìš©)")
            self._openai_client = AsyncOpenAI(api_key=settings.openai_api_key, http_client=no_ssl_httpx)
            self.openai_model = settings.openai_model
        
        # ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self._client = self._azure_client if self._azure_client else self._openai_client
        self.current_model = self.azure_model if self._azure_client else self.openai_model
        self.is_azure = bool(self._azure_client)
            
        try:
            self.tokenizer = tiktoken.encoding_for_model("gpt-4o")
        except Exception:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
    
    async def _call_llm_with_fallback(self, **kwargs) -> Any:
        """
        Azure OpenAI í˜¸ì¶œ í›„ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ OpenAIë¡œ ìë™ í´ë°±
        
        Args:
            **kwargs: OpenAI APIì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°ë“¤
            
        Returns:
            API ì‘ë‹µ ê°ì²´
        """
        # 1ì°¨ ì‹œë„: Azure OpenAI (ì„¤ì •ëœ ê²½ìš°)
        if self._azure_client and self.azure_available:
            try:
                logger.info(f"ğŸ”µ Azure OpenAI API í˜¸ì¶œ ì‹œì‘: model={self.azure_model}")
                response = await self._azure_client.chat.completions.create(
                    model=self.azure_model,
                    **kwargs
                )
                logger.info(f"âœ… Azure OpenAI API ì‘ë‹µ ì„±ê³µ")
                return response
                
            except Exception as azure_error:
                error_msg = str(azure_error)
                error_code = getattr(azure_error, 'status_code', None)
                
                # 403 ì—ëŸ¬ ë˜ëŠ” ì—°ê²° ì‹¤íŒ¨ ì‹œ í´ë°±
                if error_code == 403 or '403' in error_msg:
                    logger.warning(f"âš ï¸ Azure OpenAI 403 Forbidden - ë‚´ë¶€ë§ ì ‘ê·¼ ë¶ˆê°€")
                    logger.warning(f"ğŸ”„ ì¼ë°˜ OpenAIë¡œ í´ë°± ì‹œë„...")
                elif 'timeout' in error_msg.lower() or 'connection' in error_msg.lower():
                    logger.warning(f"âš ï¸ Azure OpenAI ì—°ê²° ì‹¤íŒ¨: {error_msg[:100]}")
                    logger.warning(f"ğŸ”„ ì¼ë°˜ OpenAIë¡œ í´ë°± ì‹œë„...")
                else:
                    # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ raise
                    logger.error(f"âŒ Azure OpenAI API ì˜¤ë¥˜: {error_msg[:200]}")
                    raise
                
                # Azure ì‚¬ìš© ë¶ˆê°€ëŠ¥ìœ¼ë¡œ ë§ˆí‚¹
                self.azure_available = False
        
        # 2ì°¨ ì‹œë„: ì¼ë°˜ OpenAI (í´ë°±)
        if self._openai_client:
            try:
                logger.info(f"ğŸŸ¢ ì¼ë°˜ OpenAI API í˜¸ì¶œ ì‹œì‘: model={self.openai_model}")
                response = await self._openai_client.chat.completions.create(
                    model=self.openai_model,
                    **kwargs
                )
                logger.info(f"âœ… ì¼ë°˜ OpenAI API ì‘ë‹µ ì„±ê³µ")
                return response
                
            except Exception as openai_error:
                logger.error(f"âŒ ì¼ë°˜ OpenAI API ì˜¤ë¥˜: {str(openai_error)[:200]}")
                raise LLMQueryError(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(openai_error)}")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŒ
        raise LLMQueryError("ì‚¬ìš© ê°€ëŠ¥í•œ LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤ (Azure, OpenAI ëª¨ë‘ ì‹¤íŒ¨)")
    
    
    def _format_tools_for_openai(self, available_tools: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Convert MCP tools format to OpenAI tools format
        
        MCP format might be: {"name": "tool_name", "description": "...", "parameters": {...}}
        OpenAI expects: {"type": "function", "function": {"name": "...", "description": "...", "parameters": {...}}}
        """
        formatted_tools = []
        for tool in available_tools:
            # Check if already in OpenAI format
            if "type" in tool and tool["type"] == "function" and "function" in tool:
                formatted_tools.append(tool)
            else:
                # Convert from MCP format to OpenAI format
                formatted_tool = {
                    "type": "function",
                    "function": {
                        "name": tool.get("name", ""),
                        "description": tool.get("description", ""),
                        "parameters": tool.get("parameters", {
                            "type": "object",
                            "properties": {},
                            "required": []
                        })
                    }
                }
                formatted_tools.append(formatted_tool)
        return formatted_tools
    
    async def query_with_raw_result_and_html(self, question: str, available_tools: List[Dict[str, Any]], html_content: str) -> tuple[str, List[str]]:
        """
        HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜ (MCP ë„êµ¬ í˜¸ì¶œ ë°©ì‹)
        - ì‚¬ìš©ìê°€ "ì¶”ì¶œí•´ì¤˜", "ìš”ì•½í•´ì¤˜" ë“±ì˜ ìš”ì²­ì„ í•˜ë©´ ari_html_to_markdown ë„êµ¬ í˜¸ì¶œ
        - trafilatura + BeautifulSoup + markdownifyë¡œ RAG ì¹œí™”ì ì¸ ë³€í™˜
        """
        try:
            logger.info(f"ğŸ¤– HTML ì²˜ë¦¬ ì‹œì‘ - ì‚¬ìš©ì ì§ˆë¬¸: {question}")
            logger.info(f"ğŸ“„ HTML í¬ê¸°: {len(html_content):,} bytes")
            
            # ì‚¬ìš©ëœ ë„êµ¬ë“¤ ì¶”ì 
            used_tools = []
            
            # 1ë‹¨ê³„: ëª¨ë¸ì´ ì˜ë„ ë¶„ë¥˜í•˜ì—¬ ë„êµ¬ ì„ íƒ
            formatted_tools = self._format_tools_for_openai(available_tools)
            
            system_prompt = """You are an AI assistant that analyzes user requests and determines the appropriate action.

When a user uploads an HTML file, analyze their question to understand their intent:

1. **HTML Processing Intent** (ì¶”ì¶œ/ìš”ì•½/ë¶„ì„/ë³€í™˜ ìš”ì²­):
   - Keywords: "ì¶”ì¶œ", "ìš”ì•½", "ë¶„ì„", "ë³€í™˜", "ì •ë¦¬", "íŒŒì‹±", "extract", "summarize", "parse", "convert"
   - Action: Call ari_html_to_markdown to process the HTML file
   - Example: "HTML ë‚´ìš©ì„ ì¶”ì¶œí•´ì¤˜", "ì´ íŒŒì¼ì„ ìš”ì•½í•´ì¤˜", "ë‚´ìš© ì •ë¦¬í•´ì¤˜"

2. **General Question** (ì¼ë°˜ ì§ˆë¬¸):
   - User asks about something unrelated to HTML processing
   - Action: Answer directly without calling tools
   - Example: "ì´ íŒŒì¼ì´ ë­ì•¼?", "HTMLì´ë€?", "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”?"

Available tools:
- ari_html_to_markdown: Converts HTML to RAG-friendly markdown format (USE THIS for HTML processing)
- health_check: System status check

**Instructions:**
- Carefully analyze the user's question to understand their intent
- If they want to extract/summarize/analyze the HTML content â†’ call ari_html_to_markdown
- If they ask a general question â†’ answer directly without tools
- Always respond in Korean for final answers"""

            user_message = f"""ì‚¬ìš©ìê°€ HTML íŒŒì¼ì„ ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ìœ„ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ê±°ë‚˜ ë‹µë³€í•´ì£¼ì„¸ìš”."""
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]
            
            # ëª¨ë¸ì´ ì˜ë„ ë¶„ë¥˜ ë° ë„êµ¬ ì„ íƒ
            logger.info("ğŸ¤– ëª¨ë¸ì´ ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ë„êµ¬ ì„ íƒ ì¤‘...")
            response = await self._call_llm_with_fallback(
                messages=messages,
                tools=formatted_tools,
                tool_choice="auto",
                timeout=60,
                max_tokens=500
            )
            
            message = response.choices[0].message
            
            if not message.tool_calls:
                # ë„êµ¬ í˜¸ì¶œ ì—†ìŒ = ì¼ë°˜ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨
                logger.info("ğŸ’¬ ëª¨ë¸ì´ ë„êµ¬ í˜¸ì¶œ ì—†ì´ ì§ì ‘ ë‹µë³€í•˜ê¸°ë¡œ ê²°ì •")
                answer = message.content or "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                # HTML ì²˜ë¦¬ ìš”ì²­ì¸ë° ë„êµ¬ í˜¸ì¶œì´ ì—†ìœ¼ë©´ ì—ëŸ¬
                if any(keyword in question.lower() for keyword in ['ì¶”ì¶œ', 'ìš”ì•½', 'ë¶„ì„', 'ë³€í™˜', 'ì •ë¦¬', 'extract', 'parse', 'convert']):
                    logger.error("âŒ HTML ì²˜ë¦¬ ìš”ì²­ì´ì§€ë§Œ ëª¨ë¸ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì§€ ì•ŠìŒ")
                    raise LLMQueryError("HTML ì²˜ë¦¬ ìš”ì²­ì´ì§€ë§Œ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return answer, []
            
            # 2ë‹¨ê³„: HTML â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜
            logger.info(f"ğŸ”§ ëª¨ë¸ì´ {len(message.tool_calls)}ê°œ ë„êµ¬ í˜¸ì¶œ ê²°ì •")
            
            tool_call = message.tool_calls[0]
            function_name = tool_call.function.name
            function_args = json.loads(tool_call.function.arguments)
            
            logger.info(f"ğŸ“„ ì„ íƒëœ ë„êµ¬: {function_name}")
            used_tools.append(function_name)
            
            # HTML ì²˜ë¦¬ ë„êµ¬ì— ì‹¤ì œ HTML ë‚´ìš© ì „ë‹¬
            if function_name == "ari_html_to_markdown":
                function_args["html_content"] = html_content
                logger.info(f"ğŸ“„ HTML ë‚´ìš© ì „ë‹¬: {len(html_content)} bytes")
            
            # ë„êµ¬ ì‹¤í–‰
            try:
                result = await mcp_service.call_tool(function_name, function_args)
                
                # fastmcp CallToolResult êµ¬ì¡° íŒŒì‹±
                markdown_content = ""
                success = False
                
                if hasattr(result, 'content') and result.content:
                    first_content = result.content[0]
                    if hasattr(first_content, 'text'):
                        json_data = json.loads(first_content.text)
                        logger.info(f"ğŸ“„ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼: {json_data.get('success')}")
                        
                        if json_data.get('success') and 'result' in json_data:
                            result_data = json_data['result']
                            if 'markdown' in result_data:
                                success = True
                                markdown_content = result_data['markdown']
                                logger.info(f"âœ… ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì™„ë£Œ: {len(markdown_content):,} ì")
                
                if not success:
                    logger.error("âŒ HTML â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹¤íŒ¨")
                    raise LLMQueryError("HTML â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
                
                # 3ë‹¨ê³„: í¬ê¸° ì œí•œ (1MB)
                max_response_size = 1024 * 1024  # 1MB
                content_size = len(markdown_content.encode('utf-8'))
                
                if content_size > max_response_size:
                    logger.warning(f"âš ï¸ ì‘ë‹µì´ ë„ˆë¬´ í½ë‹ˆë‹¤ ({content_size:,} bytes). 1MBë¡œ ì œí•œí•©ë‹ˆë‹¤.")
                    content_bytes = markdown_content.encode('utf-8')
                    truncated_bytes = content_bytes[:max_response_size-100]
                    
                    try:
                        markdown_content = truncated_bytes.decode('utf-8')
                    except UnicodeDecodeError:
                        for i in range(1, 11):
                            try:
                                markdown_content = truncated_bytes[:-i].decode('utf-8')
                                break
                            except UnicodeDecodeError:
                                continue
                    
                    markdown_content += "\n\n... (ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
                    logger.info(f"ğŸ“ ì‘ë‹µ í¬ê¸° ì¡°ì •ë¨: {len(markdown_content):,} characters")
                
                logger.info(f"âœ… HTML ì²˜ë¦¬ ì™„ë£Œ - ìµœì¢… í¬ê¸°: {len(markdown_content):,} characters")
                return markdown_content, used_tools
                    
            except Exception as e:
                logger.error(f"âŒ ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                raise LLMQueryError(f"MCP ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            
        except Exception as e:
            logger.error(f"HTML ì²˜ë¦¬ í”Œë¡œìš° ì‹¤íŒ¨: {e}")
            # LLM ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œ ìƒìœ„ ë ˆì´ì–´ì—ì„œ ì ì ˆíˆ ì²˜ë¦¬í•˜ë„ë¡ í•¨
            raise LLMQueryError(f"HTML ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    async def generate_response(self, prompt: str) -> str:
        """
        Generate a simple response using OpenAI Chat Completions API
        
        Args:
            prompt: The prompt to send to the LLM
            
        Returns:
            Generated response as string
        """
        try:
            # í† í° ìˆ˜ ê³„ì‚° ë° ì œí•œ í™•ì¸
            tokens = self.tokenizer.encode(prompt)
            token_count = len(tokens)
            
            # RAGìš©ìœ¼ë¡œëŠ” gpt-4o-mini ì‚¬ìš© (ë” ì €ë ´í•˜ê³  í† í° ì œí•œì´ í¼)
            model = "gpt-4o-mini"  # RAG ì‘ë‹µ ìƒì„±ìš©
            
            # í† í° ì œí•œ í™•ì¸ (gpt-4o-miniëŠ” 128k context)
            max_tokens = 20000  # ë” ì•ˆì „í•œ ì œí•œ
            if token_count > max_tokens:
                logger.warning(f"Prompt too long ({token_count} tokens), truncating to {max_tokens}")
                tokens = tokens[:max_tokens]
                prompt = self.tokenizer.decode(tokens)
                token_count = max_tokens
            
            logger.info(f"LLM request: {token_count} tokens")
            
            # Azureì™€ OpenAIì—ì„œ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
            if self._azure_client and self.azure_available:
                # AzureëŠ” ì„¤ì •ëœ ëª¨ë¸ ì‚¬ìš©
                response = await self._call_llm_with_fallback(
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=2000,  # ì‘ë‹µ í† í° ìˆ˜ ì¦ê°€
                    temperature=0.7
                )
            elif self._openai_client:
                # ì¼ë°˜ OpenAIëŠ” gpt-4o-mini ì‚¬ìš© (RAGìš©)
                response = await self._openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=2000,
                    temperature=0.7
                )
                logger.info(f"âœ… ì¼ë°˜ OpenAI (gpt-4o-mini) ì‘ë‹µ ì„±ê³µ")
            else:
                raise LLMQueryError("ì‚¬ìš© ê°€ëŠ¥í•œ LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Failed to generate response: {e}")
            raise LLMQueryError(f"Failed to generate response: {str(e)}")
    
    async def generate_response_stream(
        self, 
        prompt: str,
        system_prompt: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± (ChatGPT ìŠ¤íƒ€ì¼)
        Azure â†’ OpenAI ìë™ í´ë°± ì§€ì›
        
        Args:
            prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
            
        Yields:
            í† í° ë‹¨ìœ„ ë¬¸ìì—´
        """
        # í† í° ìˆ˜ ê³„ì‚° ë° ì œí•œ í™•ì¸
        tokens = self.tokenizer.encode(prompt)
        token_count = len(tokens)
        
        max_tokens = 20000
        if token_count > max_tokens:
            logger.warning(f"Prompt too long ({token_count} tokens), truncating")
            tokens = tokens[:max_tokens]
            prompt = self.tokenizer.decode(tokens)
        
        logger.info(f"ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­: {token_count} tokens")
        
        messages = [
            {
                "role": "system", 
                "content": system_prompt or "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."
            },
            {"role": "user", "content": prompt}
        ]
        
        # 1ì°¨ ì‹œë„: Azure OpenAI (ì„¤ì •ëœ ê²½ìš°)
        if self._azure_client and self.azure_available:
            try:
                logger.info(f"ğŸ”µ Azure OpenAI ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: model={self.azure_model}")
                stream = await self._azure_client.chat.completions.create(
                    model=self.azure_model,
                    messages=messages,
                    max_tokens=2000,
                    temperature=0.7,
                    stream=True
                )
                
                async for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
                
                logger.info("âœ… Azure OpenAI ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
                return  # ì„±ê³µ ì‹œ ì¢…ë£Œ
                
            except Exception as azure_error:
                error_msg = str(azure_error)
                error_code = getattr(azure_error, 'status_code', None)
                
                if error_code == 403 or '403' in error_msg:
                    logger.warning(f"âš ï¸ Azure OpenAI 403 Forbidden - ì¼ë°˜ OpenAIë¡œ í´ë°±")
                elif 'timeout' in error_msg.lower() or 'connection' in error_msg.lower():
                    logger.warning(f"âš ï¸ Azure OpenAI ì—°ê²° ì‹¤íŒ¨ - ì¼ë°˜ OpenAIë¡œ í´ë°±")
                else:
                    logger.error(f"âŒ Azure OpenAI ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {error_msg[:200]}")
                    raise LLMQueryError(f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {error_msg}")
                
                self.azure_available = False
        
        # 2ì°¨ ì‹œë„: ì¼ë°˜ OpenAI (í´ë°±)
        if self._openai_client:
            try:
                logger.info(f"ğŸŸ¢ ì¼ë°˜ OpenAI ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: model=gpt-4o-mini")
                stream = await self._openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=messages,
                    max_tokens=2000,
                    temperature=0.7,
                    stream=True
                )
                
                async for chunk in stream:
                    if chunk.choices and chunk.choices[0].delta.content:
                        yield chunk.choices[0].delta.content
                
                logger.info("âœ… ì¼ë°˜ OpenAI ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
                return
                
            except Exception as openai_error:
                logger.error(f"âŒ ì¼ë°˜ OpenAI ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(openai_error)[:200]}")
                raise LLMQueryError(f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(openai_error)}")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŒ
        raise LLMQueryError("ì‚¬ìš© ê°€ëŠ¥í•œ LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤ (Azure, OpenAI ëª¨ë‘ ì‹¤íŒ¨)")

# Global service instance
llm_service = LLMService()
