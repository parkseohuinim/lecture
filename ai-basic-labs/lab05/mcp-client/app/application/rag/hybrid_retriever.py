"""
Hybrid Retriever - BM25 + Vector + Re-ranking

Lab03ì˜ HybridRetriever, Reranker ì°¸ê³ í•˜ì—¬ êµ¬í˜„
"""
import logging
import os
import re
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import numpy as np

import chromadb
from chromadb.config import Settings
from rank_bm25 import BM25Okapi

logger = logging.getLogger(__name__)


@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    score: float
    metadata: Dict[str, Any]
    rank: int
    search_type: str  # "sparse", "dense", "hybrid", "reranked"


class HybridRetriever:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° (BM25 + Vector + Re-ranking)
    
    íŠ¹ì§•:
    - Sparse ê²€ìƒ‰: BM25 (í‚¤ì›Œë“œ ê¸°ë°˜)
    - Dense ê²€ìƒ‰: ChromaDB ë²¡í„° ê²€ìƒ‰ (ì˜ë¯¸ ê¸°ë°˜)
    - Hybrid: ë‘ ê²€ìƒ‰ ê²°ê³¼ ê²°í•©
    - Re-ranking: Cross-Encoderë¡œ ì¬ìˆœìœ„í™”
    """
    
    # í•œê¸€ ì¡°ì‚¬ íŒ¨í„´ (ê°„ë‹¨ ë²„ì „)
    KOREAN_PARTICLES = [
        'ì´ë€', 'ì´ë€?', 'ë€', 'ë€?', 'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼',
        'ì˜', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ê¹Œì§€',
        'ë¶€í„°', 'ì´ë‹¤', 'ì…ë‹ˆë‹¤', 'ì¸ê°€', 'ì¸ê°€?', 'ì¸ì§€', 'í•˜ëŠ”', 'ë˜ëŠ”'
    ]
    
    def __init__(
        self,
        collection_name: str = "rag_documents",
        persist_directory: str = "./chroma_db",
        embedding_model: str = "text-embedding-3-small",
        use_reranker: bool = True,
        reranker_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"
    ):
        """
        Args:
            collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„
            persist_directory: ChromaDB ì €ì¥ ê²½ë¡œ
            embedding_model: OpenAI ì„ë² ë”© ëª¨ë¸
            use_reranker: Re-ranker ì‚¬ìš© ì—¬ë¶€
            reranker_model: Re-ranker ëª¨ë¸ ì´ë¦„
        """
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self.embedding_model = embedding_model
        self.use_reranker = use_reranker
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.chroma_client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(anonymized_telemetry=False)
        )
        self.collection = None
        
        # BM25 ê´€ë ¨
        self.corpus: List[str] = []
        self.tokenized_corpus: List[List[str]] = []
        self.bm25: Optional[BM25Okapi] = None
        self.doc_metadata: List[Dict] = []
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸
        self.openai_client = None
        self._init_openai()
        
        # Re-ranker (lazy loading)
        self._reranker = None
        self._reranker_model = reranker_model
        
        logger.info(f"ğŸ” HybridRetriever ì´ˆê¸°í™”: collection={collection_name}")
    
    def _init_openai(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        from openai import OpenAI
        import httpx
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # SSL ì¸ì¦ì„œ ê²€ì¦ ìš°íšŒ (íšŒì‚¬ ë°©í™”ë²½ ëŒ€ì‘)
        http_client = httpx.Client(verify=False)
        self.openai_client = OpenAI(api_key=api_key, http_client=http_client)
    
    @property
    def reranker(self):
        """Re-ranker lazy loading"""
        if self._reranker is None and self.use_reranker:
            try:
                from sentence_transformers import CrossEncoder
                logger.info(f"ğŸ“¥ Re-ranker ëª¨ë¸ ë¡œë”© ì¤‘: {self._reranker_model}")
                self._reranker = CrossEncoder(self._reranker_model)
                logger.info("âœ… Re-ranker ì¤€ë¹„ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ Re-ranker ë¡œë”© ì‹¤íŒ¨: {e}")
                self.use_reranker = False
        return self._reranker
    
    def initialize_collection(self, reset: bool = False):
        """ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        if reset:
            try:
                self.chroma_client.delete_collection(name=self.collection_name)
                logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ: {self.collection_name}")
            except:
                pass
        
        self.collection = self.chroma_client.get_or_create_collection(
            name=self.collection_name,
            metadata={"hnsw:space": "l2"}  # L2 ê±°ë¦¬ ì‚¬ìš©
        )
        
        # ê¸°ì¡´ ë¬¸ì„œ ë¡œë“œ
        self._load_existing_documents()
        
        logger.info(f"ğŸ“š ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì™„ë£Œ: {self.collection.count()}ê°œ ë¬¸ì„œ")
    
    def _load_existing_documents(self):
        """ê¸°ì¡´ ë¬¸ì„œë¥¼ BM25 ì¸ë±ìŠ¤ì— ë¡œë“œ"""
        if self.collection is None:
            return
        
        count = self.collection.count()
        if count == 0:
            return
        
        # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        results = self.collection.get(include=["documents", "metadatas"])
        
        self.corpus = results["documents"] or []
        self.doc_metadata = results["metadatas"] or []
        self.tokenized_corpus = [self._tokenize_korean(doc) for doc in self.corpus]
        
        if self.tokenized_corpus:
            self.bm25 = BM25Okapi(self.tokenized_corpus)
        
        logger.info(f"ğŸ“– ê¸°ì¡´ {len(self.corpus)}ê°œ ë¬¸ì„œ BM25 ì¸ë±ìŠ¤ì— ë¡œë“œ")
    
    def _tokenize_korean(self, text: str) -> List[str]:
        """
        í•œê¸€ í† í°í™” (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
        
        âš ï¸ ì‹¤ë¬´ì—ì„œëŠ” KoNLPy í˜•íƒœì†Œ ë¶„ì„ê¸° ê¶Œì¥
        """
        # êµ¬ë‘ì ì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜
        text = re.sub(r'[.,!?;:()"\'\[\]{}]', ' ', text)
        
        # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        tokens = text.lower().split()
        
        # ì¡°ì‚¬ ì œê±° ì‹œë„
        cleaned_tokens = []
        for token in tokens:
            cleaned = token
            for particle in sorted(self.KOREAN_PARTICLES, key=len, reverse=True):
                if cleaned.endswith(particle) and len(cleaned) > len(particle):
                    cleaned = cleaned[:-len(particle)]
                    break
            if cleaned:
                cleaned_tokens.append(cleaned)
        
        return cleaned_tokens
    
    def add_documents(
        self,
        texts: List[str],
        metadatas: List[Dict[str, Any]],
        ids: Optional[List[str]] = None
    ):
        """
        ë¬¸ì„œ ì¶”ê°€ (ë²¡í„° + BM25 ì¸ë±ìŠ¤)
        
        Args:
            texts: ë¬¸ì„œ í…ìŠ¤íŠ¸ ëª©ë¡
            metadatas: ë©”íƒ€ë°ì´í„° ëª©ë¡
            ids: ë¬¸ì„œ ID ëª©ë¡
        """
        if self.collection is None:
            self.initialize_collection()
        
        if ids is None:
            start_idx = len(self.corpus)
            ids = [f"doc_{start_idx + i}" for i in range(len(texts))]
        
        # ì„ë² ë”© ìƒì„±
        logger.info(f"ğŸ”„ {len(texts)}ê°œ ë¬¸ì„œ ì„ë² ë”© ìƒì„± ì¤‘...")
        embeddings = self._get_embeddings(texts)
        
        # ChromaDBì— ì¶”ê°€
        self.collection.add(
            documents=texts,
            embeddings=embeddings,
            metadatas=metadatas,
            ids=ids
        )
        
        # BM25 ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        self.corpus.extend(texts)
        self.doc_metadata.extend(metadatas)
        new_tokenized = [self._tokenize_korean(text) for text in texts]
        self.tokenized_corpus.extend(new_tokenized)
        self.bm25 = BM25Okapi(self.tokenized_corpus)
        
        logger.info(f"âœ… {len(texts)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ (ì´ {len(self.corpus)}ê°œ)")
    
    def _get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """OpenAI ì„ë² ë”© ìƒì„±"""
        response = self.openai_client.embeddings.create(
            model=self.embedding_model,
            input=texts
        )
        return [data.embedding for data in response.data]
    
    def search(
        self,
        query: str,
        k: int = 5,
        method: str = "hybrid",
        alpha: float = 0.5,
        use_reranker: Optional[bool] = None
    ) -> List[SearchResult]:
        """
        ê²€ìƒ‰ ìˆ˜í–‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            method: ê²€ìƒ‰ ë°©ë²• ("sparse", "dense", "hybrid")
            alpha: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œ Dense ê°€ì¤‘ì¹˜ (0~1)
            use_reranker: Re-ranker ì‚¬ìš© ì—¬ë¶€ (Noneì´ë©´ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©)
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if self.collection is None or self.collection.count() == 0:
            logger.warning("âš ï¸ ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # ì‹¤ì œ k ì¡°ì •
        actual_k = min(k, len(self.corpus))
        
        # Re-ranker ì‚¬ìš© ì‹œ ë” ë§ì€ í›„ë³´ ê²€ìƒ‰
        should_rerank = use_reranker if use_reranker is not None else self.use_reranker
        search_k = actual_k * 3 if should_rerank else actual_k
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        if method == "sparse":
            results = self._sparse_search(query, search_k)
        elif method == "dense":
            results = self._dense_search(query, search_k)
        elif method == "hybrid":
            results = self._hybrid_search(query, search_k, alpha)
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ê²€ìƒ‰ ë°©ë²•: {method}")
        
        # Re-ranking
        if should_rerank and self.reranker and results:
            results = self._rerank(query, results, actual_k)
        else:
            results = results[:actual_k]
        
        return results
    
    def _sparse_search(self, query: str, k: int) -> List[SearchResult]:
        """BM25 Sparse ê²€ìƒ‰"""
        if self.bm25 is None:
            return []
        
        tokenized_query = self._tokenize_korean(query)
        scores = self.bm25.get_scores(tokenized_query)
        
        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤
        top_indices = np.argsort(scores)[::-1]
        
        results = []
        for rank, idx in enumerate(top_indices[:k], 1):
            score = float(scores[idx])
            if score > 0:
                results.append(SearchResult(
                    content=self.corpus[idx],
                    score=score,
                    metadata=self.doc_metadata[idx] if idx < len(self.doc_metadata) else {},
                    rank=rank,
                    search_type="sparse"
                ))
        
        return results
    
    def _dense_search(self, query: str, k: int) -> List[SearchResult]:
        """Vector Dense ê²€ìƒ‰"""
        query_embedding = self._get_embeddings([query])[0]
        
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=k,
            include=["documents", "metadatas", "distances"]
        )
        
        search_results = []
        for rank, (doc, meta, dist) in enumerate(zip(
            results["documents"][0],
            results["metadatas"][0],
            results["distances"][0]
        ), 1):
            # L2 ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (0~1)
            similarity = 1 / (1 + dist)
            search_results.append(SearchResult(
                content=doc,
                score=similarity,
                metadata=meta,
                rank=rank,
                search_type="dense"
            ))
        
        return search_results
    
    def _hybrid_search(self, query: str, k: int, alpha: float) -> List[SearchResult]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Sparse + Dense)
        
        ì ìˆ˜ = (1-alpha) * sparse_normalized + alpha * dense_normalized
        """
        sparse_results = self._sparse_search(query, k * 2)
        dense_results = self._dense_search(query, k * 2)
        
        # ì ìˆ˜ ì •ê·œí™”
        sparse_scores = {r.content: r.score for r in sparse_results}
        dense_scores = {r.content: r.score for r in dense_results}
        
        sparse_max = max(sparse_scores.values()) if sparse_scores else 1.0
        dense_max = max(dense_scores.values()) if dense_scores else 1.0
        
        # ê²°í•©
        combined = {}
        
        for result in sparse_results:
            content = result.content
            normalized = (result.score / sparse_max) * (1 - alpha) if sparse_max > 0 else 0
            combined[content] = SearchResult(
                content=content,
                score=normalized,
                metadata=result.metadata,
                rank=0,
                search_type="hybrid"
            )
        
        for result in dense_results:
            content = result.content
            normalized = (result.score / dense_max) * alpha if dense_max > 0 else 0
            
            if content in combined:
                combined[content].score += normalized
            else:
                combined[content] = SearchResult(
                    content=content,
                    score=normalized,
                    metadata=result.metadata,
                    rank=0,
                    search_type="hybrid"
                )
        
        # ì •ë ¬ ë° ìˆœìœ„ í• ë‹¹
        sorted_results = sorted(combined.values(), key=lambda x: x.score, reverse=True)
        for rank, result in enumerate(sorted_results[:k], 1):
            result.rank = rank
        
        return sorted_results[:k]
    
    def _rerank(
        self,
        query: str,
        results: List[SearchResult],
        top_k: int
    ) -> List[SearchResult]:
        """Cross-Encoder Re-ranking"""
        if not self.reranker or not results:
            return results[:top_k]
        
        import math
        
        # ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ìƒì„±
        pairs = [[query, r.content] for r in results]
        
        # Re-ranking ì ìˆ˜ ê³„ì‚°
        raw_scores = self.reranker.predict(pairs)
        
        # Sigmoidë¡œ ì •ê·œí™” (0~1)
        normalized_scores = [1 / (1 + math.exp(-s)) for s in raw_scores]
        
        # ê²°ê³¼ ì—…ë°ì´íŠ¸
        reranked = []
        for result, score in zip(results, normalized_scores):
            reranked.append(SearchResult(
                content=result.content,
                score=score,
                metadata={**result.metadata, "original_score": result.score},
                rank=0,
                search_type="reranked"
            ))
        
        # ì •ë ¬ ë° ìˆœìœ„ í• ë‹¹
        reranked.sort(key=lambda x: x.score, reverse=True)
        for rank, result in enumerate(reranked[:top_k], 1):
            result.rank = rank
        
        return reranked[:top_k]
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        return {
            "collection_name": self.collection_name,
            "total_documents": len(self.corpus),
            "chroma_count": self.collection.count() if self.collection else 0,
            "reranker_enabled": self.use_reranker,
            "embedding_model": self.embedding_model
        }
    
    def delete_document(self, doc_id: str) -> bool:
        """ë¬¸ì„œ ì‚­ì œ"""
        if self.collection is None:
            return False
        
        try:
            self.collection.delete(ids=[doc_id])
            # BM25 ì¸ë±ìŠ¤ ì¬êµ¬ì¶• í•„ìš”
            self._load_existing_documents()
            return True
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def clear_all(self):
        """ëª¨ë“  ë¬¸ì„œ ì‚­ì œ (ì»¬ë ‰ì…˜ ë‚´ ëª¨ë“  ë°ì´í„° ì‚­ì œ)"""
        try:
            if self.collection is not None:
                # ë°©ë²• 1: ì»¬ë ‰ì…˜ ë‚´ ëª¨ë“  ë¬¸ì„œ ID ê°€ì ¸ì™€ì„œ ì‚­ì œ
                all_ids = self.collection.get()["ids"]
                if all_ids:
                    self.collection.delete(ids=all_ids)
                    logger.info(f"ğŸ—‘ï¸ {len(all_ids)}ê°œ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")
                
                # ë°©ë²• 2: ì»¬ë ‰ì…˜ ìì²´ë¥¼ ì‚­ì œí•˜ê³  ì¬ìƒì„±
                try:
                    self.chroma_client.delete_collection(name=self.collection_name)
                    logger.info(f"ğŸ—‘ï¸ ì»¬ë ‰ì…˜ ì‚­ì œ: {self.collection_name}")
                except Exception as e:
                    logger.warning(f"ì»¬ë ‰ì…˜ ì‚­ì œ ì¤‘ ê²½ê³ : {e}")
                
                # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
                self.collection = self.chroma_client.get_or_create_collection(
                    name=self.collection_name,
                    metadata={"hnsw:space": "l2"}
                )
                logger.info(f"ğŸ“š ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ì „ì²´ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì»¬ë ‰ì…˜ ì¬ì´ˆê¸°í™” ì‹œë„
            try:
                self.collection = self.chroma_client.get_or_create_collection(
                    name=self.collection_name,
                    metadata={"hnsw:space": "l2"}
                )
            except:
                pass
        
        # ë©”ëª¨ë¦¬ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        self.corpus = []
        self.tokenized_corpus = []
        self.doc_metadata = []
        self.bm25 = None
        
        logger.info("ğŸ—‘ï¸ ëª¨ë“  ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")

