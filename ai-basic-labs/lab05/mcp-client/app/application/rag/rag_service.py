"""
RAG Service - ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ í†µí•© ì„œë¹„ìŠ¤

ê¸°ëŠ¥:
- ë¬¸ì„œ ì—…ë¡œë“œ ë° ì¸ë±ì‹±
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + Vector)
- Re-ranking
- LLM ê¸°ë°˜ ë‹µë³€ ìƒì„±
"""
import logging
import os
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime

from app.application.rag.document_processor import DocumentProcessor, ProcessedDocument, DocumentChunk
from app.application.rag.hybrid_retriever import HybridRetriever, SearchResult

logger = logging.getLogger(__name__)


@dataclass
class RAGResponse:
    """RAG ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤"""
    answer: str
    sources: List[Dict[str, Any]]
    search_method: str
    total_sources: int
    query: str
    confidence: str  # "high", "medium", "low"


@dataclass
class DocumentInfo:
    """ë¬¸ì„œ ì •ë³´ ë°ì´í„° í´ë˜ìŠ¤"""
    doc_id: str
    filename: str
    file_type: str
    total_chunks: int
    uploaded_at: str
    metadata: Dict[str, Any]


class RAGService:
    """
    RAG ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
    
    ì‚¬ìš©ë²•:
    ```python
    rag = RAGService()
    
    # ë¬¸ì„œ ì—…ë¡œë“œ
    doc_info = await rag.upload_document(file_content, filename)
    
    # ì§ˆì˜ì‘ë‹µ
    response = await rag.query("ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì€?")
    ```
    """
    
    def __init__(
        self,
        collection_name: str = "rag_documents",
        persist_directory: str = "./chroma_db",
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        use_reranker: bool = True
    ):
        """
        Args:
            collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„
            persist_directory: ë°ì´í„° ì €ì¥ ê²½ë¡œ
            chunk_size: ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ê²¹ì¹¨
            use_reranker: Re-ranker ì‚¬ìš© ì—¬ë¶€
        """
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.doc_processor = DocumentProcessor(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        self.retriever = HybridRetriever(
            collection_name=collection_name,
            persist_directory=persist_directory,
            use_reranker=use_reranker
        )
        
        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥ (ë©”ëª¨ë¦¬)
        self.documents: Dict[str, DocumentInfo] = {}
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸
        self.openai_client = None
        self._init_openai()
        
        # ì´ˆê¸°í™”
        self.retriever.initialize_collection()
        
        # ê¸°ì¡´ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë³µì›
        self._restore_document_metadata()
        
        logger.info(f"ğŸ“š RAG Service ì´ˆê¸°í™” ì™„ë£Œ (ë³µì›ëœ ë¬¸ì„œ: {len(self.documents)}ê°œ)")
    
    def _restore_document_metadata(self):
        """
        ChromaDBì—ì„œ ê¸°ì¡´ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë³µì›
        ì„œë²„ ì¬ì‹œì‘ ì‹œ ë¬¸ì„œ ëª©ë¡ì„ ìœ ì§€í•˜ê¸° ìœ„í•¨
        """
        try:
            if self.retriever.collection is None:
                return
            
            count = self.retriever.collection.count()
            if count == 0:
                return
            
            # ëª¨ë“  ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            results = self.retriever.collection.get(include=["metadatas"])
            metadatas = results.get("metadatas", [])
            
            if not metadatas:
                return
            
            # doc_idë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë¬¸ì„œ ì •ë³´ ë³µì›
            doc_chunks: Dict[str, List[Dict]] = {}
            for meta in metadatas:
                if not meta:
                    continue
                doc_id = meta.get("doc_id")
                if doc_id:
                    if doc_id not in doc_chunks:
                        doc_chunks[doc_id] = []
                    doc_chunks[doc_id].append(meta)
            
            # DocumentInfo ë³µì›
            for doc_id, chunks in doc_chunks.items():
                if not chunks:
                    continue
                    
                first_chunk = chunks[0]
                filename = first_chunk.get("filename", "unknown")
                file_type = first_chunk.get("file_type", "unknown")
                
                self.documents[doc_id] = DocumentInfo(
                    doc_id=doc_id,
                    filename=filename,
                    file_type=file_type,
                    total_chunks=len(chunks),
                    uploaded_at=first_chunk.get("uploaded_at", "unknown"),
                    metadata={
                        "restored": True,
                        "chunk_count": len(chunks)
                    }
                )
            
            logger.info(f"ğŸ“– {len(self.documents)}ê°œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë³µì›ë¨")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë³µì› ì‹¤íŒ¨: {e}")
    
    def _init_openai(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        from openai import OpenAI
        import httpx
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        http_client = httpx.Client(verify=False)
        self.openai_client = OpenAI(api_key=api_key, http_client=http_client)
    
    async def upload_document(
        self,
        file_content: bytes,
        filename: str,
        extra_metadata: Optional[Dict[str, Any]] = None
    ) -> DocumentInfo:
        """
        ë¬¸ì„œ ì—…ë¡œë“œ ë° ì¸ë±ì‹±
        
        Args:
            file_content: íŒŒì¼ ë‚´ìš© (bytes)
            filename: íŒŒì¼ëª…
            extra_metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        
        Returns:
            DocumentInfo: ì—…ë¡œë“œëœ ë¬¸ì„œ ì •ë³´
        """
        try:
            logger.info(f"ğŸ“¤ ë¬¸ì„œ ì—…ë¡œë“œ ì‹œì‘: {filename}")
            
            # ë¬¸ì„œ ì²˜ë¦¬
            processed = self.doc_processor.process_file(
                file_path=filename,
                file_content=file_content,
                extra_metadata=extra_metadata
            )
            
            # ì²­í¬ë¥¼ ê²€ìƒ‰ê¸°ì— ì¶”ê°€
            texts = [chunk.content for chunk in processed.chunks]
            metadatas = []
            ids = []
            
            uploaded_at = datetime.now().isoformat()
            
            for chunk in processed.chunks:
                meta = {
                    **chunk.metadata,
                    "doc_id": processed.doc_id,
                    "filename": processed.filename,
                    "file_type": processed.file_type,
                    "uploaded_at": uploaded_at  # ë³µì›ìš© ë©”íƒ€ë°ì´í„°
                }
                metadatas.append(meta)
                ids.append(f"{processed.doc_id}_{chunk.chunk_id}")
            
            self.retriever.add_documents(texts, metadatas, ids)
            
            # ë¬¸ì„œ ì •ë³´ ì €ì¥
            doc_info = DocumentInfo(
                doc_id=processed.doc_id,
                filename=processed.filename,
                file_type=processed.file_type,
                total_chunks=processed.total_chunks,
                uploaded_at=datetime.now().isoformat(),
                metadata=processed.metadata
            )
            self.documents[processed.doc_id] = doc_info
            
            logger.info(f"âœ… ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ: {filename} ({processed.total_chunks}ê°œ ì²­í¬)")
            
            return doc_info
        
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    async def query(
        self,
        question: str,
        k: int = 5,
        search_method: str = "hybrid",
        alpha: float = 0.5,
        use_reranker: Optional[bool] = None,
        doc_filter: Optional[str] = None
    ) -> RAGResponse:
        """
        ì§ˆì˜ì‘ë‹µ ìˆ˜í–‰
        
        Args:
            question: ì§ˆë¬¸
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            search_method: ê²€ìƒ‰ ë°©ë²• ("sparse", "dense", "hybrid")
            alpha: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œ Dense ê°€ì¤‘ì¹˜
            use_reranker: Re-ranker ì‚¬ìš© ì—¬ë¶€
            doc_filter: íŠ¹ì • ë¬¸ì„œë§Œ ê²€ìƒ‰ (doc_id)
        
        Returns:
            RAGResponse: ë‹µë³€ ë° ì¶œì²˜
        """
        try:
            logger.info(f"ğŸ” ì§ˆì˜: {question[:50]}...")
            
            # ê²€ìƒ‰
            search_results = self.retriever.search(
                query=question,
                k=k,
                method=search_method,
                alpha=alpha,
                use_reranker=use_reranker
            )
            
            # ë¬¸ì„œ í•„í„° ì ìš©
            if doc_filter:
                search_results = [
                    r for r in search_results 
                    if r.metadata.get("doc_id") == doc_filter
                ]
            
            if not search_results:
                return RAGResponse(
                    answer="ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.",
                    sources=[],
                    search_method=search_method,
                    total_sources=0,
                    query=question,
                    confidence="low"
                )
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context(search_results)
            
            # LLM ë‹µë³€ ìƒì„±
            answer = await self._generate_answer(question, context, search_results)
            
            # ì‹ ë¢°ë„ íŒë‹¨
            confidence = self._assess_confidence(search_results)
            
            # ì¶œì²˜ ì •ë³´ êµ¬ì„±
            sources = []
            for r in search_results:
                sources.append({
                    "content": r.content[:200] + "..." if len(r.content) > 200 else r.content,
                    "score": round(r.score, 4),
                    "rank": r.rank,
                    "filename": r.metadata.get("filename", "unknown"),
                    "chunk_id": r.metadata.get("chunk_id", -1)
                })
            
            logger.info(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ (ì¶œì²˜: {len(sources)}ê°œ, ì‹ ë¢°ë„: {confidence})")
            
            return RAGResponse(
                answer=answer,
                sources=sources,
                search_method=search_results[0].search_type if search_results else search_method,
                total_sources=len(sources),
                query=question,
                confidence=confidence
            )
        
        except Exception as e:
            logger.error(f"âŒ ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise
    
    def _build_context(self, results: List[SearchResult], max_tokens: int = 3000) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []
        total_length = 0
        
        for r in results:
            # ëŒ€ëµì ì¸ í† í° ì¶”ì • (í•œê¸€ ê¸°ì¤€ ì•½ 2ìë‹¹ 1í† í°)
            estimated_tokens = len(r.content) // 2
            
            if total_length + estimated_tokens > max_tokens:
                break
            
            context_parts.append(f"[ì¶œì²˜: {r.metadata.get('filename', 'unknown')}]\n{r.content}")
            total_length += estimated_tokens
        
        return "\n\n---\n\n".join(context_parts)
    
    async def _generate_answer(
        self,
        question: str,
        context: str,
        results: List[SearchResult]
    ) -> str:
        """LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        
        system_prompt = """ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì§€ì¹¨
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸(ë¬¸ì„œ)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
2. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
3. ë‹µë³€ì— ê´€ë ¨ ì¶œì²˜ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”.
4. ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
5. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì†”ì§íˆ "ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.

## ë‹µë³€ í˜•ì‹
- í•µì‹¬ ë‹µë³€ì„ ë¨¼ì € ì œì‹œ
- í•„ìš”ì‹œ ìƒì„¸ ì„¤ëª… ì¶”ê°€
- ê´€ë ¨ ì¶œì²˜ ì–¸ê¸‰"""
        
        user_prompt = f"""## ì»¨í…ìŠ¤íŠ¸ (ê²€ìƒ‰ëœ ë¬¸ì„œ)

{context}

---

## ì§ˆë¬¸

{question}

---

## ìš”ì²­

ìœ„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ìƒì„¸íˆ ë‹µë³€í•´ì£¼ì„¸ìš”."""
        
        try:
            response = self.openai_client.chat.completions.create(
                model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )
            
            return response.choices[0].message.content
        
        except Exception as e:
            logger.error(f"LLM ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def _generate_answer_stream(
        self,
        question: str,
        context: str
    ):
        """LLMìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± (í† í° ë‹¨ìœ„) - llm_service ì‚¬ìš©"""
        from app.infrastructure.llm.llm_service import llm_service
        
        system_prompt = """ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ì§€ì¹¨
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸(ë¬¸ì„œ)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
2. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
3. ë‹µë³€ì— ê´€ë ¨ ì¶œì²˜ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”.
4. ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
5. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì†”ì§íˆ "ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.

## ë‹µë³€ í˜•ì‹
- í•µì‹¬ ë‹µë³€ì„ ë¨¼ì € ì œì‹œ
- í•„ìš”ì‹œ ìƒì„¸ ì„¤ëª… ì¶”ê°€
- ê´€ë ¨ ì¶œì²˜ ì–¸ê¸‰"""
        
        user_prompt = f"""## ì»¨í…ìŠ¤íŠ¸ (ê²€ìƒ‰ëœ ë¬¸ì„œ)

{context}

---

## ì§ˆë¬¸

{question}

---

## ìš”ì²­

ìœ„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ìƒì„¸íˆ ë‹µë³€í•´ì£¼ì„¸ìš”."""
        
        try:
            # llm_serviceì˜ ìŠ¤íŠ¸ë¦¬ë° ë©”ì„œë“œ ì‚¬ìš© (Azure/OpenAI í´ë°± ì§€ì›)
            async for token in llm_service.generate_response_stream(
                prompt=user_prompt,
                system_prompt=system_prompt
            ):
                yield token
                    
        except Exception as e:
            logger.error(f"LLM ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            yield f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def query_stream(
        self,
        question: str,
        k: int = 5,
        search_method: str = "hybrid",
        alpha: float = 0.5,
        use_reranker: Optional[bool] = None,
        doc_filter: Optional[str] = None
    ):
        """
        ìŠ¤íŠ¸ë¦¬ë° ì§ˆì˜ì‘ë‹µ ìˆ˜í–‰ (í† í° ë‹¨ìœ„ ì¶œë ¥)
        
        Yields:
            dict: {"type": "...", "data": ...}
            - type: "sources" | "token" | "done" | "error"
        """
        try:
            logger.info(f"ğŸ” ìŠ¤íŠ¸ë¦¬ë° ì§ˆì˜: {question[:50]}...")
            
            # ê²€ìƒ‰
            search_results = self.retriever.search(
                query=question,
                k=k,
                method=search_method,
                alpha=alpha,
                use_reranker=use_reranker
            )
            
            # ë¬¸ì„œ í•„í„° ì ìš©
            if doc_filter:
                search_results = [
                    r for r in search_results 
                    if r.metadata.get("doc_id") == doc_filter
                ]
            
            if not search_results:
                yield {
                    "type": "error",
                    "data": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
                }
                return
            
            # ì¶œì²˜ ì •ë³´ ë¨¼ì € ì „ì†¡
            sources = []
            for r in search_results:
                sources.append({
                    "content": r.content[:200] + "..." if len(r.content) > 200 else r.content,
                    "score": round(r.score, 4),
                    "rank": r.rank,
                    "filename": r.metadata.get("filename", "unknown"),
                    "chunk_id": r.metadata.get("chunk_id", -1)
                })
            
            confidence = self._assess_confidence(search_results)
            search_type = search_results[0].search_type if search_results else search_method
            
            yield {
                "type": "sources",
                "data": {
                    "sources": sources,
                    "confidence": confidence,
                    "search_method": search_type,
                    "total_sources": len(sources)
                }
            }
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context(search_results)
            
            # í† í° ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
            async for token in self._generate_answer_stream(question, context):
                yield {
                    "type": "token",
                    "data": token
                }
            
            # ì™„ë£Œ ì‹ í˜¸
            yield {
                "type": "done",
                "data": None
            }
            
            logger.info(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ì™„ë£Œ")
        
        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì§ˆì˜ ì‹¤íŒ¨: {e}")
            yield {
                "type": "error",
                "data": str(e)
            }
    
    def _assess_confidence(self, results: List[SearchResult]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ì‹ ë¢°ë„ í‰ê°€"""
        if not results:
            return "low"
        
        top_score = results[0].score
        
        # Re-ranked ê²°ê³¼ì¸ ê²½ìš° (sigmoid ì ìš©ëœ ì ìˆ˜)
        if results[0].search_type == "reranked":
            if top_score >= 0.8:
                return "high"
            elif top_score >= 0.5:
                return "medium"
            else:
                return "low"
        
        # Hybrid/ê¸°íƒ€ ê²°ê³¼
        if top_score >= 0.7:
            return "high"
        elif top_score >= 0.4:
            return "medium"
        else:
            return "low"
    
    def list_documents(self) -> List[Dict[str, Any]]:
        """ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡ ë°˜í™˜"""
        return [asdict(doc) for doc in self.documents.values()]
    
    def delete_document(self, doc_id: str) -> bool:
        """ë¬¸ì„œ ì‚­ì œ"""
        if doc_id not in self.documents:
            return False
        
        # ChromaDBì—ì„œ í•´ë‹¹ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ì‚­ì œ
        doc_info = self.documents[doc_id]
        for i in range(doc_info.total_chunks):
            chunk_id = f"{doc_id}_{i}"
            self.retriever.delete_document(chunk_id)
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì‚­ì œ
        del self.documents[doc_id]
        
        logger.info(f"ğŸ—‘ï¸ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {doc_id}")
        return True
    
    def clear_all_documents(self):
        """ëª¨ë“  ë¬¸ì„œ ì‚­ì œ"""
        self.retriever.clear_all()
        self.documents.clear()
        logger.info("ğŸ—‘ï¸ ëª¨ë“  ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        retriever_stats = self.retriever.get_stats()
        return {
            **retriever_stats,
            "total_documents": len(self.documents),
            "document_list": list(self.documents.keys())
        }
    
    def update_settings(
        self,
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None,
        use_reranker: Optional[bool] = None
    ):
        """ì„¤ì • ì—…ë°ì´íŠ¸"""
        if chunk_size or chunk_overlap:
            self.doc_processor.update_settings(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
        
        if use_reranker is not None:
            self.retriever.use_reranker = use_reranker
        
        logger.info("âš™ï¸ ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ")


# Global service instance (ì‹±ê¸€í†¤)
_rag_service: Optional[RAGService] = None


def get_rag_service() -> RAGService:
    """RAG ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _rag_service
    if _rag_service is None:
        _rag_service = RAGService()
    return _rag_service


# í¸ì˜ë¥¼ ìœ„í•œ alias
rag_service = get_rag_service

