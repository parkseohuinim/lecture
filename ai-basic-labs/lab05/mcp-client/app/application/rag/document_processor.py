"""
Document Processor - ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ì²˜ë¦¬ ë° ì²­í‚¹

ì§€ì› í˜•ì‹: PDF, Markdown, JSON, TXT
"""
import logging
import re
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class DocumentChunk:
    """ë¬¸ì„œ ì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    chunk_id: int
    metadata: Dict[str, Any]
    

@dataclass
class ProcessedDocument:
    """ì²˜ë¦¬ëœ ë¬¸ì„œ ë°ì´í„° í´ë˜ìŠ¤"""
    doc_id: str
    filename: str
    file_type: str
    total_chunks: int
    chunks: List[DocumentChunk]
    metadata: Dict[str, Any]


class DocumentProcessor:
    """
    ë¬¸ì„œ ì²˜ë¦¬ í´ë˜ìŠ¤
    
    - PDF, Markdown, JSON, TXT íŒŒì¼ ë¡œë”©
    - ì§€ëŠ¥í˜• ì²­í‚¹ (ë¬¸ì¥/ë‹¨ë½ ê²½ê³„ ê³ ë ¤)
    - ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    """
    
    SUPPORTED_EXTENSIONS = {'.pdf', '.md', '.markdown', '.json', '.txt', '.text'}
    
    def __init__(
        self,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        separators: Optional[List[str]] = None
    ):
        """
        Args:
            chunk_size: ì²­í¬ ìµœëŒ€ í¬ê¸° (ë¬¸ì ìˆ˜)
            chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹¨ í¬ê¸°
            separators: ë¶„í• ì— ì‚¬ìš©í•  êµ¬ë¶„ì ëª©ë¡ (ìš°ì„ ìˆœìœ„ ìˆœ)
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.separators = separators or [
            "\n\n",      # ë‹¨ë½ êµ¬ë¶„
            "\n",        # ì¤„ ë°”ê¿ˆ
            ". ",        # ë¬¸ì¥ ë
            "? ",        # ì§ˆë¬¸ ë
            "! ",        # ëŠë‚Œí‘œ ë
            ", ",        # ì‰¼í‘œ
            " ",         # ê³µë°±
        ]
    
    def process_file(
        self,
        file_path: str,
        file_content: Optional[bytes] = None,
        doc_id: Optional[str] = None,
        extra_metadata: Optional[Dict[str, Any]] = None
    ) -> ProcessedDocument:
        """
        íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ì²­í¬ë¡œ ë¶„í• 
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ (í™•ì¥ì íŒë³„ìš©)
            file_content: íŒŒì¼ ë‚´ìš© (bytes), Noneì´ë©´ íŒŒì¼ì—ì„œ ì½ìŒ
            doc_id: ë¬¸ì„œ ID (Noneì´ë©´ ìë™ ìƒì„±)
            extra_metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        
        Returns:
            ProcessedDocument: ì²˜ë¦¬ëœ ë¬¸ì„œ
        """
        path = Path(file_path)
        extension = path.suffix.lower()
        filename = path.name
        
        if extension not in self.SUPPORTED_EXTENSIONS:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {extension}")
        
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        if file_content is None:
            with open(file_path, 'rb') as f:
                file_content = f.read()
        
        # í™•ì¥ìë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if extension == '.pdf':
            text = self._extract_pdf_text(file_content)
        elif extension == '.json':
            text = self._extract_json_text(file_content)
        else:  # .md, .markdown, .txt, .text
            text = file_content.decode('utf-8', errors='ignore')
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        text = self._clean_text(text)
        
        # ì²­í‚¹
        chunks = self._chunk_text(text, filename)
        
        # ë¬¸ì„œ ID ìƒì„±
        if doc_id is None:
            import hashlib
            doc_id = hashlib.md5(f"{filename}_{len(text)}".encode()).hexdigest()[:12]
        
        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "filename": filename,
            "file_type": extension[1:],  # .pdf -> pdf
            "total_length": len(text),
            "chunk_size_setting": self.chunk_size,
            "chunk_overlap_setting": self.chunk_overlap,
        }
        if extra_metadata:
            metadata.update(extra_metadata)
        
        logger.info(f"ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {filename} ({len(chunks)}ê°œ ì²­í¬)")
        
        return ProcessedDocument(
            doc_id=doc_id,
            filename=filename,
            file_type=extension[1:],
            total_chunks=len(chunks),
            chunks=chunks,
            metadata=metadata
        )
    
    def _extract_pdf_text(self, content: bytes) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            import pdfplumber
            import io
            
            text_parts = []
            with pdfplumber.open(io.BytesIO(content)) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    page_text = page.extract_text()
                    if page_text:
                        text_parts.append(f"[í˜ì´ì§€ {page_num}]\n{page_text}")
            
            return "\n\n".join(text_parts)
        except ImportError:
            logger.warning("pdfplumberê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. ëŒ€ì²´ ë°©ë²• ì‹œë„...")
            # ëŒ€ì²´: pymupdf4llm ì‚¬ìš©
            try:
                import pymupdf4llm
                import io
                return pymupdf4llm.to_markdown(io.BytesIO(content))
            except:
                raise ImportError("PDF ì²˜ë¦¬ë¥¼ ìœ„í•´ pdfplumber ë˜ëŠ” pymupdf4llmì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    def _extract_json_text(self, content: bytes) -> str:
        """JSONì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  ë¬¸ìì—´ ê°’ ì¶”ì¶œ)"""
        try:
            data = json.loads(content.decode('utf-8'))
            texts = []
            self._extract_strings_from_json(data, texts)
            return "\n\n".join(texts)
        except json.JSONDecodeError as e:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
            return content.decode('utf-8', errors='ignore')
    
    def _extract_strings_from_json(self, obj: Any, texts: List[str], prefix: str = ""):
        """JSONì—ì„œ ì¬ê·€ì ìœ¼ë¡œ ë¬¸ìì—´ ì¶”ì¶œ"""
        if isinstance(obj, str):
            if len(obj.strip()) > 10:  # ì§§ì€ ë¬¸ìì—´ ì œì™¸
                texts.append(f"{prefix}: {obj}" if prefix else obj)
        elif isinstance(obj, dict):
            for key, value in obj.items():
                new_prefix = f"{prefix}.{key}" if prefix else key
                self._extract_strings_from_json(value, texts, new_prefix)
        elif isinstance(obj, list):
            for i, item in enumerate(obj):
                new_prefix = f"{prefix}[{i}]" if prefix else f"[{i}]"
                self._extract_strings_from_json(item, texts, new_prefix)
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r' +', ' ', text)
        # ì—°ì† ì¤„ë°”ê¿ˆ ì •ë¦¬ (3ê°œ ì´ìƒ -> 2ê°œ)
        text = re.sub(r'\n{3,}', '\n\n', text)
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        return text
    
    def _chunk_text(self, text: str, source: str) -> List[DocumentChunk]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ë¬¸ì¥/ë‹¨ë½ ê²½ê³„ ê³ ë ¤)
        
        Lab03ì˜ DocumentProcessor.chunk_text ë°©ì‹ ì°¸ê³ 
        """
        chunks = []
        start = 0
        text_length = len(text)
        chunk_id = 0
        
        while start < text_length:
            end = start + self.chunk_size
            
            # í…ìŠ¤íŠ¸ ëì´ë©´ ê·¸ëƒ¥ ì¶”ê°€
            if end >= text_length:
                chunk_text = text[start:].strip()
                if chunk_text:
                    chunks.append(DocumentChunk(
                        content=chunk_text,
                        chunk_id=chunk_id,
                        metadata={
                            "source": source,
                            "chunk_id": chunk_id,
                            "start_char": start,
                            "end_char": text_length
                        }
                    ))
                break
            
            # ì ì ˆí•œ ë¶„í•  ì§€ì  ì°¾ê¸°
            best_end = self._find_split_point(text, start, end)
            
            chunk_text = text[start:best_end].strip()
            if chunk_text:
                chunks.append(DocumentChunk(
                    content=chunk_text,
                    chunk_id=chunk_id,
                    metadata={
                        "source": source,
                        "chunk_id": chunk_id,
                        "start_char": start,
                        "end_char": best_end
                    }
                ))
                chunk_id += 1
            
            # ë‹¤ìŒ ì²­í¬ ì‹œì‘ ìœ„ì¹˜ (ì˜¤ë²„ë© ì ìš©)
            next_start = best_end - self.chunk_overlap
            
            # ì§„í–‰ì´ ì—†ìœ¼ë©´ ê°•ì œë¡œ ì•ìœ¼ë¡œ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
            if next_start <= start:
                next_start = best_end
            
            start = next_start
        
        return chunks
    
    def _find_split_point(self, text: str, start: int, end: int) -> int:
        """ì ì ˆí•œ ë¶„í•  ì§€ì  ì°¾ê¸° (êµ¬ë¶„ì ìš°ì„ ìˆœìœ„ ì ìš©)"""
        search_end = min(end + 50, len(text))  # ì•½ê°„ì˜ ì—¬ìœ 
        
        for separator in self.separators:
            # í˜„ì¬ ë²”ìœ„ì—ì„œ êµ¬ë¶„ì ì°¾ê¸° (ë’¤ì—ì„œë¶€í„°)
            pos = text.rfind(separator, start, search_end)
            if pos != -1 and pos > start + self.chunk_size // 2:
                return pos + len(separator)
        
        # êµ¬ë¶„ìë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ê°•ì œ ë¶„í• 
        return end
    
    def update_settings(
        self,
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None,
        separators: Optional[List[str]] = None
    ):
        """ì²­í‚¹ ì„¤ì • ì—…ë°ì´íŠ¸"""
        if chunk_size is not None:
            self.chunk_size = chunk_size
        if chunk_overlap is not None:
            self.chunk_overlap = chunk_overlap
        if separators is not None:
            self.separators = separators
        
        logger.info(f"ğŸ“ ì²­í‚¹ ì„¤ì • ì—…ë°ì´íŠ¸: size={self.chunk_size}, overlap={self.chunk_overlap}")

